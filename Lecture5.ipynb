{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lecture 9, Back-propagation\n",
    "\n",
    "These notes cover a bit of week 4 as well. It's better to have feed forward, back propagation, and gradient checking all in one place.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks ([get a copy](https://web.stanford.edu/~hastie/ElemStatLearn/))\n",
    "* Computer Age Statistical Inference: Chapter 18, Neural Networks and Deep Learning (p. 351) ([get a copy](https://web.stanford.edu/~hastie/CASI/))\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) are a bit mathy and specific, but I've found them helpful when confused\n",
    "\n",
    "## Code\n",
    "\n",
    "### Feed-forward basics\n",
    "\n",
    "During the feed-forward, the data has to be fed to layers one after the other.\n",
    "\n",
    "#### Numpy importing\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved.\n",
    "\n",
    "Each NN library you use will have a way of handling matrices. They tend to be similar and might even just work with `numpy` matrices seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values a-flowing\n",
    "\n",
    "In a neural network, values flow through layers of synapses and neurons. This is called feed-forward.\n",
    "\n",
    "To feed-forward data through a neural network is to pass data through the network's weights and activation functions to create an output. The feed-forward receives its data at the input layer, a copy of the input. The activity begins at the first hidden layer, when the input signal is passed through synapses (weights), and adjusted (bias) and transformed (activation function) by the neuron. Here is what it does:\n",
    "\n",
    "##### First hidden layer\n",
    "\n",
    "Signal $x$ passing though weights $w_1$: $x \\times w_1$\n",
    "\n",
    "Signal adjusted by neuron bias $b_1$: $x \\times w_1 + b_1 = z_1$\n",
    "\n",
    "Signal shaped by activation function $\\sigma$: $a_1 = \\sigma(z_1)$\n",
    "\n",
    "Let's look at that more closely. The $w_1$ matrix (weights) is the collection of \"synapses\" of the \"neurons\": they are the connections the neurons use to pull in data. These synapses can be increased to amplify an incoming variable, set to zero to ignore one, or made negative to invert the inbound signal. During training they are tuned by the neurons to help the NN minimize prediction error.\n",
    "\n",
    "The biases $b_1$ are unique to each neuron. They're used by the neuron to adjust what they receive.\n",
    "\n",
    "The activation function like $\\sigma$, the sigmoid function, causes the neuron to output binary. Unlike a digital computer though, the signal can range *between* 0 and 1 if the neuron is unsure.\n",
    "\n",
    "* Synapses strengthen or weaken incoming variables with weights\n",
    "* Biases adjust the sum of the weighted signals\n",
    "* Neurons put all of it together and transform it with an activation function\n",
    "\n",
    "The weights are matrices with dimensions $in \\times out$, $in$ the size of the data coming in and $out$ the size of the data coming out. $out$ is the number of neurons in the layer, and $in$ is the amount of values each of these neurons is fed during feed-forward. The biases are $1 \\times out$, one bias for each neuron.\n",
    "\n",
    "![Dimensions](W5_SimpleNeurons.png \"Dimensions\")\n",
    "\n",
    "Above you can see that the input data has size 3 and that each neuron has 3 synapses ($2 \\times 3 = 6$ in total). There are 2 neurons in the layer and they produce 2 outputs, one each neuron. There are also 2 biases integrated inside the 2 neurons.\n",
    "\n",
    "Each layer has only two quantities: how much comes in and how much goes out.\n",
    "\n",
    "The first aspect of the feed-forward is then the flowing of data through weights, biases, and activation functions.\n",
    "\n",
    "#### Neurons a-working\n",
    "\n",
    "A neural network has a dual nature: a linear nature at the unit level and a complex non-linear one at the network level.\n",
    "\n",
    "The neurons' linear nature helps them perform computations. They each get their own copy of the data to work on. This amazing trick is possible because of matrix multiplcation (or dot product). Rows don't mix with other rows, neither columns with other columns.\n",
    "\n",
    "##### Matrices are fun\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output. The five neurons' outputs are the five elements in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will give you my own example of using the neurons' synapses to play with the input data. I can perform operations on the inputs separately for each neuron. Look at each weight column vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 2, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[-1,  0,  0,  0,  0],\n",
    "                   [ 0,  2,  0,  0,  0],\n",
    "                   [ 2,  0,  1, -1,  0],\n",
    "                   [ 0,  0,  0,  0, -1],\n",
    "                   [ 0,  0,  0,  1,  1]])\n",
    "x.dot(weight)\n",
    "\n",
    "# I could also just flip the identify matrix horizontally to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix algebra allows the NN to perform arithmetic.\n",
    "\n",
    "##### The non-linear mixing\n",
    "\n",
    "Data is not mixed within a layer, but it is mixed between them. Neural networks get their power from the interactions of their hidden layers.\n",
    "\n",
    "When you take a single-layer model like linear or logistic regression and add layers to it, you get the extra power.\n",
    "\n",
    "The feed-forward is a mixing of data over many layers of neurons. Each layer expands data into multiple copies and its neurons compress it back into a few outputs. In the diagram above you see 3 units of inputs expanded into 6 synapse signals and then collapsed into 2 output signals. The power of the neural network comes from the fact that the next layer *then copies* these 2 output signals to each of its own neurons, so everything affects everything.\n",
    "\n",
    "![Re-using footage](W5_SimpleNeurons.png \"Re-using footage\")\n",
    "\n",
    "See how many arrows there are going from layer to layer? Information expands and contracts, and network elements are generously connected together.\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive commitees. The neurons in a layer form a commitee that looks at data together, performs analysis, and then summarizes its findings into a small report. Higher commitities then analyze this report at a higher level, and so on. The final output layer makes a decision based on the accumulated wisdom of the executive summary it receives: it outputs a single value between 0 and 1.\n",
    "\n",
    "All this mixing allows the neural network to work with very complicated data.\n",
    "\n",
    "#### Hiddens layers feed-forwarding\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "1. $z_1 = X \\times W_1 + B_1$\n",
    "2. $a_1 = \\sigma(z_1)$\n",
    "3. $z_2 = a_1 \\times W_2 + B_2$\n",
    "4. $a_2 = \\sigma(z_2)$\n",
    "5. $z_{output} = a_2 \\times W_{output} + B_{output}$\n",
    "6. $a_{output} = \\sigma(z_{output})$\n",
    "\n",
    "By way of comparison, here is Andrew Ng's notation.\n",
    "\n",
    "1. $a^{(1)} = x$\n",
    "2. $z^{(2)} = \\theta^{(1)} a^{(1)})$\n",
    "3. $a^{(2)} = g(z^{(2)})$\n",
    "2. $z^{(3)} = \\theta^{(2)} a^{(2)})$\n",
    "3. $a^{(3)} = g(z^{(3)})$\n",
    "2. $z^{(4)} = \\theta^{(3)} a^{(3)})$\n",
    "6. $a_{(4)} = h_{\\theta}(x)=g(z^{(4)})$\n",
    "\n",
    "Let's generate some data. Thanks to the properties of matrix multiplication, I can have 10 rows of input data and these will be processed fully separately, yielding 10 rows of output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random((10,5)) # Ten records of 5 variables\n",
    "b1 = np.random.random((1,3)) # Bias: 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # Weight: input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # Bias: 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # Weight: layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # Bias: 1 x output_size\n",
    "w_out = np.random.random((2,1)) # Weight: layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here then are the feed-forward results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85964668  0.92911942  0.96598944]\n",
      " [ 0.74989039  0.83492003  0.894504  ]\n",
      " [ 0.82784755  0.92345775  0.94699802]\n",
      " [ 0.79368997  0.87538952  0.91478411]\n",
      " [ 0.8134559   0.87357528  0.93745025]\n",
      " [ 0.84377582  0.91963116  0.95285849]\n",
      " [ 0.82065815  0.88618434  0.9201231 ]\n",
      " [ 0.75517447  0.85887612  0.86282307]\n",
      " [ 0.7390089   0.77372     0.8186372 ]\n",
      " [ 0.72950202  0.82478689  0.88013197]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = x.dot(w1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81659826  0.91946945]\n",
      " [ 0.80826541  0.90506906]\n",
      " [ 0.81567922  0.91628225]\n",
      " [ 0.81173275  0.91072747]\n",
      " [ 0.81206762  0.9132373 ]\n",
      " [ 0.81565776  0.91750589]\n",
      " [ 0.81281659  0.91323257]\n",
      " [ 0.80962827  0.90450471]\n",
      " [ 0.80306589  0.89757058]\n",
      " [ 0.8071836   0.90235338]]\n"
     ]
    }
   ],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85298277]\n",
      " [ 0.85121437]\n",
      " [ 0.85261462]\n",
      " [ 0.85191587]\n",
      " [ 0.85219783]\n",
      " [ 0.85274726]\n",
      " [ 0.85221495]\n",
      " [ 0.85118472]\n",
      " [ 0.85026614]\n",
      " [ 0.85089053]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "\n",
    "Backprop is how the network evaluates its performance during feed forward.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "If the feed-forward is data pushed all the way forward to the outputs, then back-propagation is the trickling down of errors flowing back from the outputs all the way back to the very first hidden layer.\n",
    "\n",
    "Backprop is a way of using gradient descent on neural networks of multiple layers. It isn't necessary with a linear or logistic regression because these are simple single-layer networks. Back-propagation allows you to apply gradient descent more than once.\n",
    "\n",
    "It all starts at the output. Here there is a clear link between the choice of parameters (weights and biases) and the output error. The approach here is the same as simple gradient descent.\n",
    "\n",
    "At the layer preceeding the output, we'll call it $l_2$, there is an extra step. What is the link between $l_2$ weights and biases and the output error? It has multiple steps: $l_2$ has a direct effect on the output layer's data, and the output layer's data has a direct effect on what the model decides to output. It takes two steps to get back to the end.\n",
    "\n",
    "In other words, the output layer is the boss and it is directly responsible for the model's error. If the output layer changes its behaviour, it can directly improve its accuracy. It's the easiest to train.\n",
    "\n",
    "The hidden layers are not directly responsible for the model's error; however, they are responsible for providing the output layer accurate analyses of the model's input data. Knowing their boss, they have an idea of how to change their computations so that the big cheese makes more informed decisions. Their gradient formulas in fact depend on the output layer's weights (the boss's personality, you might say)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
