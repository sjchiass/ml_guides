{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Back-propagation\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks ([get a copy](https://web.stanford.edu/~hastie/ElemStatLearn/))\n",
    "* Computer Age Statistical Inference: Chapter 18, Neural Networks and Deep Learning (p. 351) ([get a copy](https://web.stanford.edu/~hastie/CASI/))\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) are a bit mathy and specific, but I've found them helpful when confused\n",
    "\n",
    "## Code\n",
    "\n",
    "### Feed-forward basics\n",
    "\n",
    "#### Numpy importing\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved. Each NN library you use will have a way of handling matrices. They tend to be similar and might even just work with `numpy` matrices seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values a-flowing\n",
    "\n",
    "To feed-forward data through a neural network is to pass data through the network's weights and activation functions to create an output. The feed-forward technically begins with the input layer, a copy of the input data; however, the network's actual activity begins at the first hidden layer, when the input signal is passed through synapses (weights), and adjusted (bias) and transformed (activation function) by the neuron. Here is what it does:\n",
    "\n",
    "Signal $X$ passing though weights $W_1$: $X W_1$\n",
    "\n",
    "Signal adjusted by neuron bias $B_1$: $X W_1 + B_1 = z_1$\n",
    "\n",
    "Signal shaped by activation function $\\sigma$: $a_1 = \\sigma(z_1)$\n",
    "\n",
    "Let's look at that more closely. The $W_1$ (weights) are the \"synapses\" of the \"neurons\": they are the connections the neurons use to pull in data. These synapses can be increased to amplify an incoming variable, set to zero to ignore one, or made negative to invert the inbound signal. During training they are tuned by the neurons to help the NN minimize prediction error.\n",
    "\n",
    "* Synapses strengthen or weaken incoming variables with weights\n",
    "* Biases adjust the sum of the weighted signals\n",
    "* Neurons put all of it together and transform it with an activation function\n",
    "\n",
    "The weights are matrices with dimensions $in \\times out$, $in$ the size of the data coming in and $out$ the size of the data coming out. $out$ is the number of neurons in the layers, and $in$ is the amount of values each of these neurons is fed during feed-forward. The biases are $1 \\times out$, one bias for each neuron.\n",
    "\n",
    "![Dimensions](W5_SimpleNeurons.png \"Dimensions\")\n",
    "\n",
    "Above you can see that the input data has size 3 and that each neuron has 3 synapses ($2 \\times 3 = 6$ in total). There are 2 neurons in the layer and they produce 2 outputs, one each neuron. There are also 2 biases, 1 for each of the 2 neurons.\n",
    "\n",
    "Each layer has only two quantities: how much comes in and how much goes out.\n",
    "\n",
    "#### Neurons a-working\n",
    "\n",
    "The trick of the neurons is that they each get their own copy of the data to work on. This trick occurs because of the way matrix multiplcation is performed. Rows don't mix with other rows, neither columns with other columns.\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output. The five neurons' outputs are the five elements in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will give you my own example of using the neurons' synapses to play with the input data. I can perform operations on the inputs separately for each neuron. Look at each weight column vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 2, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[-1,  0,  0,  0,  0],\n",
    "                   [ 0,  2,  0,  0,  0],\n",
    "                   [ 2,  0,  1, -1,  0],\n",
    "                   [ 0,  0,  0,  0, -1],\n",
    "                   [ 0,  0,  0,  1,  1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed-forward is then a great mixing of data over many layers of neurons. Each layer expands data into multiple copies and its neurons compress it back into a few outputs. In the diagram above you see 3 units of inputs expanded into 6 synapse signals and then collapsed into 2 output signals. The power of the neural network comes from the fact that the next layer *then copies* these 2 output signals to each of its own neurons, so everything affects everything.\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive commitees. The neurons in a layer form a commitee that looks at data together, performs analysis, and then summarizes its findings into a condensed form. Higher commitities then analyze this report at a higher level, and so on, until the final output layer makes a decision based on the accumulated wisdom so far: it outputs a single value between 0 and 1.\n",
    "\n",
    "#### Hiddens layers feed-forwarding\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "$$z_1 = B_1 + X W_1$$\n",
    "\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "\n",
    "$$z_2 = B_2 + a_1 W_2$$\n",
    "\n",
    "$$a_2 = \\sigma(z_2)$$\n",
    "\n",
    "$$z_{output} = B_{output} + a_2 W_{output}$$\n",
    "\n",
    "$$a_{output} = \\sigma(z_{output})$$\n",
    "\n",
    "Let's generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.random((10,5)) # Ten records of 5 variables\n",
    "b1 = np.random.random((1,3)) # Bias: 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # Weight: input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # Bias: 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # Weight: layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # Bias: 1 x output_size\n",
    "w_out = np.random.random((2,1)) # Weight: layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here then are the feed-forward results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64227262  0.60985251  0.78711892]\n",
      " [ 0.70114371  0.68118057  0.85642685]\n",
      " [ 0.91000195  0.85217972  0.88737508]\n",
      " [ 0.86233322  0.8824174   0.89839849]\n",
      " [ 0.87061492  0.81245418  0.92443492]\n",
      " [ 0.7876347   0.68648412  0.871667  ]\n",
      " [ 0.83796698  0.73418675  0.84921294]\n",
      " [ 0.88872636  0.89106346  0.9514142 ]\n",
      " [ 0.876029    0.87653442  0.93500203]\n",
      " [ 0.82500859  0.75478229  0.90458187]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = b1 + X.dot(w1)\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85911287  0.84150666]\n",
      " [ 0.87068112  0.85267735]\n",
      " [ 0.88492     0.87719358]\n",
      " [ 0.88616093  0.87476691]\n",
      " [ 0.88586449  0.87422142]\n",
      " [ 0.87421201  0.86118636]\n",
      " [ 0.8751496   0.86618194]\n",
      " [ 0.8915369   0.87902956]\n",
      " [ 0.88936122  0.87700965]\n",
      " [ 0.88089184  0.86792231]]\n"
     ]
    }
   ],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80715585]\n",
      " [ 0.80807655]\n",
      " [ 0.80968044]\n",
      " [ 0.80961853]\n",
      " [ 0.80958369]\n",
      " [ 0.80858287]\n",
      " [ 0.80883824]\n",
      " [ 0.81000174]\n",
      " [ 0.80983374]\n",
      " [ 0.80912454]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
