{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Back-propagation\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks ([get a copy](https://web.stanford.edu/~hastie/ElemStatLearn/))\n",
    "* Computer Age Statistical Inference: Chapter 18, Neural Networks and Deep Learning (p. 351) ([get a copy](https://web.stanford.edu/~hastie/CASI/))\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) are a bit mathy and specific, but I've found them helpful when confused\n",
    "\n",
    "## Code\n",
    "\n",
    "### Feed-forward basics\n",
    "\n",
    "#### Numpy importing\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved. Each NN library you use will have a way of handling matrices. They tend to be similar and might even just work with `numpy` matrices seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values a-flowing\n",
    "\n",
    "To feed-forward data through a neural network is to pass data through the network's weights and activation function in order to produce an output. The feed-forward begins with the input layer, a copy of the input data. The network's activity begins at the first hidden layer, when the input signal is passed through synapses (weights) and is transformed by the neuron (activation function). Here is what it does:\n",
    "\n",
    "Signal passing though weights: $z_1 = B_1 + x W_1$\n",
    "\n",
    "Signal shaped by activation function: $a_1 = \\sigma(z_1)$\n",
    "\n",
    "Let's look at that more closely. The $B_1$ (biases) and the $W_1$ (weights) are the \"synapses\" of the \"neurons\": they are the connections the neurons use to pull in data. During training these connections are tuned by the neurons to help the NN perform better. They can be increased to amplify an incoming variable, set to zero to ignore one, or made negative to invert the inbound signal.\n",
    "\n",
    "* Synapses strengthen or weaken incoming variables with weights\n",
    "* Neurons combine incoming data and transform it with an activation function\n",
    "\n",
    "The weights are matrices with dimensions $in \\times out$, $in$ the size of the data coming in and $out$ the size of the data coming out. $out$ is the number of neurons in the layers, and $in$ is the amount of values each of these neurons is fed during feed-forward. When you look\n",
    "\n",
    "These matrices have dimensions of $in \\times out$, $in$ the size of data coming in and $out$ the number of neurons. Each neuron produces a single output, so the number of columns in the weight matrices also corresponds to the size of the layer's output.\n",
    "\n",
    "![Dimensions](\"W5_SimpleNeurons.png\" Dimensions)\n",
    "\n",
    "Above you can see that the input data has size 3 and that each neuron has three synapses. There are 2 neurons in the layer and they produce 2 outputs, one each.\n",
    "\n",
    "The crucial part of the neural network is that $X$ is matrix-multiplied by $W_1$, which is really $1 \\times in$ by $in \\times out$ giving a $1 \\times out$ matrix. The properties of matrix multiplication being what they are, each neuron \"works\" on the whole data independently and outputs a value separately.\n",
    "\n",
    "#### Neurons a-working\n",
    "\n",
    "The data is attended by multiple neurons, each able to perform its own processing of the data.\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed-forward is then a great mixing of data among layers of neurons, finally creating a network output. This \"densely connected\" NN has each neuron working separately from its neighbors but seeing the whole of the previous layer's data. These plentiful connections mean that the NN can model much more complicated functions. Although each layer is essentially linear, great power comes from the layers *interacting*.\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive commitees. Each neuron of each commitee works on a group report sent to its superior commitee, who work at a \"higher\" level further removed from the raw data. The big cheese at the output layer summarizes everything into a value from 0 to 1.\n",
    "\n",
    "#### Hiddens layers feed-forwarding\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "$$z_1 = B_1 + X W_1$$\n",
    "\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "\n",
    "$$z_2 = B_2 + a_1 W_2$$\n",
    "\n",
    "$$a_2 = \\sigma(z_2)$$\n",
    "\n",
    "$$z_{output} = B_{output} + a_2 W_{output}$$\n",
    "\n",
    "$$a_{output} = \\sigma(z_{output})$$\n",
    "\n",
    "Let's generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.random((10,5)) # Ten records of 5 variables\n",
    "b1 = np.random.random((1,3)) # 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # 1 x output_size\n",
    "w_out = np.random.random((2,1)) # layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here then are the feed-forward results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82777957  0.72575849  0.77176295]\n",
      " [ 0.92620955  0.85706331  0.8890858 ]\n",
      " [ 0.83853692  0.77898447  0.75915107]\n",
      " [ 0.91532254  0.76986905  0.90732909]\n",
      " [ 0.77760853  0.66409317  0.75770406]\n",
      " [ 0.8264613   0.74243828  0.79285232]\n",
      " [ 0.89717011  0.82897145  0.83053974]\n",
      " [ 0.91557286  0.8278428   0.85564208]\n",
      " [ 0.89115127  0.81131168  0.83602588]\n",
      " [ 0.81392367  0.69700482  0.799218  ]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = b1 + X.dot(w1)\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84824552  0.83219524]\n",
      " [ 0.86711719  0.84638292]\n",
      " [ 0.85392645  0.83686628]\n",
      " [ 0.85859515  0.83913049]\n",
      " [ 0.839436    0.82559796]\n",
      " [ 0.85042012  0.83387462]\n",
      " [ 0.86233629  0.84292561]\n",
      " [ 0.863309    0.84342722]\n",
      " [ 0.86050841  0.8414055 ]\n",
      " [ 0.84527275  0.82972201]]\n"
     ]
    }
   ],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79886702]\n",
      " [ 0.80021694]\n",
      " [ 0.79927688]\n",
      " [ 0.79960233]\n",
      " [ 0.79823471]\n",
      " [ 0.79902323]\n",
      " [ 0.79987655]\n",
      " [ 0.79994444]\n",
      " [ 0.79974492]\n",
      " [ 0.7986521 ]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
