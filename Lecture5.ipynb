{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Back-propagation\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks\n",
    "\n",
    "## Code\n",
    "\n",
    "### Feed-forward basics\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed-forward of a nmeural network runs input through the network's weights and actication functions to produce an output. The first \"layer\" of the NN is called the input layer, and it's merely a copy of the input data. The first active layer is the first of the NN's hidden layers. Here is what it does:\n",
    "\n",
    "$$z_1 = B_1 + x W_1$$\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "\n",
    "Let's look at that more closely. $B_1$ (biases) and $W_1$ (weights) are the \"neurons\" of the first layer. The convention is that their dimensions are $in \\times out$: $in$ is the number of input variables and $out$ is the number of neurons emitting output signals. The number of columns in the biases and weights are the neurons that individually perform some work on the incoming data.\n",
    "\n",
    "The crucial part of the neural network is that $x$ is matrix multiplied by $W_1$, which is $1 by in \\times in by out$ giving $1 by out$. The properties of matrix multiplication being what they are, each neuron \"works\" on the whole data independently and outputs a value separately.\n",
    "\n",
    "The data is attended by multiple neurons, each able to perform its own processing of the data.\n",
    "\n",
    "The feed-forward is then a great mixing of data among layers of neurons, finally creating a network output. This \"densely connected\" NN has each neuron working separately from its neighbors but seeing the whole of the previous layer's data. These plentiful connections mean that the NN can model much more complicated functions.\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "$$z_1 = B_1 + x W_1$$\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "$$z_2 = B_2 + a_1 W_2$$\n",
    "$$a_2 = \\sigma(z_2)$$\n",
    "$$z_{output} = B_{output} + a_2 W_{output}$$\n",
    "$$a_{output} = \\sigma(z_{output})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
