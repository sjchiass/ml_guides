{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Backprop guide: simple TensorFlow implementation\n",
    "\n",
    "This is my first time using TensorFlow, so the code might not be great; however, some of the lessons I've learned as a newbie are fresh in my mind, so they may be useful to you too.\n",
    "\n",
    "(Note: it's hard to see, but the text below has a few links to the TF documentation.)\n",
    "\n",
    "TF runs things through a [`tf.Session()`](https://www.tensorflow.org/api_docs/python/tf/Session). When I first booted up this notebook, I was ready to start multiplying matrices NumPy style, but then I realized something was missing: `tf.Session`. You can do some setup before starting the session, but any operation must be pass through `tf.Session().run()` to actually run.\n",
    "\n",
    "The other bit of confusion was with using Numpy arrays with TF. It turns out to be relatively easy. At first I thought I needed to [`tf.convert_to_tensor()`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor), but it turns out that the `feed_dict` method does not like TF.tensors and only likes Numpy (and a few other formats).\n",
    "\n",
    "Feeding the data to the model turned out to be simple, after more confusion. You can feed data to your model using [`feed_dict=`](https://www.tensorflow.org/versions/r0.12/how_tos/reading_data/#feeding) and [`tf.placeholder()`](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops/placeholders). The `feed_dict` is just a Python dictionary obect with variable names as keys (pointing to placeholders) and data as values. At first this seemed weird, but it really just seems to be you designating a drop-off point or a parking lot for your data. One pleasant surprise is that you can also feed a value like a learning rate this way.\n",
    "\n",
    "There you have it. Another beginner example of TensorFlow, this one written by a beginner. I'd like to thank my friends for coming over for some BBQ and some TensorFlow as we tried to make sense of this all.\n",
    "\n",
    "(Note: the SGD optimizer doesn't work too well. Uncomment the Adam optimizer for a boost in performance!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import load_digits # For the digit recognition example\n",
    "# Quickly get the data, as before\n",
    "# TF will convert everything itself\n",
    "digits = load_digits(n_class=10, return_X_y=True)\n",
    "x_train = digits[0]/16\n",
    "y_train = np.eye(10)[digits[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1, learning_rate   1.00, loss   0.25, accuracy  62.94%\n",
      "epoch   2, learning_rate   0.75, loss   0.12, accuracy  86.61%\n",
      "epoch   3, learning_rate   0.56, loss   0.09, accuracy  90.03%\n",
      "epoch   4, learning_rate   0.42, loss   0.08, accuracy  91.97%\n",
      "epoch   5, learning_rate   0.32, loss   0.07, accuracy  92.68%\n",
      "epoch   6, learning_rate   0.24, loss   0.07, accuracy  93.13%\n",
      "epoch   7, learning_rate   0.18, loss   0.06, accuracy  93.48%\n",
      "epoch   8, learning_rate   0.13, loss   0.06, accuracy  93.59%\n",
      "epoch   9, learning_rate   0.10, loss   0.06, accuracy  93.70%\n",
      "epoch  10, learning_rate   0.08, loss   0.06, accuracy  93.98%\n"
     ]
    }
   ],
   "source": [
    "# This class stores the NN's parameters\n",
    "class TFNeuralNetwork:\n",
    "    # This is the constructor: it's called as the object is created\n",
    "    # The important elements here are that tf.random_normal returns TF tensors\n",
    "    # and that tf.Variable marks these as parameters of the model, to be trained\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # First, save settings into class members\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the parameters\n",
    "        # These will only actually be initialized when \n",
    "        # tf.global_variables_initializer() is run\n",
    "        self.b1 = tf.Variable(tf.random_normal(shape=(1,self.hidden_size)))\n",
    "        self.w1 = tf.Variable(tf.random_normal(shape=(self.input_size, \n",
    "                                                      self.hidden_size)))\n",
    "        \n",
    "        self.b2 = tf.Variable(tf.random_normal(shape=(1,self.hidden_size)))\n",
    "        self.w2 = tf.Variable(tf.random_normal(shape=(self.hidden_size, \n",
    "                                                      self.hidden_size)))\n",
    "        \n",
    "        self.b_out = tf.Variable(tf.random_normal(shape=(1,self.output_size)))\n",
    "        self.w_out = tf.Variable(tf.random_normal(shape=(self.hidden_size, \n",
    "                                                         self.output_size)))\n",
    "        \n",
    "        # Create the placeholders\n",
    "        # These are filled with data only when training occurs\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "        self.y = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "        \n",
    "        # You can also store a learning rate in them (pleasantly surprised)\n",
    "        self.lr = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        # Define the NLL loss\n",
    "        # This has a gotcha: it wants the \"logits\", meaning outputs not passed \n",
    "        # through the sigmoid function. It does this internally to speed things \n",
    "        # up. I don't know what it's doing in the background to speed things up \n",
    "        # though.\n",
    "        self.loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(self.y, \n",
    "                                                                   self.feedforward(self.x)))\n",
    "        \n",
    "        # Define accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.y,1), \n",
    "                                           tf.argmax(self.feedforward(self.x),1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        # Define the optimizer\n",
    "        # This appears to be how TF calls the basic SGD optimizer. There are \n",
    "        # other options and they're pretty easy to choose if you want them.\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # For example, here's the Adam optimizer commented out, ready to go\n",
    "        #self.optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "        # Define the backprop step\n",
    "        self.train_step=self.optimizer.minimize(self.loss)\n",
    "    def feedforward(self, x):\n",
    "        # We just need to use the TF equivalents, otherwise not much different\n",
    "        z1 = tf.matmul(x, self.w1) + self.b1\n",
    "        a1 = tf.nn.sigmoid(z1)\n",
    "        z2 = tf.matmul(a1, self.w2) + self.b2\n",
    "        a2 = tf.nn.sigmoid(z2)\n",
    "        z_out = tf.matmul(a2, self.w_out) + self.b_out\n",
    "        a_out = z_out # don't apply sigmoid yourself\n",
    "        return a_out\n",
    "\n",
    "# Create a new NN\n",
    "NewNeuralNetwork = TFNeuralNetwork(64, 64, 10)\n",
    "\n",
    "# The session is what runs all TF operations\n",
    "sess = tf.Session()\n",
    "\n",
    "# When the initializer is run, all of our variables/parameters are given values\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "learning_rate = 1.0\n",
    "batch_size = 10\n",
    "batch_pos = list(range(0, digits[0].data.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)\n",
    "epochs = 10\n",
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for b in batch_pos:\n",
    "        # The session will run a training step, but also return loss and accuracy.\n",
    "        # The feed_dict is how you assign values for TF to use during its operation. \n",
    "        # As you can see, x, y, and the lr are set this way\n",
    "        _, loss, accuracy = sess.run([\n",
    "                NewNeuralNetwork.train_step, \n",
    "                NewNeuralNetwork.loss, \n",
    "                NewNeuralNetwork.accuracy\n",
    "            ], \n",
    "             feed_dict={\n",
    "                 NewNeuralNetwork.x: x_train[b:b+batch_size], \n",
    "                 NewNeuralNetwork.y: y_train[b:b+batch_size],\n",
    "                 NewNeuralNetwork.lr : learning_rate\n",
    "             }\n",
    "        )\n",
    "        epoch_loss += loss\n",
    "        epoch_accuracy += accuracy\n",
    "        batch_num += 1\n",
    "    print(\"epoch {:3d}, learning_rate {:6.2f}, loss {:6.2f}, accuracy {:6.2f}%\".format(\n",
    "        ep, learning_rate, epoch_loss/batch_num, 100.0*epoch_accuracy/batch_num))\n",
    "    learning_rate *= 0.75 # learning rate slowdown\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
