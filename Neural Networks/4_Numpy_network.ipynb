{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop guide: basic numpy code\n",
    "\n",
    "This code appears in the main notebook, but here it is on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For plotting things\n",
    "from sklearn.datasets import load_digits # Data to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the main part of the code. It will train itself to recognize the digits. The line with `MyNeuralNetwork = NeuralNetwork(64, 64, 10)` is the one you want to modify to change the input, hidden, and output sizes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1, batch  50, lr 0.10000, loss   2.09\n",
      "epoch   1, batch 100, lr 0.10000, loss   2.11\n",
      "epoch   1, batch 150, lr 0.10000, loss   1.23\n",
      "accuracy  76.91%\n",
      "epoch   2, batch  50, lr 0.07500, loss   1.51\n",
      "epoch   2, batch 100, lr 0.07500, loss   1.31\n",
      "epoch   2, batch 150, lr 0.07500, loss   0.87\n",
      "accuracy  87.09%\n",
      "epoch   3, batch  50, lr 0.05625, loss   1.38\n",
      "epoch   3, batch 100, lr 0.05625, loss   0.98\n",
      "epoch   3, batch 150, lr 0.05625, loss   0.73\n",
      "accuracy  90.09%\n",
      "epoch   4, batch  50, lr 0.04219, loss   1.29\n",
      "epoch   4, batch 100, lr 0.04219, loss   0.83\n",
      "epoch   4, batch 150, lr 0.04219, loss   0.65\n",
      "accuracy  91.04%\n",
      "epoch   5, batch  50, lr 0.03164, loss   1.24\n",
      "epoch   5, batch 100, lr 0.03164, loss   0.74\n",
      "epoch   5, batch 150, lr 0.03164, loss   0.61\n",
      "accuracy  92.26%\n",
      "epoch   6, batch  50, lr 0.02373, loss   1.19\n",
      "epoch   6, batch 100, lr 0.02373, loss   0.69\n",
      "epoch   6, batch 150, lr 0.02373, loss   0.58\n",
      "accuracy  92.93%\n",
      "epoch   7, batch  50, lr 0.01780, loss   1.16\n",
      "epoch   7, batch 100, lr 0.01780, loss   0.65\n",
      "epoch   7, batch 150, lr 0.01780, loss   0.57\n",
      "accuracy  93.10%\n",
      "epoch   8, batch  50, lr 0.01335, loss   1.14\n",
      "epoch   8, batch 100, lr 0.01335, loss   0.63\n",
      "epoch   8, batch 150, lr 0.01335, loss   0.56\n",
      "accuracy  93.32%\n",
      "epoch   9, batch  50, lr 0.01001, loss   1.12\n",
      "epoch   9, batch 100, lr 0.01001, loss   0.61\n",
      "epoch   9, batch 150, lr 0.01001, loss   0.56\n",
      "accuracy  93.60%\n",
      "epoch  10, batch  50, lr 0.00751, loss   1.12\n",
      "epoch  10, batch 100, lr 0.00751, loss   0.60\n",
      "epoch  10, batch 150, lr 0.00751, loss   0.56\n",
      "accuracy  93.77%\n"
     ]
    }
   ],
   "source": [
    "# This class stores the NN's parameters\n",
    "class NeuralNetwork:\n",
    "    # This is the constructor: it's called as the object is created\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.b1 = np.random.normal(size=(1,hidden_size))\n",
    "        self.w1 = np.random.normal(size=(input_size,hidden_size))\n",
    "        self.b2 = np.random.normal(size=(1,hidden_size))\n",
    "        self.w2 = np.random.normal(size=(hidden_size,hidden_size))\n",
    "        self.b_out = np.random.normal(size=(1,output_size))\n",
    "        self.w_out = np.random.normal(size=(hidden_size,output_size))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# This is the negative log-likelihood cost\n",
    "def NLLcost(a_out, y):\n",
    "    return -np.mean(np.sum(y*np.log(a_out) + (1.0-y)*np.log(1.0-a_out), 1))\n",
    "\n",
    "# Calculating accuracy\n",
    "def accuracy(a_out, labels):\n",
    "    # The following will return True if the maximum index of the output activation\n",
    "    # is the same as the label\n",
    "    tests = a_out.argmax(axis=1) == labels\n",
    "    return(tests.sum() / a_out.shape[0])\n",
    "\n",
    "# Feed forward\n",
    "def feedforward(x, NN):\n",
    "    # as you can see, feed forward is short and sweet\n",
    "    z1 = x @ NN.w1 + NN.b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ NN.w2 + NN.b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z_out = a2 @ NN.w_out + NN.b_out\n",
    "    a_out = sigmoid(z_out)\n",
    "    return a1, a2, a_out\n",
    "\n",
    "# Back propagation\n",
    "# You can see all of the steps written out a few sections above\n",
    "def backprop(x, y, NN, a1, a2, a_out):\n",
    "    # prepare a row vector of ones of width equal to the batch size\n",
    "    ones = np.ones((1, x.shape[0])) # this ensures the bias gradient is the right shape\n",
    "    # output parameters\n",
    "    delta = (a_out - y) # the error\n",
    "    w_out_grad = a2.transpose() @ delta # gradient for w_out\n",
    "    b_out_grad = ones @ delta # gradient for b_out\n",
    "    delta_out = delta @ NN.w_out.transpose() # the delta passed on to the next layer\n",
    "    # layer 2 parameters\n",
    "    prime2 = a2 * (1.0 - a2) # the derivative of the sigmoid\n",
    "    w2_grad = a1.transpose() @ (prime2 * delta_out) # w2 gradient\n",
    "    b2_grad = ones @ (prime2 * delta_out) # b2 gradient\n",
    "    delta2 = delta_out * prime2 @ NN.w2.transpose() # layer 2's delta\n",
    "    # layer 1 parameters\n",
    "    prime1 = a1 * (1.0 - a1) # derivative of the sigmoid\n",
    "    w1_grad = x.transpose() @ (prime1 * delta2) # w1 gradient\n",
    "    b1_grad = ones @ (prime1 * delta2) # b1 gradient\n",
    "    return w_out_grad, b_out_grad, w2_grad, b2_grad, w1_grad, b1_grad\n",
    "\n",
    "# Updates the parameters\n",
    "def step(NN, w2_grad, b2_grad, w1_grad, b1_grad, w_out_grad, b_out_grad, lr, batch_size):\n",
    "    # Note\n",
    "    NN.w1 = NN.w1 - (lr / batch_size) * w1_grad\n",
    "    NN.b1 = NN.b1 - (lr / batch_size) * b1_grad\n",
    "    NN.w2 = NN.w2 - (lr / batch_size) * w2_grad\n",
    "    NN.b2 = NN.b2 - (lr / batch_size) * b2_grad\n",
    "    NN.w_out = NN.w_out - (lr / batch_size) * w_out_grad\n",
    "    NN.b_out = NN.b_out - (lr / batch_size) * b_out_grad\n",
    "\n",
    "# Set our learning rate\n",
    "lr = 0.1\n",
    "# Initialize a neural network    \n",
    "MyNeuralNetwork = NeuralNetwork(64, 64, 10)\n",
    "# Load the data, this time with labels, index 0 is the input, 1 is the output\n",
    "digits = load_digits(n_class=10, return_X_y=True)\n",
    "# A sloppy data rescale (16 is the max pixel intensity)\n",
    "x = digits[0]/16\n",
    "# Create one-hot vectors for the outputs\n",
    "# I am lucky here that the digits are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "# Otherwise you would need a label dictionary to concert indices back to their labels\n",
    "y = np.eye(10)[digits[1]]\n",
    "# We'll do batches of 10... we need to find out the indexes to use\n",
    "batch_size = 10\n",
    "batch_pos = list(range(0, digits[0].data.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)\n",
    "epochs = 10\n",
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    for b in batch_pos:\n",
    "        batch_x = x[b:b+batch_size]\n",
    "        batch_y = y[b:b+batch_size]\n",
    "        a1, a2, a_out = feedforward(batch_x, MyNeuralNetwork)\n",
    "        cost = NLLcost(a_out, batch_y)\n",
    "        w_out_grad, b_out_grad, w2_grad, b2_grad, w1_grad, b1_grad \\\n",
    "            = backprop(batch_x, batch_y, MyNeuralNetwork, a1, a2, a_out)\n",
    "        step(MyNeuralNetwork, w2_grad, b2_grad, w1_grad, b1_grad, w_out_grad, b_out_grad, lr, batch_size)\n",
    "        if batch_num % 50 == 0:\n",
    "            print(\"epoch {:3d}, batch {:3d}, lr {:7.5f}, loss {:6.2f}\".format(ep, batch_num, lr, cost))\n",
    "        batch_num += 1\n",
    "    lr *= 0.75\n",
    "    a1, a2, a_out = feedforward(x, MyNeuralNetwork)\n",
    "    print(\"accuracy {:6.2f}%\".format(100 * accuracy(a_out, digits[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some code for testing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d37fc5208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d37fcceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict  4,  98.43% certain\n"
     ]
    }
   ],
   "source": [
    "def predict(x, NN):\n",
    "    pick = np.random.randint(1, x.shape[0])\n",
    "    image = np.reshape(x[pick,], (8,8))\n",
    "    plt.gray()\n",
    "    plt.matshow(image) \n",
    "    plt.show() \n",
    "    _, _, a_out = feedforward(x[pick,], NN)\n",
    "    # The certainty here is the value of the highest output activation\n",
    "    print(\"I predict {:2d}, {:6.2f}% certain\".format(np.asscalar(a_out.argmax(axis=1)), 100.0*np.max(a_out)))\n",
    "\n",
    "predict(x, MyNeuralNetwork)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
