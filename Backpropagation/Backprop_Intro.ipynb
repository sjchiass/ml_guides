{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop guide: a backprop narrative\n",
    "\n",
    "This was originally a guide for teaching week 5 of the Andrew Ng course, but it got out of control. The text is pretty long, but it has all my understanding of basic neural networks.\n",
    "\n",
    "# Further reading\n",
    "\n",
    "* Andrew Ng Machine Learning Coursera, [weeks 4 and 5](https://www.coursera.org/learn/machine-learning#syllabus)\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks ([get a copy](https://web.stanford.edu/~hastie/ElemStatLearn/))\n",
    "* Computer Age Statistical Inference: Chapter 18, Neural Networks and Deep Learning (p. 351) ([get a copy](https://web.stanford.edu/~hastie/CASI/))\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) are a bit mathy and specific, but I've found them helpful when confused\n",
    "* [Deep Learning Basics](http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html), a guide that covers about the same ground as this one\n",
    "* [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) using actual numbers\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## What this covers\n",
    "\n",
    "This covers the classic neural network, called the mutli-layer perceptron model, the feed-forward model, or the densely-connected neural network. It is made of a network of units that process information together.\n",
    "\n",
    "What's the point of the feed forward model? It acts like an overcharged logistic regression, powerful enough to do some basic machine learning tasks. It's pretty simple in its construction, so it's a favourite project for beginners.\n",
    "\n",
    "If you are familiar with logistic regressions, you will recognize some aspects of the feed forward network. The feed forward network is a network of logistic regressions.\n",
    "\n",
    "![Regression Relation](Images/intro_logit_and_mlp.png \"Regression Relation\")\n",
    "\n",
    "The downside of the feed forward model is its simplicity: it isn't spectacular on more interesting tasks, such as image classification and natural language processing. You could say it's a bit inflexible. As an added penalty on top of this inflexibility, the feed forward is a black box you can't look inside to see what's going on.\n",
    "\n",
    "Here is a short glossary you can refer to if you run into unknown terms here or elsewhere:\n",
    "\n",
    "* Model: in the classical sense, it's a representation of a real-world process. A neural network is a model because it approximates the real-life process that transforms the input data into the output data.\n",
    "* Loss, cost, objective function: generally mean the same thing, but can be used specifically. These are how neural networks quantify error. The goal of this neural network is to minimize these functions.\n",
    "* Neuron, perceptron: the neural network units and its point of reference. Data flows into the neurals and information flows out of it. In diagrams they appear as nodes.\n",
    "\n",
    "# Deep dive\n",
    "\n",
    "## The basic unit: the perceptron\n",
    "\n",
    "The following diagram shows a perceptron.\n",
    "\n",
    "![Just a lonely neuron](Images/intro_perceptron.png \"Just a lonely neuron\")\n",
    "\n",
    "Depending on its setup, it may be known as a neuron, a linear classifier, a linear regression (identity), or a logistic regression (sigmoid).\n",
    "\n",
    "## Feed forward basics\n",
    "\n",
    "A neural network is a stack of layers of perceptrons working together, and the term feed forward refers to \"feeding\" data through this network from start to finish. As you can see with the arbitrary example below, that can be a lot of feeding.\n",
    "\n",
    "![A lot](Images/intro_ManyNeurons.png \"A lot\")\n",
    "\n",
    "The diagram above is just to scare you. Neural networks typically have *a lot* more connections!\n",
    "\n",
    "[*Drawn with this python script*](https://gist.github.com/craffel/2d727968c3aaebd10359#file-draw_neural_net-py)\n",
    "\n",
    "### Numpy importing\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved.\n",
    "\n",
    "Each NN library you use will have a way of handling matrices. They tend to be similar and might even just work with `numpy` matrices seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values flowing\n",
    "\n",
    "In a neural network, values flow through layers of synapses and neurons. This is called feed-forward.\n",
    "\n",
    "Everything is passed forward to whatever is down the line. A neuron like the one below feeds information to many other neurons in the network.\n",
    "\n",
    "![Everything flows](Images/intro_perceptron_feed.png \"Everything flows\")\n",
    "\n",
    "To feed-forward data through a neural network is to pass data through the network's weights and activation functions to create an output. The feed-forward receives its data at the input layer, a copy of the input. The activity begins at the first hidden layer, when the input signal is passed through synapses (weights), and adjusted (bias) and transformed (activation function) by the neuron. Here is what it does:\n",
    "\n",
    "#### First hidden layer\n",
    "\n",
    "Signal $x$ passing though weights $w_1$: $x \\times w_1$\n",
    "\n",
    "Signal adjusted by neuron bias $b_1$: $x \\times w_1 + b_1 = z_1$\n",
    "\n",
    "Signal shaped by activation function $\\sigma$: $a_1 = \\sigma(z_1)$\n",
    "\n",
    "Let's look at that more closely. The $w_1$ matrix (weights) is the collection of \"synapses\" of the \"neurons\": they are the connections the neurons use to pull in data. These synapses can be increased to amplify an incoming variable, set to zero to ignore one, or made negative to invert the inbound signal. During training they are tuned by the neurons to help the NN minimize prediction error.\n",
    "\n",
    "The biases $b_1$ are unique to each neuron. They're used by the neuron to adjust what they receive.\n",
    "\n",
    "The activation function $\\sigma$, the sigmoid function, causes the neuron to output a binary signal. Unlike a digital computer though, the signal can range *between* 0 and 1 if the neuron is unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62668d85c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-10,10,100)\n",
    "plt.plot(x,1/(1 + np.exp(-x)))\n",
    "plt.title('Sigmoid activation function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Synapses strengthen or weaken incoming variables with their weights\n",
    "* Biases adjust the sum of these weighted signals\n",
    "* Neuron does a weighted sum of everything and transforms it with its activation function\n",
    "\n",
    "The weights are matrices with dimensions $in \\times out$, $in$ the size of the data coming in and $out$ the size of the data coming out. $out$ is the number of neurons in the layer, and $in$ is the amount of values each of these neurons is fed during feed-forward. The biases are $1 \\times out$, one bias for each neuron.\n",
    "\n",
    "![Dimensions](Images/intro_NeuronSimple.png \"Dimensions\")\n",
    "\n",
    "Above you can see that the input data is 3 values and that each neuron has 3 synapses ($2 \\times 3 = 6$ in total). There are 2 neurons in the layer and they produce 2 outputs, one each neuron. There are also 2 biases integrated inside the 2 neurons.\n",
    "\n",
    "Each layer has only two quantities: how much comes in and how much goes out.\n",
    "\n",
    "The first aspect of the feed-forward is then the flowing of data through weights, biases, and activation functions.\n",
    "\n",
    "### Neurons working\n",
    "\n",
    "A neural network has a dual nature: a linear nature at the unit level and a complex non-linear nature at the network level.\n",
    "\n",
    "The neurons' linear nature helps them perform computations. They each get their own copy of the data to work on. This amazing trick is possible because of matrix multiplication (or dot product). Rows don't mix with other rows, neither columns with other columns.\n",
    "\n",
    "#### Matrices are fun\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output. The five neurons' outputs are the five elements in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     x = np.array([[1, 2, 3, 4, 5]])\n",
    "\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple matrix multiply](Images/intro_matrix_identiy.png \"Simple matrix multiply\")\n",
    "\n",
    "I will give you my own example of using the neurons' synapses to play with the input data. I can perform operations on the inputs separately for each neuron. Look at each weight column vertically.\n",
    "\n",
    "For example, $(-1 \\cdot 1) + (0 \\cdot 2) + (2 \\cdot 3) + (0 \\cdot 4) + (0 \\cdot 5) = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 2, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "      x = np.array([[1,  2,  3,  4,  5]])\n",
    "\n",
    "weight = np.array([[-1,  0,  0,  0,  0],  # 1 the\n",
    "                   [ 0,  2,  0,  0,  0],  # 2 numbers\n",
    "                   [ 2,  0,  1, -1,  0],  # 3 go down\n",
    "                   [ 0,  0,  0,  0, -1],  # 4 like\n",
    "                   [ 0,  0,  0,  1,  1]]) # 5 this\n",
    "x.dot(weight)\n",
    "\n",
    "# I could also just flip the identity matrix horizontally to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Matrix algebra allows the NN to perform arithmetic.\n",
    "\n",
    "#### The non-linear mixing\n",
    "\n",
    "Data is not mixed within a layer, but it is mixed between them. Neural networks get their power from the interactions of their hidden layers.\n",
    "\n",
    "But power means nothing if it's only ever summed together in a big pile.\n",
    "\n",
    "The model is linear within a layer, but it passes through a non-linearity between layers. Neural networks get their logical \"thinking\" ability from these non-linearities.\n",
    "\n",
    "When you take a single-layer model like a logistic regression and replicate copies of it into a network, you get a model than can adapt to more than one situation.\n",
    "\n",
    "Here is an example with logic gates.\n",
    "\n",
    "![Truth tables](Images/intro_truth_table.png \"Truth tables\")\n",
    "\n",
    "* Adding: you can just summing\n",
    "* OR: you need the sigmoid to drive the output high as soon as input is present\n",
    "* NAND: you need the sigmoid to stay high until both inputs are present, then deactivate completely\n",
    "* XOR: you can't do this in one step, but you can combine OR and NAND together\n",
    "\n",
    "The feed-forward is a mixing of data over many layers of neurons. Each layer expands data into multiple copies and its neurons compress it back into a few outputs. In the diagram above you see 3 units of inputs expanded into 6 synapse signals and then collapsed into 2 output signals. The power of the neural network comes from the fact that the next layer *then copies* these 2 output signals to each of its own neurons, so everything affects everything.\n",
    "\n",
    "![Colorful](Images/intro_Colorful.png \"Colorful\")\n",
    "\n",
    "As you can see in the example above, data expands and contracts. It is also generously connected.\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive committees. The neurons in a layer form a committee that looks at data together, performs analysis, and then summarizes its findings into a small report. Higher committees then analyze this report at a higher level, and so on. The final output layer makes a decision based on the accumulated wisdom of the executive summary it receives: it outputs a single value between 0 and 1.\n",
    "\n",
    "In the committee example, the office workers use\n",
    "\n",
    "* weights to increase, decrease or invert the importance of data\n",
    "* biases to make their voices louder or weaker\n",
    "* activation functions to simply their reports into a range [0,1]\n",
    "\n",
    "The expansion and contraction of information is repeated multiple times in the neural network.\n",
    "\n",
    "All this mixing allows the neural network to work with very complicated data.\n",
    "\n",
    "### Hiddens layers feed-forwarding\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "1. $z_1 = X W_1 + B_1$\n",
    "2. $a_1 = \\sigma(z_1)$\n",
    "3. $z_2 = a_1 W_2 + B_2$\n",
    "4. $a_2 = \\sigma(z_2)$\n",
    "5. $z_{output} = a_2 W_{output} + B_{output}$\n",
    "6. $a_{output} = \\sigma(z_{output})$\n",
    "\n",
    "By way of comparison, here is Andrew Ng's notation.\n",
    "\n",
    "1. $a^{(1)} = x$\n",
    "2. $z^{(2)} = \\theta^{(1)} a^{(1)})$\n",
    "3. $a^{(2)} = g(z^{(2)})$\n",
    "2. $z^{(3)} = \\theta^{(2)} a^{(2)})$\n",
    "3. $a^{(3)} = g(z^{(3)})$\n",
    "2. $z^{(4)} = \\theta^{(3)} a^{(3)})$\n",
    "6. $a_{(4)} = h_{\\theta}(x)=g(z^{(4)})$\n",
    "\n",
    "Let's generate some data. Thanks to the properties of matrix multiplication, I can have 4 rows of input data and these will be processed fully separately, yielding 4 rows of output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random((4,5)) # Four records of 5 variables\n",
    "b1 = np.random.random((1,3)) # Bias: 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # Weight: input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # Bias: 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # Weight: layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # Bias: 1 x output_size\n",
    "w_out = np.random.random((2,1)) # Weight: layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the activations of all the layers participating in the feed forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83836843  0.91233646  0.79597908]\n",
      " [ 0.92638387  0.93010051  0.83676269]\n",
      " [ 0.91310152  0.95530761  0.87179181]\n",
      " [ 0.92764     0.93849366  0.88610275]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = x.dot(w1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88372231  0.93019092]\n",
      " [ 0.88914692  0.9364722 ]\n",
      " [ 0.89342689  0.9382375 ]\n",
      " [ 0.89353883  0.93868309]]\n"
     ]
    }
   ],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87086418]\n",
      " [ 0.87187312]\n",
      " [ 0.87243173]\n",
      " [ 0.87247545]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation I\n",
    "\n",
    "Backprop is how the network evaluates its performance during feed forward.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "If the feed-forward is data pushed all the way forward to the outputs, then back-propagation is the trickling down of errors flowing back from the outputs all the way back to the very first hidden layer.\n",
    "\n",
    "Backprop is a way of using gradient descent on neural networks of multiple layers. It isn't necessary with a linear or logistic regression because these are simple single-layer networks. Back-propagation allows you to apply gradient descent more than once.\n",
    "\n",
    "It all starts at the output. Here there is a clear link between the choice of parameters (weights and biases) and the output error. The approach here is the same as simple gradient descent.\n",
    "\n",
    "At the layer preceeding the output, we'll call it $l_2$, there is an extra step. What is the link between $l_2$ weights and biases and the output error? It has multiple steps: $l_2$ has a direct effect on the output layer's data, and the output layer's data has a direct effect on what the model decides to output. It takes two steps to get back to the end.\n",
    "\n",
    "In other words, the output layer is the boss and it is directly responsible for the model's error. If the output layer changes its behaviour, it can directly improve its accuracy. It's the easiest to train.\n",
    "\n",
    "The hidden layers are not directly responsible for the model's error; however, they are responsible for providing the output layer accurate analyses of the model's input data. Knowing their boss, they have an idea of how to change their computations so that the big cheese makes more informed decisions. Their gradient formulas in fact depend on the output layer's weights (the boss's personality, you might say).\n",
    "\n",
    "## Detour: gradient checking\n",
    "\n",
    "Backprop takes snapshots of errors everywhere in the NN and uses these to adjust parameters. Normally this is done with calculus and repeated applications of the chain rule of differentiation.\n",
    "\n",
    "Backprop can also be done by more primitive methods, albeit much more slowly. Numerical differentiation is used to teach students calculus, so it makes sense to show it here first before breaking out the chain rule.\n",
    "\n",
    "The idea behind numerical differentiation is this: \n",
    "\n",
    "1. Take your NN as is\n",
    "2. Adjust a parameter slightly and see the effect on output error\n",
    "3. You now know the effect of that parameter on error\n",
    "\n",
    "Given $J(\\theta_{i,j})$ your cost function, $\\theta$ any parameter anywhere in the neural network, and $\\epsilon$ a small value as a \"nudge\", \n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i,j}} \\approx \\frac{J(\\theta_{i,j} + \\epsilon) - J(\\theta_{i,j} - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "Backprop does this for every parameter in the NN. If it sees that error increases when a parameter is increased, it will decrease the parameter. The opposite is done with a parameter that increases error when it decreases.\n",
    "\n",
    "Ultimately, this makes a neural network more complicated than any collection of corporate committees. Except in rare prophetic instances, an office worker will not know how many dollars their actions win/lose their company. With neural networks though, it is calculated.\n",
    "\n",
    "We will incorporate gradient checking into a NN later.\n",
    "\n",
    "## Back propagation II\n",
    "\n",
    "I hope the above has made back propagation make some sense. It's now time for some light mathematics. Don't worry I'll just paste the answers and skip the algebraic Tetris.\n",
    "\n",
    "Here are the gradients with respect to error for parameters in the NN model.\n",
    "\n",
    "**Note 1:** With these equations, the most important part is whether they're positive or negative, so you can look at them to see what affects their sign. Gradient decsent will generally work alright as long as it's heading in the right direction (has the right sign).\n",
    "\n",
    "**Note 2:** When a gradient is positive (error is increasing with parameter) you want to decrease the parameter. When the gradient is negative, you want to increase the parameter.\n",
    "\n",
    "The gradients for the output layer weights and biases are\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial W_{out}} = a_2^T \\cdot (a_{out} - y), \\frac{\\partial J(\\theta)}{\\partial B_{out}} = 1 \\cdot (a_{out} - y)$$\n",
    "\n",
    "The above equations make some sense. If the output neuron is overshooting the target, reduce the bias. It's a similar idea with the weights: if the weights cause the neuron to overshoot when they are given a positive input, they need to be reduced.\n",
    "\n",
    "(You need the 1 in the bias gradient. It represents the intercept but is also necessary to get the right dimension.)\n",
    "\n",
    "To proceed lower into the previous layer, we have to do some backprop. Here it is:\n",
    "\n",
    "$$\\delta_{out} = (a_{out} - y) \\cdot W_{out}^T$$\n",
    "\n",
    "We also need the derivative of the sigmoid function. We'll just call it $\\sigma^\\prime$.\n",
    "\n",
    "We just have to include that in our equations and we'll be fine. The gradients for the second hidden layer are:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial W_2} = a_1^T \\cdot (\\sigma^\\prime(a_2) \\circ \\delta_{out}), \\frac{\\partial J(\\theta)}{\\partial B_2} = 1 \\cdot \\sigma^\\prime(a_2) \\circ \\delta_{out}$$\n",
    "\n",
    "The sigmoid derivative $\\sigma^\\prime$ is a newcomer, but otherwise these are similar to before. The weight gradients depend on layer 2's input, which comes from layer 1. The bias gradient is simpler, but it still has to pass through the $\\sigma^\\prime$ and the $\\delta_{out}$.\n",
    "\n",
    "For layer 1 we need a new delta.\n",
    "\n",
    "$$\\delta_2 = (\\delta_{out} \\circ \\sigma^\\prime(a_2)) \\cdot W_2^T$$\n",
    "\n",
    "Finally, the last backprop step.\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial W_1} = x^T \\cdot (\\sigma^\\prime(a_1) \\circ \\delta_2), \\frac{\\partial J(\\theta)}{\\partial B_1} = 1 \\cdot \\sigma^\\prime(a_1) \\circ \\delta_2$$\n",
    "\n",
    "That's all there is to it.\n",
    "\n",
    "## Gradient interpretation\n",
    "\n",
    "In this section, I'll do my best to narrate what back propagation is doing. Feel free to skip this section.\n",
    "\n",
    "Back propagation is a repeated application of the chain-rule of differentiation, and its purpose is to determine the effect of a parameter on model error, which is the error gradient with respect to the parameter ($W_1$, $B_{out}$, etc).\n",
    "\n",
    "Let's take the weight update below as an example, starting with the $a_1^T$ term.\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial W_2} = a_1^T \\cdot (\\sigma^\\prime(a_2) \\circ \\delta_{out})$$\n",
    "\n",
    "Recall that $W_2$'s role in the neural network is to do the following:\n",
    "\n",
    "$$a_2 = \\sigma(a_1 W_2 + B_2), z_2 = a_1 W_2 + B_2$$\n",
    "\n",
    "Let's put everything together by answering the question: *how does $W_2$ affect $z_2$?*\n",
    "\n",
    "The answer: *$W_2$ affects $z_2$ through its interaction with $a_1$.*\n",
    "\n",
    "The derivative $\\frac{\\partial z_2}{\\partial W_2} = \\frac{\\partial a_1 W_2 + B_2}{\\partial W_2} = a_1$ signifies that $z_2$ increases by $a_1$ when $W_2$ increases by 1. This works out nicely here since $a_1 W_2$ is linear; normally though the derivative \"slope\" only holds in a very small area around the current point.\n",
    "\n",
    "This is where the $a_1^T$ in the gradient comes from. What does it mean? It means that $W_2$'s job is to multiply $a_1$, so its contribution to the model output is $a_1$. Since model error is closely related to model output (through the cost function), $a_1$ is also $W_2$'s contribution to model error.\n",
    "\n",
    "That covers $a_1^T$.\n",
    "\n",
    "Let's now look at that $\\sigma^\\prime(a_2)$ term.\n",
    "\n",
    "$\\sigma^\\prime(a_2)$ is the sigmoid's contribution to error. $W_2$'s contribution to error, seen above, passes through the simgoid prime. The derivative of the sigmoid function is $\\sigma(a_2)^\\prime = \\sigma(a_2) (1 - \\sigma(a_2))$. Looking at it a bit, it becomes apparent that the derivative reaches its maximum value when $\\sigma(a_2) = 1 - \\sigma(a_2)$, or when $\\sigma(a_2) = 0.5$ or $a_2 = 0$. The slope of the derivative vanishes when $a_2$ approaches 0 or 1. Thus the sigmoid's contribution to error: it restricts the flow of error depending on the value of $a_2$ fed to it, $\\sigma^\\prime(a_2)$.\n",
    "\n",
    "Story so far: $\\frac{\\partial J(\\theta)}{\\partial W_2}$ is $a_1$ passed through $\\sigma^\\prime$, the latter at most being 0.5 but possibly 0.0.\n",
    "\n",
    "The next component of $W_2$'s gradient is $\\delta_{out} = (a_{out} - y) \\cdot W_{out}^T$.\n",
    "\n",
    "We are seeing $W_{out}$ here because $W_2$'s effect must pass through it to reach the model error. The idea here is that stronger $W_{out}$ values mean that whatever $x W$ outputs will be mangnified, while weaker $W_{out}$ will attenuate $W_2$'s influence. Therefore $W_{out}$ is a part of $W_2$'s effect on error.\n",
    "\n",
    "$(a_{out} - y)$ is more difficult to explain because it is an algebraic simplication. Its full form is $\\frac{\\partial NLL_{cost} \\sigma(a_{out})}{\\partial a_{out}}$. But that isn't too important. The first role of $a_{out} - y$ is to keep the gradient positive if $a_{out} > y$ but turn it negative if $a_{out} < y$: this makes sense since you want to increase/lower $a_{out}$ if it has undershot/overshot y. The second role of $a_{out} - y$ is to return a higher error value the wider a gap there is between model output and true output: this gap resides in $[-1, 1]$ since all outputs belong in $[0, 1]$.\n",
    "\n",
    "There you have it. I hope this has helped you understand gradients a little bit better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
