{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Backprop guide: a cleaner numpy implementation](#10)\n",
    "2. [Building the classes](#20)\n",
    "\t1. [Simplest layers](#21)\n",
    "\t2. [Layers that learn](#22)\n",
    "\t3. [The network itself, as a class](#23)\n",
    "\t4. [Setting up \"cool\" layers: ReLUs, noise, dropout, etc.](#24)\n",
    "3. [Testing the network on real data](#30)\n",
    "\t1. [First, testing that things work](#31)\n",
    "\t2. [Gradient checking](#32)\n",
    "\t3. [Testing the model on the mini-MNIST dataset](#33)\n",
    "\t4. [Testing the model on the full MNIST dataset](#34)\n",
    "\n",
    "# Backprop guide: a cleaner numpy implementation <a name=\"10\"></a>\n",
    "\n",
    "I've already made a simple feed forward neural network using Numpy; however, it's a lot cleaner to use a object-oriented design. I'll work that out here.\n",
    "\n",
    "I will define the various neural network layers as classes. I'll use inheritence here, but it's not really crucial. I just want to make sure that all the layer classes follow some sort of common style and logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For plotting things\n",
    "from sklearn.datasets import load_digits # Data to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classes <a name=\"20\"></a>\n",
    "\n",
    "## Simplest layers <a name=\"21\"></a>\n",
    "\n",
    "Let's start with an identify layer. You can think of as a sigmoid or a ReLU activation function layer, except that the identity layer does nothing at all, $f(x) = y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityActivation:\n",
    "    def __init__(self, predecessor):\n",
    "        # Remember what precedes this layer\n",
    "        self.predecessor = predecessor\n",
    "        # The activation function keeps the dimensions of its predecessor\n",
    "        self.input_size = self.predecessor.output_size        \n",
    "        self.output_size = self.input_size\n",
    "        # Create an empty matrix to store this layer's last activation\n",
    "        self.activation = np.zeros(self.output_size)\n",
    "        # Initialize weights, if necessary\n",
    "        self.init_weights()\n",
    "        # Will never require a gradient\n",
    "        self.require_gradient = False\n",
    "    # This activation function has no parameters, so pass\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "    # It also has to gradients, so pass here too\n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    # During a forward pass, it just passes its input forward unmodified\n",
    "    def forward(self, x, evaluate=False):\n",
    "        # Save a copy of the activation\n",
    "        self.activation = x\n",
    "        return self.activation\n",
    "    # During backrop it passes the delta on unmodified\n",
    "    def backprop(self, delta, y):\n",
    "        return delta\n",
    "    # It has no parameters\n",
    "    def report_params(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above doesn't really do anything except let me develop a blueprint for the rest of my classes. I definitely need `forward`, `backprop`, and `update` methods for all my layers. It's also nice to have some `identify` method to make debugging easier.\n",
    "\n",
    "Actually, here are all the requirements:\n",
    "* `params()` method that dumps its parameters to the console\n",
    "* `name()` method for dumping its properties to the console\n",
    "* `init_weights()` when you want its parameters initialized\n",
    "* `zero_grad()` when you want to zero its saved gradients\n",
    "* `forward()` when you want it to estimate input\n",
    "* `backprop()` when you want it to backpropagate error\n",
    "* `report_params()` return a list of all of its parameters\n",
    "\n",
    "What is not included\n",
    "* `update()` when you want it to update its parameters: this should be done by an optimizer since you want to choose between SGD, adagrad, Adam, etc.\n",
    "\n",
    "It's nice to have all of these as manual methods because you can start taking apart the NN to do a few things. For example, if I want to do a gradient check, I just want to calculate the gradients without actually updating them. When I'm done checking, I can call `zero_grad` and nothing will have changed.\n",
    "\n",
    "The Input layer is nothing really special. It just has to take a size (of features, variables), and otherwise it just feeds data to the first hidden layer, so it turns out it's pretty much just an identity activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InputLayer(IdentityActivation):\n",
    "    def __init__(self, input_size):\n",
    "        # The size here is determined by the data that's going to be used\n",
    "        self.input_size = (0, input_size)\n",
    "        self.output_size = self.input_size\n",
    "        # Create an empty matrix to store this layer's last activation\n",
    "        self.activation = np.zeros(self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our hand at a sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidActivation(IdentityActivation):\n",
    "    # During a forward pass, it just applies the sigmoid function\n",
    "    def forward(self, x, evaluate=False):\n",
    "        self.activation = 1.0/(1.0+np.exp(-x))\n",
    "        return self.activation\n",
    "    # During backprop, it passes the delta through its derivative\n",
    "    def backprop(self, delta, y):\n",
    "        return delta * self.activation * (1 - self.activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers that learn <a name=\"22\"></a>\n",
    "\n",
    "Let's try a dense layer next.\n",
    "\n",
    "I am putting in two useful things here. The `use_bias` flag is pretty easy as a way of disabling the bias term, which is sometimes necessary. The `require_gradient` can be useful in some situations, such as freezing a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way of dealing with parameters is with a `Variable` class. This is how torch and pytorch do it: I'll keep the same names since there's no point messing with what works.\n",
    "\n",
    "We need to do operator overloading. [You can find a list of the operators you can define here.](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types) *However, you cannot override the assignment operator `=`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.shape = self.data.shape\n",
    "        self.grad = np.zeros(self.shape)\n",
    "    # Cheap way of zeroing its gradient\n",
    "    def zero_grad(self):\n",
    "        self.grad *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLayer(IdentityActivation):\n",
    "    def __init__(self, predecessor, hidden, use_bias=True, require_gradient=True, positive_params=False):\n",
    "        # Remember what precedes this layer\n",
    "        self.predecessor = predecessor\n",
    "        self.input_size = self.predecessor.output_size\n",
    "        self.hidden = hidden\n",
    "        self.output_size = (0, self.hidden)\n",
    "        # It is possible that we don't want a bias term\n",
    "        self.use_bias = use_bias\n",
    "        # If you need non-negative parameters\n",
    "        self.positive_params = positive_params\n",
    "        # Save its activation\n",
    "        self.activation = np.zeros(self.output_size)\n",
    "        # Initialize the weights and biases\n",
    "        self.init_params()\n",
    "        # Most of the time, this layer will use gradients to train itself\n",
    "        # However, this can be disabled manually\n",
    "        self.require_gradient = require_gradient\n",
    "    def zero_grad(self):\n",
    "        self.weight.zero_grad()\n",
    "        if self.use_bias:\n",
    "            self.bias.zero_grad()\n",
    "    def init_params(self):\n",
    "        size_measure = self.input_size[1]\n",
    "        if self.positive_params:\n",
    "            lower, upper = 0., 0.5\n",
    "        else:\n",
    "            lower, upper = -1., 1.\n",
    "        # Weights are initialized by a normal distribution\n",
    "        self.weight = Variable(\n",
    "            np.sqrt(2/size_measure) * np.random.uniform(lower, upper, size=(self.input_size[1], self.hidden))\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bias = Variable(\n",
    "                np.sqrt(2/size_measure) * np.random.uniform(lower, upper, size=(1, self.hidden))\n",
    "            )\n",
    "    # The forward pass is a matrix multiplication, with optional bias\n",
    "    def forward(self, x, evaluate=False):\n",
    "        x = x @ self.weight.data\n",
    "        if self.use_bias:\n",
    "            x += self.bias.data\n",
    "        self.activation = x\n",
    "        return self.activation\n",
    "    # The delta just needs to be multipled by the layer's weight\n",
    "    def backprop(self, delta, y):\n",
    "        # Only calculate gradients if it's required\n",
    "        if self.require_gradient:\n",
    "            # The weight update requires the previous layer's activation\n",
    "            self.weight.grad += self.predecessor.activation.transpose() @ delta\n",
    "            # The bias update requires the delta to be \"squished\"\n",
    "            # This can be done by multiplying by a vector of 1s\n",
    "            if self.use_bias:\n",
    "                self.bias.grad += np.ones((1, delta.shape[0])) @ delta\n",
    "        return delta @ self.weight.data.transpose()\n",
    "    # This DenseLayer is the first example of a layer with parameters\n",
    "    def report_params(self):\n",
    "        if self.use_bias:\n",
    "            return [self.bias, self.weight]\n",
    "        else:\n",
    "            return [self.weight]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright... Now we must attempt an output layer, particulary a **softmax output activation** combined with a **dense layer**. We combine these because the math collapses the two during backprop, making things more efficient.\n",
    "\n",
    "This will be a special layer because it will deliver predictions during feed forward and also trigger the start of backprop.\n",
    "\n",
    "We want to run feed forward and backprop with for-loops. For feed forward, it will just have to output the proper `x`, which is not overly difficult. For backprop, it will require its own activations (easy) and the true outputs `y` (which makes things a bit clunky).\n",
    "\n",
    "This will be an opportunity to use some nice inheritence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidNLL(DenseLayer):\n",
    "    # The forward pass is a matrix multiplication, with optional bias\n",
    "    def forward(self, x, evaluate=False):\n",
    "        # The feed forward starts off normal\n",
    "        x = x @ self.weight.data\n",
    "        if self.use_bias:\n",
    "            x += self.bias.data\n",
    "        # It changes when we apply the sigmoid\n",
    "        self.activation = 1.0/(1.0+np.exp(-x))\n",
    "        return self.activation\n",
    "    # The delta is started here\n",
    "    def backprop(self, delta, y):\n",
    "        # Starting the delta\n",
    "        delta = self.activation - y\n",
    "        # The update is the same as a DenseLayer\n",
    "        self.weight.grad += self.predecessor.activation.transpose() @ delta\n",
    "        if self.use_bias:\n",
    "            self.bias.grad += np.ones((1, delta.shape[0])) @ delta\n",
    "        # The delta is passed backwards like a Denselayer\n",
    "        return delta @ self.weight.data.transpose()\n",
    "\n",
    "class SoftmaxCrossEntropy(DenseLayer):\n",
    "    # The forward pass is a matrix multiplication, with optional bias\n",
    "    def forward(self, x, evaluate=False):\n",
    "        # The feed forward starts off normal\n",
    "        x = x @ self.weight.data\n",
    "        if self.use_bias:\n",
    "            x += self.bias.data\n",
    "        # It changes when we apply the sigmoid\n",
    "        softmax_sum = np.exp(x) @ np.ones((self.hidden, 1))\n",
    "        self.activation = np.exp(x) / softmax_sum\n",
    "        return self.activation\n",
    "    # The delta is started here\n",
    "    def backprop(self, delta, y):\n",
    "        # Starting the delta\n",
    "        delta = self.activation - y\n",
    "        # The update is the same as a DenseLayer\n",
    "        self.weight.grad += self.predecessor.activation.transpose() @ delta\n",
    "        if self.use_bias:\n",
    "            self.bias.grad += np.ones((1, delta.shape[0])) @ delta\n",
    "        # The delta is passed backwards like a Denselayer\n",
    "        return delta @ self.weight.data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network itself, as a class <a name=\"23\"></a>\n",
    "\n",
    "I will set up a NeuralNetwork class very, very simplistically. It will just be a list that you append to. Nevertheless, I prefer this way because it forces you to be verbose when you're stacking your layers.\n",
    "\n",
    "The `FeedForward` method is very simple though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class stores a list of layers for training\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    def feed_forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    # When you want the model to know you're not training\n",
    "    def evaluate(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, evaluate=True)\n",
    "        return x\n",
    "    def back_propagation(self, y):\n",
    "        delta = np.zeros((0,0))\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backprop(delta, y)\n",
    "    def step(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)\n",
    "    def zero_gradients(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this network to work, it will require an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, list_of_layers):\n",
    "        self.list_of_layers = list_of_layers\n",
    "        self.list_of_variables = []\n",
    "        for layer in self.list_of_layers:\n",
    "            self.list_of_variables += layer.report_params()\n",
    "    def step(self, lr):\n",
    "        for variable in self.list_of_variables:\n",
    "            variable.data -= lr * variable.grad\n",
    "\n",
    "class AdaGradOptimizer:\n",
    "    def __init__(self, list_of_layers):\n",
    "        self.list_of_layers = list_of_layers\n",
    "        self.list_of_variables = []\n",
    "        for layer in self.list_of_layers:\n",
    "            self.list_of_variables += layer.report_params()\n",
    "        self.gradient_histories = dict()\n",
    "        for variable in self.list_of_variables:\n",
    "            self.gradient_histories[variable] = np.ones(variable.shape)\n",
    "    def step(self, lr):\n",
    "        for variable in self.list_of_variables:\n",
    "            variable.data -= (lr / np.sqrt(self.gradient_histories[variable])) * variable.grad\n",
    "            self.gradient_histories[variable] += variable.grad**2\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, list_of_layers, beta1=0.9, beta2=0.999, eps=0.00000001):\n",
    "        self.list_of_layers = list_of_layers\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.list_of_variables = []\n",
    "        for layer in self.list_of_layers:\n",
    "            self.list_of_variables += layer.report_params()\n",
    "        self.adam_mean = dict()\n",
    "        self.adam_var = dict()\n",
    "        for variable in self.list_of_variables:\n",
    "            self.adam_mean[variable] = np.zeros(variable.shape)\n",
    "            self.adam_var[variable] = np.zeros(variable.shape)\n",
    "    def step(self, lr):\n",
    "        for variable in self.list_of_variables:\n",
    "            self.adam_mean[variable] = self.adam_mean[variable] * self.beta1 + variable.grad * (1 - self.beta1)\n",
    "            self.adam_var[variable] = self.adam_var[variable] * self.beta2 + (variable.grad**2) * (1 - self.beta2)\n",
    "            mean_hat = self.adam_mean[variable] / (1 - self.beta1)\n",
    "            var_hat = self.adam_var[variable] / (1 - self.beta2)\n",
    "            variable.data -= (lr * mean_hat)/(np.sqrt(var_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up \"cool\" layers: ReLUs, noise, dropout, etc. <a name=\"24\"></a>\n",
    "\n",
    "Lets set up a ReLU. https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy\n",
    "\n",
    "I also want a softplus layer. Because I like them. https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLUActivation(IdentityActivation):\n",
    "    # During a forward pass, it just applies the sigmoid function\n",
    "    def forward(self, x, evaluate=False):\n",
    "        self.activation = x * (x > 0)\n",
    "        return self.activation\n",
    "    # During backprop, it passes the delta through its derivative\n",
    "    def backprop(self, delta, y):\n",
    "        return delta * (self.activation > 0)\n",
    "\n",
    "class SoftplusActivation(IdentityActivation):\n",
    "    # During a forward pass, it just applies the sigmoid function\n",
    "    def forward(self, x, evaluate=False):\n",
    "        self.activation = np.log(1.0 + np.exp(x))\n",
    "        return self.activation\n",
    "    # During backprop, it passes the delta through its derivative\n",
    "    def backprop(self, delta, y):\n",
    "        return delta * 1.0/(1.0 + np.exp(-self.activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set up a noise-generating layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(IdentityActivation):\n",
    "    def __init__(self, predecessor, mean=0, stdev=0.1):\n",
    "        self.predecessor = predecessor\n",
    "        self.input_size = self.predecessor.output_size        \n",
    "        self.output_size = self.input_size\n",
    "        self.activation = np.zeros(self.output_size)\n",
    "        self.gradient = np.zeros(self.output_size)\n",
    "        self.init_weights()\n",
    "        self.require_gradient = False\n",
    "        # Noise\n",
    "        self.mean = mean\n",
    "        self.stdev=stdev\n",
    "    def forward(self, x, evaluate=False):\n",
    "        if evaluate:\n",
    "            self.activation = x\n",
    "        else:\n",
    "            noise = np.random.normal(loc=self.mean, scale=self.stdev, size=x.shape)\n",
    "            self.activation = x + noise\n",
    "        return self.activation\n",
    "    \n",
    "class DropoutLayer(IdentityActivation):\n",
    "    def __init__(self, predecessor, probability=0.5):\n",
    "        self.predecessor = predecessor\n",
    "        self.input_size = self.predecessor.output_size        \n",
    "        self.output_size = self.input_size\n",
    "        self.activation = np.zeros(self.output_size)\n",
    "        self.gradient = np.zeros(self.output_size)\n",
    "        self.init_weights()\n",
    "        self.require_gradient = False\n",
    "        # Noise\n",
    "        self.probability = probability\n",
    "    def forward(self, x, evaluate=False):\n",
    "        if evaluate:\n",
    "            self.activation = x\n",
    "        else:\n",
    "            dropout = np.random.choice([0, 1], size=x.shape, p=[self.probability, 1 - self.probability])\n",
    "            self.activation = (x * dropout)/(1 - self.probability)\n",
    "        return self.activation\n",
    "    \n",
    "class RollLayer(IdentityActivation):\n",
    "    def __init__(self, predecessor, list_of_offsets=(-1, 0, 1)):\n",
    "        self.predecessor = predecessor\n",
    "        self.input_size = self.predecessor.output_size        \n",
    "        self.output_size = self.input_size\n",
    "        self.activation = np.zeros(self.output_size)\n",
    "        self.gradient = np.zeros(self.output_size)\n",
    "        self.init_weights()\n",
    "        self.require_gradient = False\n",
    "        # Noise\n",
    "        self.list_of_offsets = list_of_offsets\n",
    "    def forward(self, x, evaluate=False):\n",
    "        if evaluate:\n",
    "            self.activation = x\n",
    "        else:\n",
    "            offset = np.random.choice(self.list_of_offsets, 1)\n",
    "            self.activation = np.roll(x, offset, axis=1)\n",
    "        return self.activation\n",
    "    # Probably requires a backprop if it isn't at input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing batchnorm is a bit trickier. [This blog post helped.](https://wiseodd.github.io/techblog/2016/07/04/batchnorm/) However you really just need to read the original paper (which I didn't have the time to but should have!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchNormLayer(IdentityActivation):\n",
    "    def __init__(self, predecessor, eps=0.01):\n",
    "        self.predecessor = predecessor\n",
    "        self.input_size = self.predecessor.output_size\n",
    "        self.output_size = self.input_size\n",
    "        self.hidden = self.output_size[1]\n",
    "        self.activation = np.zeros(self.output_size)\n",
    "        self.gradient = np.zeros(self.output_size)\n",
    "        self.init_params()\n",
    "        self.zero_grad()\n",
    "        self.require_gradient = True\n",
    "        # Batchnorm requires a constant for \"numerical stability\"\n",
    "        self.eps = eps\n",
    "        # We need to save mean and variance as constants for backprop\n",
    "        self.mean = np.zeros((1, self.hidden))\n",
    "        self.var = np.zeros((1, self.hidden))\n",
    "        # Also, save the xhat\n",
    "        self.xhat = np.zeros((1, self.hidden))\n",
    "        # We also want to keep running means and variances during training\n",
    "        # These become evaluation statistics\n",
    "        self.eval_mean = np.zeros((1, self.hidden))\n",
    "        self.eval_var = np.zeros((1, self.hidden))\n",
    "    def init_params(self):\n",
    "        # Initialize gamma (mean) and beta (variance)\n",
    "        self.gamma = Variable(np.ones((1, self.hidden)))\n",
    "        self.beta = Variable(np.zeros((1, self.hidden)))\n",
    "    def zero_grad(self):\n",
    "        self.gamma.zero_grad()\n",
    "        self.beta.zero_grad()\n",
    "    def forward(self, x, evaluate=False):\n",
    "        if evaluate:\n",
    "            xhat = (x - self.eval_mean) / np.sqrt(self.eval_var + self.eps)\n",
    "            self.activation = self.gamma.data * xhat + self.beta.data            \n",
    "        else:\n",
    "            # Batch mean and variance\n",
    "            self.mean = np.mean(x, axis=0)\n",
    "            self.var = np.var(x, axis=0)\n",
    "            # Evaluation mean and variance\n",
    "            self.eval_mean = 0.9*self.eval_mean + 0.1*self.mean\n",
    "            self.eval_var = 0.9*self.eval_var + 0.1*self.var\n",
    "            # Calculate xhat and the final normalized activation\n",
    "            self.xhat = (x - self.mean) / np.sqrt(self.var + self.eps)\n",
    "            self.activation = self.gamma.data * self.xhat + self.beta.data\n",
    "        return self.activation\n",
    "    def backprop(self, delta, y):\n",
    "        N = delta.shape[0]\n",
    "\n",
    "        self.gamma.grad += np.sum(delta * self.xhat, axis=0)\n",
    "        self.beta.grad += np.sum(delta, axis=0)\n",
    "\n",
    "        x_mean = self.predecessor.activation - self.mean\n",
    "        inv_var_eps = 1 / np.sqrt(self.var + self.eps)\n",
    "\n",
    "        d_xhat = delta * self.gamma.data\n",
    "        d_var = np.sum(d_xhat * x_mean, axis=0) * -0.5 * inv_var_eps**3\n",
    "        d_mean = np.sum(d_xhat * -inv_var_eps, axis=0) + (d_var * np.mean(-2.0 * x_mean))\n",
    "        delta = (d_xhat * inv_var_eps) + (d_var * 2 * x_mean / N) + (d_mean / N)\n",
    "\n",
    "        return delta\n",
    "    def report_params(self):\n",
    "        return [self.beta, self.gamma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the network on real data <a name=\"30\"></a>\n",
    "## First, testing that things work <a name=\"31\"></a>\n",
    "\n",
    "The simplest way to test a new network is to feed it what amounts to a truth table and see if it can recreate it.\n",
    "\n",
    "Below is a list of four 2-bit combinations, `binary_inputs`, and the outputs for `or`, `and`, `nand`, and `xor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "or_outputs = np.array([[0], [1], [1], [1]])\n",
    "and_outputs = np.array([[0], [0], [0], [1]])\n",
    "nand_outputs = np.array([[1], [1], [1], [0]])\n",
    "xor_outputs = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "current_outputs = xor_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New network\n",
    "BinaryNet = NeuralNetwork()\n",
    "\n",
    "# A very simple network\n",
    "BinaryNet.layers.append(InputLayer(input_size=2))\n",
    "BinaryNet.layers.append(DenseLayer(predecessor=BinaryNet.layers[-1], hidden=2, use_bias=True))\n",
    "BinaryNet.layers.append(SigmoidActivation(predecessor=BinaryNet.layers[-1]))\n",
    "BinaryNet.layers.append(SigmoidNLL(predecessor=BinaryNet.layers[-1], hidden=1, use_bias=True))\n",
    "\n",
    "# The SGD optimizer\n",
    "BinaryNet.optimizer = SGDOptimizer(BinaryNet.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the FeedForward on a bunch of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Got:\n",
      "[[ 0.002]\n",
      " [ 0.999]\n",
      " [ 0.999]\n",
      " [ 1.   ]]\n",
      "Expected:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "Got:\n",
      "[[ 0.   ]\n",
      " [ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.999]]\n",
      "Expected:\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Got:\n",
      "[[ 0.999]\n",
      " [ 0.999]\n",
      " [ 0.999]\n",
      " [ 0.002]]\n",
      "Expected:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Got:\n",
      "[[ 0.003]\n",
      " [ 0.999]\n",
      " [ 0.999]\n",
      " [ 0.001]]\n"
     ]
    }
   ],
   "source": [
    "# We'll try each of the possibilities\n",
    "for output in [or_outputs, and_outputs, nand_outputs, xor_outputs]:\n",
    "    # Reset the network before starting\n",
    "    for layer in BinaryNet.layers:\n",
    "        layer.init_weights()\n",
    "    # Train it a lot so that it gets to converge\n",
    "    for i in range(10000):\n",
    "        BinaryNet.feed_forward(binary_inputs)\n",
    "        BinaryNet.back_propagation(output)\n",
    "        BinaryNet.optimizer.step(0.1)\n",
    "        BinaryNet.zero_gradients()\n",
    "    print(\"Expected:\")\n",
    "    print(output)\n",
    "    print(\"Got:\")\n",
    "    print(np.around(BinaryNet.feed_forward(binary_inputs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking <a name=\"32\"></a>\n",
    "\n",
    "This may be a chore to set up, but it ends up being useful for debugging. In fact, the code above was not working at first, and gradient checking was able to determine that neither the output layers or the dense layers were to blame: by process of elimination, I finally found the typo in the `SigmoidActivation` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NNet = NeuralNetwork()\n",
    "\n",
    "NNet.layers.append(InputLayer(input_size=64))\n",
    "NNet.layers.append(DenseLayer(predecessor=NNet.layers[-1], hidden=32, use_bias=True))\n",
    "NNet.layers.append(BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(DenseLayer(predecessor=NNet.layers[-1], hidden=32, use_bias=True))\n",
    "NNet.layers.append(ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(SoftmaxCrossEntropy(predecessor=NNet.layers[-1], hidden=10, use_bias=True))\n",
    "\n",
    "NNet.optimizer = AdaGradOptimizer(NNet.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some loss functions. For the time being, these are separate from everything else. Ideally, we'd have a setup like in pytorch and TF where these are optimized and integrated.\n",
    "\n",
    "I will have to read up on how they did it. Sounds interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NLLCost(prediction, truth):\n",
    "    return -np.mean(np.sum(truth*np.log(prediction) + (1.0-truth)*np.log(1.0-prediction), 1))\n",
    "\n",
    "def CrossEntropy(prediction, truth):\n",
    "    return -np.mean(np.sum(truth*np.log(prediction), 1))\n",
    "\n",
    "def accuracy(prediction, labels):\n",
    "    tests = prediction.argmax(axis=1) == labels\n",
    "    return(tests.sum() / prediction.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checking is the best way to make sure your back-propagation is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Early gradient-checking code\n",
    "#\n",
    "digits = load_digits(n_class=10, return_X_y=True)\n",
    "x = digits[0]/16\n",
    "y = np.eye(10)[digits[1]]\n",
    "\n",
    "def accuracy(a_out, labels):\n",
    "    tests = a_out.argmax(axis=1) == labels\n",
    "    return(tests.sum() / a_out.shape[0])\n",
    "\n",
    "batch_x = x[0:1]\n",
    "batch_y = y[0:1]\n",
    "\n",
    "import copy\n",
    "\n",
    "layer_number = -1\n",
    "row = np.random.randint(0, NNet.layers[layer_number].weight.shape[0])\n",
    "col = np.random.randint(0, NNet.layers[layer_number].weight.shape[1])\n",
    "\n",
    "# create two copies of your NN\n",
    "negative_copy = copy.deepcopy(NNet)\n",
    "positive_copy = copy.deepcopy(NNet)\n",
    "\n",
    "# nudge them\n",
    "negative_copy.layers[layer_number].weight.data[row,col] -= 0.0001\n",
    "positive_copy.layers[layer_number].weight.data[row,col] += 0.0001\n",
    "\n",
    "# numeric gradient\n",
    "negative_cost = CrossEntropy(negative_copy.feed_forward(batch_x), batch_y)\n",
    "positive_cost = CrossEntropy(positive_copy.feed_forward(batch_x), batch_y)\n",
    "numeric_gradient = (positive_cost - negative_cost)/0.0002\n",
    "print(numeric_gradient)\n",
    "\n",
    "NNet.feed_forward(batch_x)\n",
    "\n",
    "# gradient check\n",
    "# backprop first\n",
    "NNet.back_propagation(batch_y)\n",
    "print(NNet.layers[layer_number].weight.grad[row,col])\n",
    "NNet.zero_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the mini-MNIST dataset <a name=\"33\"></a>\n",
    "\n",
    "Here is a complete run on scikit's mini-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1, lr 0.10000, loss   1.05, accuracy  87.87%\n",
      "epoch   2, lr 0.10000, loss   0.54, accuracy  94.88%\n",
      "epoch   3, lr 0.10000, loss   0.38, accuracy  95.83%\n",
      "epoch   4, lr 0.10000, loss   0.31, accuracy  96.49%\n",
      "epoch   5, lr 0.10000, loss   0.27, accuracy  96.66%\n",
      "epoch   6, lr 0.10000, loss   0.24, accuracy  96.83%\n",
      "epoch   7, lr 0.10000, loss   0.22, accuracy  96.83%\n",
      "epoch   8, lr 0.10000, loss   0.21, accuracy  97.11%\n",
      "epoch   9, lr 0.10000, loss   0.19, accuracy  97.16%\n",
      "epoch  10, lr 0.10000, loss   0.18, accuracy  97.22%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "digits = load_digits(n_class=10, return_X_y=True)\n",
    "x = digits[0]/16\n",
    "y = np.eye(10)[digits[1]]\n",
    "batch_size = 10\n",
    "batch_pos = list(range(0, digits[0].data.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)\n",
    "\n",
    "epochs = 10\n",
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    for b in batch_pos:\n",
    "        batch_x = x[b:b+batch_size]\n",
    "        batch_y = y[b:b+batch_size]\n",
    "        NNet.feed_forward(batch_x)\n",
    "        NNet.back_propagation(batch_y)\n",
    "        NNet.optimizer.step(lr/batch_size)\n",
    "        NNet.zero_gradients()\n",
    "    predicted = NNet.feed_forward(x)\n",
    "    print(\"epoch {:3d}, lr {:7.5f}, loss {:6.2f}, accuracy {:6.2f}%\".format(ep, lr, \n",
    "        NLLCost(predicted, y), 100 * accuracy(predicted, digits[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the full MNIST dataset <a name=\"34\"></a>\n",
    "\n",
    "This will download the full MNIST dataset. Uncomment it to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we need a better data storage solution than just dumping numpy arrays on the floor and sweeping them into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFeeder:\n",
    "    def __init__(self, input_dataset, output_dataset):\n",
    "        self.input = input_dataset\n",
    "        self.output = output_dataset\n",
    "        assert self.input.shape[0] == self.output.shape[0]\n",
    "        self.number_of_rows = self.input.shape[0]\n",
    "    def shuffle(self):\n",
    "        p = numpy.random.permutation(self.number_of_rows)\n",
    "        self.input = self.input[p]\n",
    "        self.output = self.output[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 stats: lr 0.01000, batch size 100, validation loss   2.27\n",
      "  training accuracy  12.73%, validation accuracy  12.85%\n",
      "Epoch   2 stats: lr 0.01000, batch size 100, validation loss   0.38\n",
      "  training accuracy  90.04%, validation accuracy  90.59%\n",
      "Epoch   3 stats: lr 0.01000, batch size 100, validation loss   0.24\n",
      "  training accuracy  93.42%, validation accuracy  93.90%\n",
      "Epoch   4 stats: lr 0.01000, batch size 100, validation loss   0.19\n",
      "  training accuracy  94.72%, validation accuracy  94.98%\n",
      "Epoch   5 stats: lr 0.01000, batch size 100, validation loss   0.16\n",
      "  training accuracy  95.56%, validation accuracy  95.78%\n",
      "Epoch   6 stats: lr 0.01000, batch size 100, validation loss   0.13\n",
      "  training accuracy  96.21%, validation accuracy  96.40%\n",
      "Epoch   7 stats: lr 0.01000, batch size 100, validation loss   0.12\n",
      "  training accuracy  96.59%, validation accuracy  96.56%\n",
      "Epoch   8 stats: lr 0.01000, batch size 100, validation loss   0.11\n",
      "  training accuracy  96.91%, validation accuracy  96.99%\n",
      "Epoch   9 stats: lr 0.01000, batch size 100, validation loss   0.10\n",
      "  training accuracy  97.19%, validation accuracy  97.23%\n",
      "Epoch  10 stats: lr 0.01000, batch size 100, validation loss   0.09\n",
      "  training accuracy  97.29%, validation accuracy  97.26%\n",
      "Epoch  11 stats: lr 0.01000, batch size 100, validation loss   0.08\n",
      "  training accuracy  97.61%, validation accuracy  97.56%\n",
      "Epoch  12 stats: lr 0.01000, batch size 100, validation loss   0.08\n",
      "  training accuracy  97.76%, validation accuracy  97.63%\n",
      "Epoch  13 stats: lr 0.01000, batch size 100, validation loss   0.07\n",
      "  training accuracy  97.87%, validation accuracy  97.67%\n",
      "Epoch  14 stats: lr 0.01000, batch size 100, validation loss   0.07\n",
      "  training accuracy  98.00%, validation accuracy  97.76%\n",
      "Epoch  15 stats: lr 0.01000, batch size 100, validation loss   0.06\n",
      "  training accuracy  98.15%, validation accuracy  97.91%\n",
      "Epoch  16 stats: lr 0.01000, batch size 100, validation loss   0.07\n",
      "  training accuracy  98.22%, validation accuracy  97.91%\n",
      "Epoch  17 stats: lr 0.01000, batch size 100, validation loss   0.06\n",
      "  training accuracy  98.32%, validation accuracy  97.94%\n",
      "Epoch  18 stats: lr 0.01000, batch size 100, validation loss   0.06\n",
      "  training accuracy  98.43%, validation accuracy  97.95%\n",
      "Epoch  19 stats: lr 0.01000, batch size 100, validation loss   0.06\n",
      "  training accuracy  98.47%, validation accuracy  98.04%\n",
      "Epoch  20 stats: lr 0.01000, batch size 100, validation loss   0.06\n",
      "  training accuracy  98.59%, validation accuracy  97.98%\n",
      "Epoch  21 stats: lr 0.01000, batch size 100, validation loss   0.06\n",
      "  training accuracy  98.59%, validation accuracy  98.13%\n",
      "Epoch  22 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.68%, validation accuracy  98.24%\n",
      "Epoch  23 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.72%, validation accuracy  98.17%\n",
      "Epoch  24 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.77%, validation accuracy  98.28%\n",
      "Epoch  25 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.86%, validation accuracy  98.31%\n",
      "Epoch  26 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.91%, validation accuracy  98.33%\n",
      "Epoch  27 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.94%, validation accuracy  98.40%\n",
      "Epoch  28 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  98.96%, validation accuracy  98.39%\n",
      "Epoch  29 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  99.02%, validation accuracy  98.40%\n",
      "Epoch  30 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  99.07%, validation accuracy  98.44%\n",
      "Epoch  31 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  99.08%, validation accuracy  98.47%\n",
      "Epoch  32 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.16%, validation accuracy  98.58%\n",
      "Epoch  33 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.19%, validation accuracy  98.43%\n",
      "Epoch  34 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  99.17%, validation accuracy  98.48%\n",
      "Epoch  35 stats: lr 0.01000, batch size 100, validation loss   0.05\n",
      "  training accuracy  99.16%, validation accuracy  98.51%\n",
      "Epoch  36 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.24%, validation accuracy  98.55%\n",
      "Epoch  37 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.28%, validation accuracy  98.68%\n",
      "Epoch  38 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.26%, validation accuracy  98.61%\n",
      "Epoch  39 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.25%, validation accuracy  98.54%\n",
      "Epoch  40 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.35%, validation accuracy  98.64%\n",
      "Epoch  41 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.30%, validation accuracy  98.66%\n",
      "Epoch  42 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.38%, validation accuracy  98.66%\n",
      "Epoch  43 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.37%, validation accuracy  98.66%\n",
      "Epoch  44 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.37%, validation accuracy  98.67%\n",
      "Epoch  45 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.41%, validation accuracy  98.71%\n",
      "Epoch  46 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.40%, validation accuracy  98.61%\n",
      "Epoch  47 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.46%, validation accuracy  98.73%\n",
      "Epoch  48 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.48%, validation accuracy  98.74%\n",
      "Epoch  49 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.45%, validation accuracy  98.65%\n",
      "Epoch  50 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.46%, validation accuracy  98.77%\n",
      "Epoch  51 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.49%, validation accuracy  98.78%\n",
      "Epoch  52 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.50%, validation accuracy  98.75%\n",
      "Epoch  53 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.48%, validation accuracy  98.69%\n",
      "Epoch  54 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.51%, validation accuracy  98.80%\n",
      "Epoch  55 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.50%, validation accuracy  98.75%\n",
      "Epoch  56 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.54%, validation accuracy  98.74%\n",
      "Epoch  57 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.54%, validation accuracy  98.84%\n",
      "Epoch  58 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.54%, validation accuracy  98.87%\n",
      "Epoch  59 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.59%, validation accuracy  98.86%\n",
      "Epoch  60 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.61%, validation accuracy  98.79%\n",
      "Epoch  61 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.60%, validation accuracy  98.83%\n",
      "Epoch  62 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.61%, validation accuracy  98.89%\n",
      "Epoch  63 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.59%, validation accuracy  98.67%\n",
      "Epoch  64 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.67%, validation accuracy  98.86%\n",
      "Epoch  65 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.63%, validation accuracy  98.77%\n",
      "Epoch  66 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.63%, validation accuracy  98.89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  67 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.66%, validation accuracy  98.87%\n",
      "Epoch  68 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.66%, validation accuracy  98.79%\n",
      "Epoch  69 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.66%, validation accuracy  98.86%\n",
      "Epoch  70 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.65%, validation accuracy  98.85%\n",
      "Epoch  71 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.67%, validation accuracy  98.82%\n",
      "Epoch  72 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.70%, validation accuracy  98.82%\n",
      "Epoch  73 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.70%, validation accuracy  98.83%\n",
      "Epoch  74 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.69%, validation accuracy  98.89%\n",
      "Epoch  75 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.67%, validation accuracy  98.79%\n",
      "Epoch  76 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.71%, validation accuracy  98.89%\n",
      "Epoch  77 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.73%, validation accuracy  98.89%\n",
      "Epoch  78 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.75%, validation accuracy  98.88%\n",
      "Epoch  79 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.74%, validation accuracy  98.95%\n",
      "Epoch  80 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.76%, validation accuracy  98.85%\n",
      "Epoch  81 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.74%, validation accuracy  98.82%\n",
      "Epoch  82 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.80%, validation accuracy  98.94%\n",
      "Epoch  83 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.75%, validation accuracy  98.85%\n",
      "Epoch  84 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.76%, validation accuracy  98.86%\n",
      "Epoch  85 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.79%, validation accuracy  98.96%\n",
      "Epoch  86 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.77%, validation accuracy  98.90%\n",
      "Epoch  87 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.80%, validation accuracy  98.91%\n",
      "Epoch  88 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.79%, validation accuracy  98.87%\n",
      "Epoch  89 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.79%, validation accuracy  98.88%\n",
      "Epoch  90 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.82%, validation accuracy  98.93%\n",
      "Epoch  91 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.78%, validation accuracy  98.86%\n",
      "Epoch  92 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.80%, validation accuracy  98.94%\n",
      "Epoch  93 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.82%, validation accuracy  98.92%\n",
      "Epoch  94 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.77%, validation accuracy  98.92%\n",
      "Epoch  95 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.82%, validation accuracy  98.97%\n",
      "Epoch  96 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.82%, validation accuracy  98.91%\n",
      "Epoch  97 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.82%, validation accuracy  98.90%\n",
      "Epoch  98 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.83%, validation accuracy  98.95%\n",
      "Epoch  99 stats: lr 0.01000, batch size 100, validation loss   0.03\n",
      "  training accuracy  99.81%, validation accuracy  99.00%\n",
      "Epoch 100 stats: lr 0.01000, batch size 100, validation loss   0.04\n",
      "  training accuracy  99.81%, validation accuracy  98.86%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "# training set\n",
    "train_x = mnist['data'][:60000, :]/256\n",
    "train_labels = mnist['target'][:60000].astype(\"int\")\n",
    "train_y = np.eye(10)[train_labels]\n",
    "# validation set\n",
    "valid_x = mnist['data'][60000:, :]/256\n",
    "valid_labels = mnist['target'][60000:].astype(\"int\")\n",
    "valid_y = np.eye(10)[valid_labels]\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "batch_pos = list(range(0, train_x.data.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)\n",
    "\n",
    "NNet = NeuralNetwork()\n",
    "\n",
    "NNet.layers.append(InputLayer(input_size=784))\n",
    "NNet.layers.append(RollLayer(predecessor=NNet.layers[-1], list_of_offsets=(-56, -28, -2, -1, 0, 1, 2, 28, 56)))\n",
    "NNet.layers.append(GaussianNoise(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(DenseLayer(predecessor=NNet.layers[-1], hidden=256, use_bias=False, positive_params=False))\n",
    "NNet.layers.append(BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(DenseLayer(predecessor=NNet.layers[-1], hidden=256, use_bias=False, positive_params=False))\n",
    "NNet.layers.append(BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(DenseLayer(predecessor=NNet.layers[-1], hidden=256, use_bias=False, positive_params=False))\n",
    "NNet.layers.append(BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(DenseLayer(predecessor=NNet.layers[-1], hidden=256, use_bias=False, positive_params=False))\n",
    "NNet.layers.append(BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "#NNet.layers.append(DropoutLayer(predecessor=NNet.layers[-1], probability=0.5))\n",
    "NNet.layers.append(SoftmaxCrossEntropy(predecessor=NNet.layers[-1], hidden=10, use_bias=True))\n",
    "\n",
    "NNet.optimizer = AdamOptimizer(NNet.layers)\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    for b in batch_pos:\n",
    "        batch_x = train_x[b:b+batch_size]\n",
    "        batch_y = train_y[b:b+batch_size]\n",
    "        batch_labels = train_labels[b:b+batch_size]\n",
    "        NNet.feed_forward(batch_x)\n",
    "        NNet.back_propagation(batch_y)\n",
    "        NNet.optimizer.step(lr/batch_size)\n",
    "        NNet.zero_gradients()\n",
    "    train_predicted = NNet.evaluate(train_x)\n",
    "    valid_predicted = NNet.evaluate(valid_x)\n",
    "    print(\"Epoch {:3d} stats: lr {:7.5f}, batch size {:3d}, validation loss {:6.2f}\".format(ep, lr, batch_size, \n",
    "        CrossEntropy(valid_predicted, valid_y)))\n",
    "    print(\"  training accuracy {:6.2f}%, validation accuracy {:6.2f}%\".format(\n",
    "        100 * accuracy(train_predicted, train_labels), 100 * accuracy(valid_predicted, valid_labels)))\n",
    "    #lr *= 0.8\n",
    "    p = np.random.permutation(train_x.shape[0])\n",
    "    train_x = train_x[p,:]\n",
    "    train_labels = train_labels[p]\n",
    "    train_y = train_y[p,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
