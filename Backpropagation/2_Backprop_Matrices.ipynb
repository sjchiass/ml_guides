{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop guide: matrices\n",
    "\n",
    "When I browse [\"/r/learnmachinelearning\"](https://www.reddit.com/r/learnmachinelearning/), I often see people asking \"what kind of math do I need for machine learning\"?\n",
    "\n",
    "There are relly two questions here:\n",
    "1. How much math do I need to understand all of machine learning?\n",
    "2. How much math do I need to feel comfortable?\n",
    "\n",
    "I want to try to help people feel comfortable with machine learning, and I believe this only requires a very basic level of math, enough to just see that there's nothing magical about neural networks.\n",
    "\n",
    "With all the hype that comes with machine learning, there is also a lot of intimidation.\n",
    "\n",
    "Once neural networks have stopped being intimidating, you can later learn the more advanced math with confidence. Confidence is everything: you can't learn if you don't believe in your ability to learn!\n",
    "\n",
    "# Further reading\n",
    "\n",
    "* Probably the best way to get started with matrix algebra is with [Andrew Ng's Coursera videos](https://www.youtube.com/watch?v=6AP4IvfKmwg&list=PLnnr1O8OWc6boN4WHeuisJWmeQHH9D_Vg).\n",
    "* [3Blue1Brown's linear algebra videos](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) These are bit more than you need to understand neural networks, but they're good for making math less intimidating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons working\n",
    "\n",
    "Neural network layers have a dual nature: complex neural processing on one hand, and non-linear modulation on the other.\n",
    "\n",
    "The neurons' linear nature helps them perform computations. They each get their own copy of the data to work on. This amazing trick is possible because of matrix multiplication (or dot product). Rows don't mix with other rows, neither columns with other columns.\n",
    "\n",
    "The non-linear modulation, using a sigmoid, lets the layer generate more varied data. When things are only linear, they're pretty predictable. With the help of the sigmoid, the layer can now swerve to 0 or 1 depending on the situation.\n",
    "\n",
    "In summary: the large amount of connections help with analyzing lots of data simultaneously and the non-linear activation functions help deal with a large variety of situations.\n",
    "\n",
    "#### Matrices are fun\n",
    "\n",
    "Matrices shouldn't be scary. They're just a very fun way of cramming a lot of operations into a small package.\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output. The five neurons' outputs are the five elements in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     x = np.array([[1, 2, 3, 4, 5]])\n",
    "\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices can be thought as mappings from one matrix to another. In the example below, each element of $[1, 2, 3, 4, 5]$ has 5 places in that $5 \\times 5$ weight matrix.\n",
    "\n",
    "Let's look at that first weight column:\n",
    "1. Take $-1 \\times 1$ giving $-1$\n",
    "2. $0$\n",
    "3. Take $2 \\times 3$ giving $6$\n",
    "4. $0$\n",
    "5. $0$\n",
    "\n",
    "Add this up and you get $5$, which is the first element in the answer below.\n",
    "\n",
    "Here is a nice way of visualizing these operations. You just need to line up your matrices and it's a lot clearer.\n",
    "\n",
    "![Matrix cells combined](Images/intro_Matrix_multiplication.png \"Matrix cells combined\")\n",
    "\n",
    "You can play around with the weight matrix to get different results. Change some values in $x$ and $weight$ to see how the output is affected. **Try it out!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 2, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "      x = np.array([[1,  2,  3,  4,  5]])\n",
    "\n",
    "weight = np.array([[-1,  0,  0,  0,  0],  # 1 the\n",
    "                   [ 0,  2,  0,  0,  0],  # 2 numbers\n",
    "                   [ 2,  0,  1, -1,  0],  # 3 go down\n",
    "                   [ 0,  0,  0,  0, -1],  # 4 like\n",
    "                   [ 0,  0,  0,  1,  1]]) # 5 this\n",
    "x.dot(weight)\n",
    "\n",
    "# I could also just flip the identity matrix horizontally to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What this means\n",
    "\n",
    "The matrix multiplication allows the neural network layer to perform some basic arithmetic. When the network learns itself a *good* weight matrix, it has a list of instructions on how to process incoming data *just right*.\n",
    "\n",
    "There is also a statistical angle to this. One column of the weight matrix is like the linear regression model\n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 \\Longrightarrow \\hat{y} = -1 \\cdot 1 + 2 \\cdot 3 = 5$$\n",
    "\n",
    "That linear model is a way of taking input data and transforming it, but it's only one of the columns of that weight matrix! A $5 \\times 5$ weight matrix can process 5 variables 5 times! In fact, the 5 columns of the weight matrix represent a layer's 5 neurons.\n",
    "\n",
    "A medium-size hidden layer can have 100 neurons. This means it has the equivalent of 100 linear regression models working on the data.\n",
    "\n",
    "However, linear regressions are the solution to everything, which is why we need...\n",
    "\n",
    "#### The non-linearity advantage\n",
    "\n",
    "The matrix multiplication may be impressive, but all it really does is come up with $p$ different versions of the same data. Non-linearity functions like the sigmoid give the network the ability to act \"logically\".\n",
    "\n",
    "When someone says that a person's thinking is \"too linear\" they mean that person's thinking is too straightforward. What we want for a neural network is not straightforward thinking: we want the network to change its mind when it has to, when the data is a certain way. What we want is some non-linearity.\n",
    "\n",
    "Let's bring up the sigmoid function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5439258d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-10,10,100)\n",
    "plt.plot(x,1/(1 + np.exp(-x)))\n",
    "plt.title('Sigmoid activation function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function like this is useful for the network to be \"logical\".\n",
    "\n",
    "Here is an example with logic gates.\n",
    "\n",
    "![Truth tables](Images/intro_truth_table.png \"Truth tables\")\n",
    "\n",
    "* Adding: you can just sum\n",
    "* OR: you need the sigmoid to drive the output high as soon as input is present\n",
    "* NAND: you need the sigmoid to stay high until both inputs are present, then deactivate completely\n",
    "* XOR: you can't do this in one step, but you can combine OR and NAND together (using an AND gate)\n",
    "\n",
    "As you can see, the sigmoid can do the OR and the NAND individually. Three sigmoid neurons working together can do the XOR. With a linear function (adding), it's impossible to get to the XOR. Can't be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is an OR gate\n",
      "input1  0, input2  0, output   0.0\n",
      "input1  0, input2  1, output   1.0\n",
      "input1  1, input2  0, output   1.0\n",
      "input1  1, input2  1, output   1.0\n",
      "here is a NAND gate\n",
      "input1  0, input2  0, output   1.0\n",
      "input1  0, input2  1, output   1.0\n",
      "input1  1, input2  0, output   1.0\n",
      "input1  1, input2  1, output   0.0\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x): return 1/(1 + np.exp(-x))\n",
    "\n",
    "def neuron(bias, weight1, weight2):\n",
    "    print(\"input1 {:2d}, input2 {:2d}, output {:5.1f}\".format(0, 0, sigmoid(weight1 * 0 + weight2 * 0 + bias)))\n",
    "    print(\"input1 {:2d}, input2 {:2d}, output {:5.1f}\".format(0, 1, sigmoid(weight1 * 0 + weight2 * 1 + bias)))\n",
    "    print(\"input1 {:2d}, input2 {:2d}, output {:5.1f}\".format(1, 0, sigmoid(weight1 * 1 + weight2 * 0 + bias)))\n",
    "    print(\"input1 {:2d}, input2 {:2d}, output {:5.1f}\".format(1, 1, sigmoid(weight1 * 1 + weight2 * 1 + bias)))\n",
    "\n",
    "print(\"here is an OR gate\")\n",
    "neuron(-5, 10, 10)\n",
    "\n",
    "print(\"here is a NAND gate\")\n",
    "neuron(15, -10, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, have a look at how the combination of three gates is able to replicate the XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the XOR gate\n",
      "input1  0, input2  0, output   0.0\n",
      "input1  1, input2  0, output   1.0\n",
      "input1  0, input2  1, output   1.0\n",
      "input1  1, input2  1, output   0.0\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x): return 1/(1 + np.exp(-x))\n",
    "\n",
    "def more_neurons(input1, input2, \n",
    "                 bias_a, weight_a1, weight_a2, \n",
    "                 bias_b, weight_b1, weight_b2, \n",
    "                 bias_c, weight_c1, weight_c2):\n",
    "    print(\"input1 {:2d}, input2 {:2d}, output {:5.1f}\".format(input1, input2, \n",
    "       sigmoid(\n",
    "           weight_c1 * sigmoid(weight_a1 * input1 + weight_a2 * input2 + bias_a) +\n",
    "           weight_c2 * sigmoid(weight_b1 * input1 + weight_b2 * input2 + bias_b) +\n",
    "           bias_c)\n",
    "       ))\n",
    "\n",
    "print(\"here is the XOR gate\")\n",
    "more_neurons(0, 0, -5, 10, 10, 15, -10, -10, -10, 5, 5)\n",
    "more_neurons(1, 0, -5, 10, 10, 15, -10, -10, -10, 10, 10)\n",
    "more_neurons(0, 1, -5, 10, 10, 15, -10, -10, -10, 10, 10)\n",
    "more_neurons(1, 1, -5, 10, 10, 15, -10, -10, -10, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Sorry for the messy code. Using matrices makes things a bit cleaner.\n",
    "\n",
    "#### The power of mixing non-linearities\n",
    "\n",
    "The feed-forward is a mixing of data over many layers of neurons. Each layer expands data into multiple copies and its neurons compress it back into a few outputs. In the diagram far above you saw 3 units of inputs expanded into 6 synapse signals and then collapsed into 2 output signals. The power of the neural network comes from the fact that the next layer *then copies* these 2 output signals to each of its own neurons, so everything affects everything.\n",
    "\n",
    "If three neurons in two layers can replicate the XOR logic gate, many more neurons in many more layers can take more complicated decisions.\n",
    "\n",
    "![Colorful](Images/intro_Colorful.png \"Colorful\")\n",
    "\n",
    "As you can see in the example above, data expands and contracts.\n",
    "\n",
    "Below is another representation. You can see that the synapses/connections allow the network to perform a lot of operations on the data.\n",
    "\n",
    "![Colorful](Images/intro_Colorful_Sizes.png \"Colorful\")\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive committees. The neurons in a layer form a committee that looks at data together, performs analysis, and then summarizes its findings into a small report. Higher committees then analyze this report at a higher level, and so on. The final output layer makes a decision based on the accumulated wisdom of the executive summary it receives: it outputs a single value between 0 and 1.\n",
    "\n",
    "In the committee example, the office workers use\n",
    "\n",
    "* weights to increase, decrease or invert the importance of data\n",
    "* biases to make their voices louder or weaker\n",
    "* activation functions to simply their reports into a range [0,1]\n",
    "\n",
    "The expansion and contraction of information is repeated multiple times in the neural network.\n",
    "\n",
    "* The OR gate translated two binary signals into a 0 or 1\n",
    "* The NAND gate translated two binary signals into a 0 or 1\n",
    "* The AND gate looked at the OR and NAND gates' work and returned a 0 or a 1\n",
    "* XOR!\n",
    "\n",
    "All this mixing allows the neural network to do useful things. A feed forward network is not *artificial intelligence* but it is still something artificially created that is pretty useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: hiddens layers feed-forwarding\n",
    "\n",
    "With all that in mind, here is a basic feed-forward network in its full matrix glory:\n",
    "\n",
    "1. $z_1 = X W_1 + B_1$\n",
    "2. $a_1 = \\sigma(z_1)$\n",
    "3. $z_2 = a_1 W_2 + B_2$\n",
    "4. $a_2 = \\sigma(z_2)$\n",
    "5. $z_{output} = a_2 W_{output} + B_{output}$\n",
    "6. $a_{output} = \\sigma(z_{output})$\n",
    "\n",
    "By way of comparison, here is Andrew Ng's notation.\n",
    "\n",
    "1. $a^{(1)} = x$\n",
    "2. $z^{(2)} = \\theta^{(1)} a^{(1)})$\n",
    "3. $a^{(2)} = g(z^{(2)})$\n",
    "2. $z^{(3)} = \\theta^{(2)} a^{(2)})$\n",
    "3. $a^{(3)} = g(z^{(3)})$\n",
    "2. $z^{(4)} = \\theta^{(3)} a^{(3)})$\n",
    "6. $a_{(4)} = h_{\\theta}(x)=g(z^{(4)})$\n",
    "\n",
    "Everyone has their own style.\n",
    "\n",
    "Let's generate some data. Thanks to the properties of matrix multiplication, I can have 4 rows of input data and these will be processed fully separately, yielding 4 rows of output data. This is how you can process multiple rows of data at once, speeding things up considerably!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random((4,5)) # Four records of 5 variables\n",
    "b1 = np.random.random((1,3)) # Bias: 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # Weight: input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # Bias: 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # Weight: layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # Bias: 1 x output_size\n",
    "w_out = np.random.random((2,1)) # Weight: layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the activations of all the layers participating in the feed forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83480346  0.89431969  0.74474755]\n",
      " [ 0.92122836  0.95077352  0.85754254]\n",
      " [ 0.95632763  0.95462462  0.87675394]\n",
      " [ 0.90937448  0.88471319  0.79687147]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = x.dot(w1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87012514  0.86695005]\n",
      " [ 0.88290643  0.88010459]\n",
      " [ 0.88458368  0.88404378]\n",
      " [ 0.87342185  0.87610352]]\n"
     ]
    }
   ],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76292978]\n",
      " [ 0.76503123]\n",
      " [ 0.76560533]\n",
      " [ 0.76425578]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
