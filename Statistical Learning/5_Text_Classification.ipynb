{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load various models from scikit-learn's library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# also get some metrics to try\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized, fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDb dataset from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = {\"name\" : \"imdb\", \"ovr\" : False}\n",
    "(a, b), (c, d) = imdb.load_data(num_words=20000)\n",
    "imdb_data[\"X_train_ids\"], imdb_data[\"y_train\"], imdb_data[\"X_test_ids\"], imdb_data[\"y_test\"] = a, b, c, d\n",
    "\n",
    "# For scikit-learn to like the input data, it will needs strings\n",
    "imdb_data[\"X_train\"] = [\" \".join([str(x) for x in line]) for line in imdb_data[\"X_train_ids\"]]\n",
    "imdb_data[\"X_test\"] = [\" \".join([str(x) for x in line]) for line in imdb_data[\"X_test_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baby names from social security card applications. https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data\n",
    "\n",
    "```\n",
    "import os\n",
    "import re\n",
    "\n",
    "with open(\"babies.csv\", \"w\") as w:\n",
    "    for f in [f for f in os.listdir(os.getcwd()) if \"txt\" in str(f)]:\n",
    "        with open(f) as f:\n",
    "            year = re.search(r'[\\d]{4}', f.name)[0]\n",
    "            for line in f:\n",
    "                w.write(year+\",\"+line)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"babies.csv\") as f:\n",
    "    baby_list = f.readlines()\n",
    "\n",
    "baby_list.sort(key=lambda x: x[:4])\n",
    "\n",
    "print(len(baby_list))\n",
    "print(baby_list[:10])\n",
    "print(baby_list[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107973\n",
      "['2017,Zavdiel,M,5\\n', '2017,Zavonte,M,5\\n', '2017,Zayer,M,5\\n', '2017,Zechari,M,5\\n', '2017,Zennith,M,5\\n', '2017,Zeo,M,5\\n', '2017,Zhiyuan,M,5\\n', '2017,Zkari,M,5\\n', '2017,Zohaan,M,5\\n', '2017,Zykai,M,5\\n']\n"
     ]
    }
   ],
   "source": [
    "baby_set = set()\n",
    "unique_baby_list = []\n",
    "\n",
    "for baby in baby_list:\n",
    "    if \" \".join(baby.split(\",\")[1:3]) in baby_set:\n",
    "        pass\n",
    "    else:\n",
    "        baby_set.add(\" \".join(baby.split(\",\")[1:3]))\n",
    "        unique_baby_list.append(baby)\n",
    "\n",
    "print(len(unique_baby_list))\n",
    "print(unique_baby_list[:10])\n",
    "print(unique_baby_list[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87973\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "baby_train = unique_baby_list[:-20000]\n",
    "baby_test = unique_baby_list[-20000:]\n",
    "print(len(baby_train))\n",
    "print(len(baby_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newsgroup20 dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_raw = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "ng_test_raw = fetch_20newsgroups(subset=\"test\", remove=(\"headers\", \"footers\", \"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(ng_train_raw.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(ng_train_raw.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_data = {\"name\" : \"newsgroup20\", \"ovr\" : False}\n",
    "ng_data.update({\"X_train\" : ng_train_raw.data, \"y_train\" : ng_train_raw.target})\n",
    "ng_data.update({\"X_test\" : ng_test_raw.data, \"y_test\" : ng_test_raw.target})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide to using Reuters dataset here: https://martin-thoma.com/nlp-reuters/\n",
    "\n",
    "To get a copy of the Reuters data, you have to use `nltk.download(\"reuters\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reuters():\n",
    "    reuters_data = {\"name\" : \"reuters\", \"ovr\" : True}\n",
    "    \n",
    "    # The test and train sets are listed as IDs in the .fileids() member\n",
    "    train_ids = list(filter(lambda x: x[:5] == \"train\", reuters.fileids()))\n",
    "    test_ids = list(filter(lambda x: x[:4] == \"test\", reuters.fileids()))\n",
    "    reuters_data[\"X_train\"] = list(map(lambda x: reuters.raw(x), train_ids))\n",
    "    reuters_data[\"X_test\"] = list(map(lambda x: reuters.raw(x), test_ids))\n",
    "    \n",
    "    # The MultiLabelBinarizer will get you the 1s and 0s your model wants\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    reuters_data[\"y_train\"] = mlb.fit_transform(list(map(lambda x: reuters.categories(x), train_ids)))\n",
    "    reuters_data[\"y_test\"] = mlb.transform(list(map(lambda x: reuters.categories(x), test_ids)))\n",
    "    \n",
    "    return reuters_data\n",
    "    \n",
    "reuters_data = load_reuters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(vectorizer, x_train, x_test=None):\n",
    "    train_vec = vectorizer.fit_transform(x_train)\n",
    "    if x_test:\n",
    "        test_vec = vectorizer.transform(x_test)\n",
    "    else:\n",
    "        test_vec = None\n",
    "\n",
    "    return train_vec, test_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = {\"Logistic\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1), \n",
    "                  \"Logistic C=1000\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1, C=1000), \n",
    "                  \"RandomForest 10\" : RandomForestClassifier(n_jobs = -1), \n",
    "                  \"RandomForest 200\" : RandomForestClassifier(n_jobs = -1, n_estimators=200), \n",
    "                  \"MultinomialNB\":MultinomialNB()\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_eval(models, datasets, train_key=\"X_train_vec\", test_key=\"X_test_vec\"):\n",
    "    for dataset in datasets:\n",
    "        print(f\"{dataset['name']:20}{63*'-'}\")\n",
    "        results = []\n",
    "        for name, model in models.items():\n",
    "            if dataset[\"ovr\"]: model = OneVsRestClassifier(model)\n",
    "            timer = timeit.default_timer()\n",
    "            model.fit(dataset[train_key], dataset[\"y_train\"])\n",
    "            elapsed = timeit.default_timer() - timer\n",
    "            results.append({\n",
    "                \"name\" : name, \n",
    "                \"model\" : model, \n",
    "                \"train_acc\" : accuracy_score(y_true = dataset[\"y_train\"], y_pred = model.predict(X=dataset[train_key])), \n",
    "                \"test_acc\" : accuracy_score(y_true = dataset[\"y_test\"], y_pred = model.predict(X=dataset[test_key])), \n",
    "                \"precision\" : precision_score(y_true = dataset[\"y_test\"], y_pred = model.predict(X=dataset[test_key]), average=\"micro\"), \n",
    "                \"recall\" : recall_score(y_true = dataset[\"y_test\"], y_pred = model.predict(X=dataset[test_key]), average=\"micro\"), \n",
    "                \"f1_score\" : f1_score(y_true = dataset[\"y_test\"], y_pred = model.predict(X=dataset[test_key]), average=\"micro\"), \n",
    "                \"elapsed\" : elapsed\n",
    "                })\n",
    "        results.sort(key=lambda x: -x[\"f1_score\"])\n",
    "        for result in results:\n",
    "            print(\"{:>19} | {:5.2f}s | TRAIN/TEST acc {:4.2f}/{:4.2f} | pr/re/f1 {:4.2f}/{:4.2f}/{:4.2f} |\".format(\n",
    "                result[\"name\"], \n",
    "                result[\"elapsed\"], \n",
    "                result[\"train_acc\"], \n",
    "                result[\"test_acc\"], \n",
    "                result[\"precision\"], \n",
    "                result[\"recall\"], \n",
    "                result[\"f1_score\"]\n",
    "            ))\n",
    "        print(20*\" \"+63*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tdidf with unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb                ---------------------------------------------------------------\n",
      "           Logistic |  0.96s | TRAIN/TEST acc 0.93/0.89 | pr/re/f1 0.89/0.89/0.89 |\n",
      "    Logistic C=1000 |  1.47s | TRAIN/TEST acc 1.00/0.86 | pr/re/f1 0.86/0.86/0.86 |\n",
      "   RandomForest 200 | 18.03s | TRAIN/TEST acc 1.00/0.85 | pr/re/f1 0.85/0.85/0.85 |\n",
      "      MultinomialNB |  0.06s | TRAIN/TEST acc 0.89/0.83 | pr/re/f1 0.83/0.83/0.83 |\n",
      "    RandomForest 10 |  1.25s | TRAIN/TEST acc 0.99/0.76 | pr/re/f1 0.76/0.76/0.76 |\n",
      "                    ---------------------------------------------------------------\n",
      "newsgroup20         ---------------------------------------------------------------\n",
      "    Logistic C=1000 | 28.20s | TRAIN/TEST acc 0.97/0.68 | pr/re/f1 0.68/0.68/0.68 |\n",
      "           Logistic | 11.48s | TRAIN/TEST acc 0.90/0.68 | pr/re/f1 0.68/0.68/0.68 |\n",
      "   RandomForest 200 | 21.59s | TRAIN/TEST acc 0.97/0.61 | pr/re/f1 0.61/0.61/0.61 |\n",
      "      MultinomialNB |  0.12s | TRAIN/TEST acc 0.81/0.61 | pr/re/f1 0.61/0.61/0.61 |\n",
      "    RandomForest 10 |  1.34s | TRAIN/TEST acc 0.97/0.42 | pr/re/f1 0.42/0.42/0.42 |\n",
      "                    ---------------------------------------------------------------\n",
      "reuters             ---------------------------------------------------------------\n",
      "    Logistic C=1000 | 58.22s | TRAIN/TEST acc 1.00/0.80 | pr/re/f1 0.94/0.79/0.86 |\n",
      "           Logistic | 50.92s | TRAIN/TEST acc 0.72/0.67 | pr/re/f1 0.97/0.61/0.75 |\n",
      "   RandomForest 200 | 48.14s | TRAIN/TEST acc 0.99/0.65 | pr/re/f1 0.97/0.59/0.73 |\n",
      "    RandomForest 10 | 11.10s | TRAIN/TEST acc 0.94/0.60 | pr/re/f1 0.96/0.54/0.69 |\n",
      "      MultinomialNB |  0.38s | TRAIN/TEST acc 0.45/0.41 | pr/re/f1 0.99/0.33/0.50 |\n",
      "                    ---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dataset in [reuters_data, ng_data, imdb_data]:\n",
    "    dataset[\"X_train_vec\"], dataset[\"X_test_vec\"] = vectorize(TfidfVectorizer(), dataset[\"X_train\"], dataset[\"X_test\"])\n",
    "\n",
    "models_eval(list_of_models, [imdb_data, ng_data, reuters_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tdidf with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [reuters_data, ng_data, imdb_data]:\n",
    "    dataset[\"X_train_vec\"], dataset[\"X_test_vec\"] = vectorize(TfidfVectorizer(ngram_range = [2, 2]), dataset[\"X_train\"], dataset[\"X_test\"])\n",
    "\n",
    "models_eval(list_of_models, [imdb_data, ng_data, reuters_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hashing trick with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = {\"Logistic\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1), \n",
    "                  \"Logistic C=1000\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1, C=1000), \n",
    "                  \"RandomForest 10\" : RandomForestClassifier(n_jobs = -1), \n",
    "                  \"RandomForest 200\" : RandomForestClassifier(n_jobs = -1, n_estimators=200)\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuters             ------------------------------------------------------\n",
      "    Logistic C=1000 | 40.30s | TRAIN/TEST acc 0.99/0.79 | pr/re/f1 0.92/0.79/0.85 |\n",
      "   RandomForest 200 | 54.19s | TRAIN/TEST acc 1.00/0.66 | pr/re/f1 0.97/0.60/0.74 |\n",
      "           Logistic | 30.20s | TRAIN/TEST acc 0.67/0.65 | pr/re/f1 0.97/0.59/0.73 |\n",
      "    RandomForest 10 | 11.63s | TRAIN/TEST acc 0.94/0.61 | pr/re/f1 0.96/0.56/0.71 |\n",
      "                    ------------------------------------------------------\n",
      "newsgroup20         ------------------------------------------------------\n",
      "    Logistic C=1000 |  6.72s | TRAIN/TEST acc 0.97/0.58 | pr/re/f1 0.58/0.58/0.58 |\n",
      "   RandomForest 200 | 10.37s | TRAIN/TEST acc 0.97/0.55 | pr/re/f1 0.55/0.55/0.55 |\n",
      "           Logistic |  4.08s | TRAIN/TEST acc 0.72/0.54 | pr/re/f1 0.54/0.54/0.54 |\n",
      "    RandomForest 10 |  0.83s | TRAIN/TEST acc 0.97/0.42 | pr/re/f1 0.42/0.42/0.42 |\n",
      "                    ------------------------------------------------------\n",
      "imdb                ------------------------------------------------------\n",
      "           Logistic |  1.29s | TRAIN/TEST acc 0.88/0.86 | pr/re/f1 0.86/0.86/0.86 |\n",
      "    Logistic C=1000 |  1.11s | TRAIN/TEST acc 0.93/0.85 | pr/re/f1 0.85/0.85/0.85 |\n",
      "   RandomForest 200 | 22.04s | TRAIN/TEST acc 1.00/0.84 | pr/re/f1 0.84/0.84/0.84 |\n",
      "    RandomForest 10 |  1.45s | TRAIN/TEST acc 0.99/0.75 | pr/re/f1 0.75/0.75/0.75 |\n",
      "                    ------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dataset in [reuters_data, ng_data, imdb_data]:\n",
    "    dataset[\"X_train_vec\"], dataset[\"X_test_vec\"] = vectorize(HashingVectorizer(n_features = 5000), dataset[\"X_train\"], dataset[\"X_test\"])\n",
    "\n",
    "models_eval(list_of_models, [imdb_data, ng_data, reuters_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [reuters_data, ng_data, imdb_data]:\n",
    "    dataset[\"X_train_vec\"], dataset[\"X_test_vec\"] = vectorize(HashingVectorizer(n_features = 50000, analyzer=\"char_wb\", ngram_range=[2,5]), \n",
    "                                                              dataset[\"X_train\"], dataset[\"X_test\"])\n",
    "\n",
    "models_eval(list_of_models, [imdb_data, ng_data, reuters_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with the output data, except build one-hot vectors instead\n",
    "y_set = set(ng_train_raw.target)\n",
    "y_train_onehot = []\n",
    "for i in ng_train_raw.target:\n",
    "    y_train_onehot.append([0] * len(y_set))\n",
    "    y_train_onehot[-1][i] = 1\n",
    "y_train_onehot = np.array(y_train_onehot)\n",
    "print(y_train_onehot[0:2])\n",
    "print(y_train_onehot.shape)\n",
    "\n",
    "y_test_onehot = []\n",
    "for i in ng_test_raw.target:\n",
    "    y_test_onehot.append([0] * len(y_set))\n",
    "    y_test_onehot[-1][i] = 1\n",
    "y_test_onehot = np.array(y_test_onehot)\n",
    "print(y_test_onehot[0:2])\n",
    "print(y_test_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train, ng_test = vectorize(TfidfVectorizer(max_features=50000))\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(50000,)))\n",
    "\n",
    "model.add(Dense(len(ng_train_raw.target_names), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(min_delta=0.01)\n",
    "\n",
    "model.fit(ng_train[\"X\"], y_train_onehot,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stop],\n",
    "          validation_data=(ng_test[\"X\"], y_test_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(x):\n",
    "    return re.sub(r\"[ ]+\", \" \", re.sub(r\"[^\\w]+\", \" \", x)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [preprocessor(line).split() for line in ng_train_raw.data]\n",
    "\n",
    "print(len(ng_train_raw.data))\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model has 26319 words\n",
      "word2vec model has 101675 words\n",
      "word2vec model has 19998 words\n"
     ]
    }
   ],
   "source": [
    "def w2v_prepare(dataset):\n",
    "    return [preprocessor(line).split() for line in dataset]\n",
    "\n",
    "def w2v_fit(text, size=100, alpha=0.025, window=5, min_count=5, workers=4, iter=5):\n",
    "    w2v_model = Word2Vec(text, size=size, alpha=alpha, window=window, min_count=min_count, workers=workers)\n",
    "    word_vectors = w2v_model.wv\n",
    "    del w2v_model\n",
    "    print(f\"word2vec model has {len(word_vectors.vocab)} words\")\n",
    "    return word_vectors\n",
    "\n",
    "reuters_wv = w2v_fit(w2v_prepare(reuters_data[\"X_train\"]), min_count=1, iter=50, alpha=0.05)\n",
    "ng_wv = w2v_fit(w2v_prepare(ng_data[\"X_train\"]), min_count=1, iter=50, alpha=0.05)\n",
    "imdb_wv = w2v_fit(w2v_prepare(imdb_data[\"X_train\"]), min_count=1, iter=50, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_transform(text, word_vectors):\n",
    "    vocab = set(word_vectors.vocab)\n",
    "    size = word_vectors.vector_size\n",
    "    vectorized = []\n",
    "    for line in text:\n",
    "        line = list(filter(lambda x: x in vocab, line))\n",
    "        if line:\n",
    "            line = np.mean(list(map(lambda x: word_vectors[x], line)), axis=0)\n",
    "            vectorized.append(line)\n",
    "        else:\n",
    "            vectorized.append(np.zeros(size))\n",
    "    return np.array(vectorized)\n",
    "\n",
    "imdb_data[\"X_train_wv\"] = w2v_transform(w2v_prepare(imdb_data[\"X_train\"]), imdb_wv)\n",
    "imdb_data[\"X_test_wv\"] = w2v_transform(w2v_prepare(imdb_data[\"X_test\"]), imdb_wv)\n",
    "\n",
    "ng_data[\"X_train_wv\"] = w2v_transform(w2v_prepare(ng_data[\"X_train\"]), ng_wv)\n",
    "ng_data[\"X_test_wv\"] = w2v_transform(w2v_prepare(ng_data[\"X_test\"]), ng_wv)\n",
    "\n",
    "reuters_data[\"X_train_wv\"] = w2v_transform(w2v_prepare(reuters_data[\"X_train\"]), reuters_wv)\n",
    "reuters_data[\"X_test_wv\"] = w2v_transform(w2v_prepare(reuters_data[\"X_test\"]), reuters_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = {\"Logistic\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1), \n",
    "                  \"Logistic C=1000\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1, C=1000), \n",
    "                  \"RandomForest 10\" : RandomForestClassifier(n_jobs = -1), \n",
    "                  \"RandomForest 200\" : RandomForestClassifier(n_jobs = -1, n_estimators=200)\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb                ---------------------------------------------------------------\n",
      "    Logistic C=1000 |  1.03s | TRAIN/TEST acc 0.86/0.85 | pr/re/f1 0.85/0.85/0.85 |\n",
      "           Logistic |  1.40s | TRAIN/TEST acc 0.86/0.85 | pr/re/f1 0.85/0.85/0.85 |\n",
      "   RandomForest 200 |  7.24s | TRAIN/TEST acc 1.00/0.81 | pr/re/f1 0.81/0.81/0.81 |\n",
      "    RandomForest 10 |  0.61s | TRAIN/TEST acc 0.99/0.75 | pr/re/f1 0.75/0.75/0.75 |\n",
      "                    ---------------------------------------------------------------\n",
      "newsgroup20         ---------------------------------------------------------------\n",
      "           Logistic |  8.43s | TRAIN/TEST acc 0.54/0.47 | pr/re/f1 0.47/0.47/0.47 |\n",
      "    Logistic C=1000 |  8.95s | TRAIN/TEST acc 0.54/0.46 | pr/re/f1 0.46/0.46/0.46 |\n",
      "   RandomForest 200 |  3.87s | TRAIN/TEST acc 0.97/0.41 | pr/re/f1 0.41/0.41/0.41 |\n",
      "    RandomForest 10 |  0.41s | TRAIN/TEST acc 0.97/0.30 | pr/re/f1 0.30/0.30/0.30 |\n",
      "                    ---------------------------------------------------------------\n",
      "reuters             ---------------------------------------------------------------\n",
      "           Logistic | 37.89s | TRAIN/TEST acc 0.72/0.70 | pr/re/f1 0.91/0.66/0.77 |\n",
      "   RandomForest 200 | 72.50s | TRAIN/TEST acc 1.00/0.67 | pr/re/f1 0.94/0.61/0.74 |\n",
      "    Logistic C=1000 | 46.43s | TRAIN/TEST acc 0.81/0.68 | pr/re/f1 0.75/0.73/0.74 |\n",
      "    RandomForest 10 | 15.84s | TRAIN/TEST acc 0.93/0.65 | pr/re/f1 0.94/0.59/0.73 |\n",
      "                    ---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models_eval(list_of_models, [imdb_data, ng_data, reuters_data], train_key=\"X_train_wv\", test_key=\"X_test_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model has 218316 words\n"
     ]
    }
   ],
   "source": [
    "fil9_wv = w2v_fit(LineSentence(\"fil9\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_data[\"X_train_wv\"] = w2v_transform(w2v_prepare(ng_data[\"X_train\"]), fil9_wv)\n",
    "ng_data[\"X_test_wv\"] = w2v_transform(w2v_prepare(ng_data[\"X_test\"]), fil9_wv)\n",
    "\n",
    "reuters_data[\"X_train_wv\"] = w2v_transform(w2v_prepare(reuters_data[\"X_train\"]), fil9_wv)\n",
    "reuters_data[\"X_test_wv\"] = w2v_transform(w2v_prepare(reuters_data[\"X_test\"]), fil9_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsgroup20         ---------------------------------------------------------------\n",
      "           Logistic |  8.90s | TRAIN/TEST acc 0.59/0.53 | pr/re/f1 0.53/0.53/0.53 |\n",
      "    Logistic C=1000 |  8.78s | TRAIN/TEST acc 0.59/0.53 | pr/re/f1 0.53/0.53/0.53 |\n",
      "   RandomForest 200 |  3.84s | TRAIN/TEST acc 0.97/0.44 | pr/re/f1 0.44/0.44/0.44 |\n",
      "    RandomForest 10 |  0.31s | TRAIN/TEST acc 0.97/0.30 | pr/re/f1 0.30/0.30/0.30 |\n",
      "                    ---------------------------------------------------------------\n",
      "reuters             ---------------------------------------------------------------\n",
      "           Logistic | 54.86s | TRAIN/TEST acc 0.67/0.66 | pr/re/f1 0.85/0.66/0.74 |\n",
      "    Logistic C=1000 | 59.35s | TRAIN/TEST acc 0.74/0.63 | pr/re/f1 0.69/0.71/0.70 |\n",
      "   RandomForest 200 | 84.15s | TRAIN/TEST acc 0.99/0.57 | pr/re/f1 0.96/0.49/0.65 |\n",
      "    RandomForest 10 | 16.64s | TRAIN/TEST acc 0.90/0.54 | pr/re/f1 0.92/0.47/0.63 |\n",
      "                    ---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models_eval(list_of_models, [ng_data, reuters_data], train_key=\"X_train_wv\", test_key=\"X_test_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_shape=(200,)))\n",
    "\n",
    "model.add(Dense(len(ng_train_raw.target_names), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(w2v_train, y_train_onehot,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "validation_data=(w2v_test, y_test_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = {\"LogisticRegression\" : LogisticRegression(solver=\"lbfgs\", n_jobs = -1), \n",
    "                  \"RandomForest\" : RandomForestClassifier(n_jobs = -1), \n",
    "                  \"RandomForest 100\" : RandomForestClassifier(n_jobs = -1, n_estimators=100),\n",
    "                  \"GradientBoost\" : GradientBoostingClassifier()\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in list_of_models.items():\n",
    "    print()\n",
    "    print(name)\n",
    "    model.fit(w2v_train, ng_train_raw.target)\n",
    "    train_score = accuracy_score(y_true = ng_train_raw.target, y_pred = model.predict(X=w2v_train))\n",
    "    print(\"Train score {0}:\".format(train_score))\n",
    "    test_score = accuracy_score(y_true = ng_test_raw.target, y_pred = model.predict(X=w2v_test))\n",
    "    print(\"Test score {0}:\".format(test_score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [preprocessor(line).split() for line in ng_train_raw.data]\n",
    "test_sentences = [preprocessor(line).split() for line in ng_train_raw.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_data(train_set, test_set):\n",
    "    train_set = w2v_prepare(train_set)\n",
    "    test_set = w2v_prepare(test_set)\n",
    "    \n",
    "    # Keep 0 for unknown tokens\n",
    "    id2word = [\"<NULL>\"] + list(set([i for j in train_set for i in j]))\n",
    "\n",
    "    word2id = dict()\n",
    "    for i in range(len(id2word)):\n",
    "        word2id[id2word[i]] = i + 1\n",
    "\n",
    "    train_set = [[word2id.get(token, 0) for token in line] for line in train_set]\n",
    "    test_set = [[word2id.get(token, 0) for token in line] for line in test_set]\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "ng_data[\"X_train_ids\"], ng_data[\"X_test_ids\"] = keras_data(ng_data[\"X_train\"], ng_data[\"X_test\"])\n",
    "reuters_data[\"X_train_ids\"], reuters_data[\"X_test_ids\"] = keras_data(reuters_data[\"X_train\"], reuters_data[\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_y(train_set, test_set):\n",
    "    y_set = set(train_set)\n",
    "    y_train_onehot = []\n",
    "    for i in train_set:\n",
    "        y_train_onehot.append([0] * len(y_set))\n",
    "        y_train_onehot[-1][i] = 1\n",
    "    y_train_onehot = np.array(y_train_onehot)\n",
    "\n",
    "    y_test_onehot = []\n",
    "    for i in test_set:\n",
    "        y_test_onehot.append([0] * len(y_set))\n",
    "        y_test_onehot[-1][i] = 1\n",
    "    y_test_onehot = np.array(y_test_onehot)\n",
    "    \n",
    "    return y_train_onehot, y_test_onehot\n",
    "\n",
    "imdb_data[\"y_train_onehot\"], imdb_data[\"y_test_onehot\"] = imdb_data[\"y_train\"], imdb_data[\"y_test\"]\n",
    "ng_data[\"y_train_onehot\"], ng_data[\"y_test_onehot\"] = onehot_y(ng_data[\"y_train\"], ng_data[\"y_test\"])\n",
    "#reuters_data[\"y_train_onehot\"], reuters_data[\"y_test_onehot\"] = onehot_y(reuters_data[\"y_train\"], reuters_data[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [[word2id.get(token, 0) for token in preprocessor(line).split()] for line in ng_train_raw.data]\n",
    "test_sentences = [[word2id.get(token, 0) for token in preprocessor(line).split()] for line in ng_test_raw.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data[\"y_train_onehot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 173us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Build model...\n",
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 2s 180us/step - loss: 2.5223 - acc: 0.2794 - val_loss: 2.0461 - val_acc: 0.3842\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 2s 150us/step - loss: 1.4485 - acc: 0.6640 - val_loss: 1.4893 - val_acc: 0.6184\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 2s 151us/step - loss: 0.9023 - acc: 0.8048 - val_loss: 1.3182 - val_acc: 0.6401\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 2s 150us/step - loss: 0.6259 - acc: 0.8671 - val_loss: 1.2538 - val_acc: 0.6584\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 2s 151us/step - loss: 0.4659 - acc: 0.9022 - val_loss: 1.2439 - val_acc: 0.6681\n"
     ]
    }
   ],
   "source": [
    "for dataset, classes, loss in zip([imdb_data, ng_data], [1, 20], [\"binary_crossentropy\", \"sparse_categorical_crossentropy\"]):\n",
    "    # Set parameters:\n",
    "    max_features = 100000\n",
    "    batch_size = 32\n",
    "    embedding_dims = 50\n",
    "    epochs = 20\n",
    "    maxlen = 400\n",
    "\n",
    "    x_train = sequence.pad_sequences(dataset[\"X_train_ids\"], maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(dataset[\"X_test_ids\"], maxlen=maxlen)\n",
    "\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    optimizer = Adam(lr=0.01)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(min_delta=0.01)\n",
    "\n",
    "    model.fit(x_train, dataset[\"y_train\"],\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stop],\n",
    "              validation_data=(x_test, dataset[\"y_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "max_features = 100000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "epochs = 20\n",
    "maxlen = 100\n",
    "\n",
    "x_train = sequence.pad_sequences(train_sentences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(test_sentences, maxlen=maxlen)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(len(ng_train_raw.target_names), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_onehot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
