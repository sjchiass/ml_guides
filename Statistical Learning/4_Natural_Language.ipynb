{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You will likely have to download nltk packages to use it\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language ToolKit\n",
    "\n",
    "The `nltk` python package has lots of tools to help you work with text. The following functions may all appear to be magic, but they're mostly based off of statistical models.\n",
    "\n",
    "You can find tokenizers and part-of-speech taggers for other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragram = \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split text into tokens (words) using the punkt tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part-of-speech tagger requires the averaged perceptron tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = nltk.word_tokenize(paragram)\n",
    "nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urbem Romam a principio reges habuere; libertatem et consulatum L. Brutus instituit. Dictaturae ad tempus sumebantur; neque decemviralis potestas ultra biennium, neque tribunorum militum consulare ius diu valuit. Non Cinnae, non Sullae longa dominatio; et Pompei Crassique potentia cito in Caesarem, Lepidi atque Antonii arma in Augustum cessere, qui cuncta discordiis civilibus fessa nomine principis sub imperium accepit.\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Principio.txt\", \"r\") as f:\n",
    "    principio = \" \".join(f.readlines()).replace(\"\\n\", \"\")\n",
    "\n",
    "print(principio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library can also tokenize sentences. This is useful for when you want to build a corpus of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Urbem Romam a principio reges habuere; libertatem et consulatum L. Brutus instituit.',\n",
       " 'Dictaturae ad tempus sumebantur; neque decemviralis potestas ultra biennium, neque tribunorum militum consulare ius diu valuit.',\n",
       " 'Non Cinnae, non Sullae longa dominatio; et Pompei Crassique potentia cito in Caesarem, Lepidi atque Antonii arma in Augustum cessere, qui cuncta discordiis civilibus fessa nomine principis sub imperium accepit.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(principio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim for word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as word2vec\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./enwik8.txt\", \"r\") as f:\n",
    "    enwik8 = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        <id>8029</id>',\n",
       " '      </contributor>',\n",
       " '      <minor />',\n",
       " '      <comment>adding cur_id=5: {{R from CamelCase}}</comment>',\n",
       " '      <text xml:space=\"preserve\">#REDIRECT [[Algeria]]{{R from CamelCase}}</text>',\n",
       " '    </revision>',\n",
       " '  </page>',\n",
       " '  <page>',\n",
       " '    <title>AmericanSamoa</title>',\n",
       " '    <id>6</id>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enwik8[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're doing word-based vectorization, you're wasting energy treating \"castle\" (no adjacent period) the same as \"castle.\". You can `word_tokenize()` to fix this. Doing this also saves memory by reducing vocabulary size.\n",
    "\n",
    "https://github.com/facebookresearch/fastText/blob/master/wikifil.pl\n",
    "\n",
    "I checked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_enwik8 = []\n",
    "\n",
    "# I've kept the comments in the code, but I've otherwise tweaked it to run in Python\n",
    "\n",
    "# Program to filter Wikipedia XML dumps to \"clean\" text consisting only of lowercase\n",
    "# letters (a-z, converted from A-Z), and spaces (never consecutive).  \n",
    "# All other characters are converted to spaces.  Only text which normally appears \n",
    "# in the web browser is displayed.  Tables are removed.  Image captions are \n",
    "# preserved.  Links are converted to normal text.  Digits are spelled out.\n",
    "\n",
    "# Written by Matt Mahoney, June 10, 2006.  This program is released to the public domain.\n",
    "for line in enwik8:\n",
    "    if \"<text\" in line.lower() and \"#redirect\" not in line.lower():\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"<.*>\", r\"\", line) # remove xml tags\n",
    "        line = re.sub(r\"&amp;\", r\"&\", line) # decode URL encoded chars\n",
    "        line = re.sub(r\"&lt;\", r\"<\", line)\n",
    "        line = re.sub(r\"&gt;\", r\">\", line)\n",
    "        line = re.sub(r\"<ref[^<]*<\\/ref>\", r\"\", line) # remove references <ref...> ... </ref>\n",
    "        line = re.sub(r\"<[^>]*>\", r\"\", line) # remove xhtml tags\n",
    "        line = re.sub(r\"\\[http:[^] ]*\", r\"[]\", line) # remove normal url, preserve visible text\n",
    "        line = re.sub(r\"\\|thumb\", \"\", line) # remove images links, preserve caption\n",
    "        line = re.sub(r\"\\|left\", \"\", line)\n",
    "        line = re.sub(r\"\\|right\", \"\", line)\n",
    "        line = re.sub(r\"\\|\\d+px\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\[image:[^\\[\\]]*\\|\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\[category:([^|\\]]*)[^]]*\\]\\]\", \"[[$1]]\", line) # show categories without markup\n",
    "        line = re.sub(r\"\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\", \"\", line) # remove links to other languages\n",
    "        line = re.sub(r\"\\[\\[[^\\|\\]]*\\|\", \"[[\", line) # remove wiki url, preserve visible text\n",
    "        line = re.sub(r\"\\{\\{[^\\}]*\\}\\}\", \"\", line) # remove {{icons}} and {tables}\n",
    "        line = re.sub(r\"\\{[^\\}]*\\}\", \"\", line) # remove [ and ]\n",
    "        line = re.sub(r\"\\[\", \"\", line)\n",
    "        line = re.sub(r\"\\]\", \"\", line)\n",
    "        line = re.sub(r\"&[^;]*;\", \"\", line) # remove URL encoded chars\n",
    "        # convert to lowercase letters and spaces, spell digits\n",
    "        line = \" \"+line+\" \"\n",
    "        line = re.sub(r\"0\", \" zero \", line)\n",
    "        line = re.sub(r\"1\", \" one \", line)\n",
    "        line = re.sub(r\"2\", \" two \", line)\n",
    "        line = re.sub(r\"3\", \" three \", line)\n",
    "        line = re.sub(r\"4\", \" four \", line)\n",
    "        line = re.sub(r\"5\", \" five \", line)\n",
    "        line = re.sub(r\"6\", \" six \", line)\n",
    "        line = re.sub(r\"7\", \" seven \", line)\n",
    "        line = re.sub(r\"8\", \" eight \", line)\n",
    "        line = re.sub(r\"9\", \" nine\", line)\n",
    "        line = re.sub(r\"[^\\w]+\", \" \", line)\n",
    "        line = re.sub(r\"[ ]+\", \" \", line)\n",
    "        line = line.strip()\n",
    "        if len(line) > 0 :\n",
    "            cleaned_enwik8.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notes', 'view of abu dhabi', 'for other uses see achilles disambiguation', 'for other uses of the name abraham lincoln see abraham lincoln disambiguation', 'infobox_philosopher', 'an american in paris is also a one nine five one film musical starring gene kelly', 'the academy awards popularly known as the oscars are the most prominent film awards in the united states and arguably the world the awards are granted by the academy of motion picture arts and sciences a professional honorary organization which as of two zero zero three had a voting membership of five eight one six actors with a membership of one three one one make up the largest voting bloc the votes have been tabulated and certified by auditing firm pricewaterhousecoopers since close to the awards inception', 'temps atomique international tai or international atomic time is a very accurate and stable time scale it is a weighted average of the time kept by about three zero zero atomic clocks including a large number of caesium atomic clocks in over five zero national laboratories worldwide it has been available since one nine five five and became the international standard on which utc is based on january one one nine seven two as decided by the one four th general conference on weights and measures cgpm the international bureau of weights and measures is in charge of the realization of tai', 'altruism is considered a belief a practice a habit or an ethical doctrine many cultures and religious traditions judge altruism to be virtuous in english the idea was often described as golden rule of ethics in buddhism it is considered a fundamental property of human nature', 'lee accepting the best foreign film award for crouching tiger hidden dragon at the seven three rd academy awards', 'alain connes born april one one nine four seven is a french mathematician currently professor at the college de france paris france ihes bures sur yvette france and vanderbilt university nashville tennessee he is a specialist of von neumann algebras and succeeded in completing the classification of factors of these objects although his work in physics was not very convincing he tried to connect the planckian scales with what he called a two brane universe model which was largely rejected by string theorists so far', 'allan dwan april three one eight eight five december two one one nine eight one was a pioneering canadian born american motion picture director producer and screenwriter', 'characters in ayn rand s novel atlas shrugged', 'technology in atlas shrugged ayn rand s novel includes a variety of technological products and devices in addition to real world technology aircraft automobiles diesel engines phonograph records radios telephones television and traffic signals atlas shrugged also includes various fictional technologies or fictional variants on real inventions', 'companies in atlas shrugged the ayn rand novel generally are divided into two groups these that are operated by sympathetic characters are given the name of the owner while companies operated by evil or incompetent characters are given generic names in atlas shrugged men who give their names to their companies all become strikers in due time', 'some of the important concepts discussed in atlas shrugged include the sanction of the victim and the theory of sex', 'this is a list of general items in ayn rand s atlas shrugged', 'atlas', 'atlas shrugged cover by nick gaetano', 'anthropology from the greek word άνθρωπος human or person consists of the study of humanity see genus homo it is holistic in two senses it is concerned with all humans at all times and with all dimensions of humanity a primary trait that traditionally distinguished anthropology from other humanistic disciplines is an emphasis on cultural relativity indepth examination of context and cross cultural comparisons']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_enwik8[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation will now be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">', '  <siteinfo>', '    <sitename>Wikipedia</sitename>', '    <base>http://en.wikipedia.org/wiki/Main_Page</base>', '    <generator>MediaWiki 1.6alpha</generator>', '    <case>first-letter</case>', '      <namespaces>', '      <namespace key=\"-2\">Media</namespace>', '      <namespace key=\"-1\">Special</namespace>', '      <namespace key=\"0\" />']\n"
     ]
    }
   ],
   "source": [
    "print(enwik8[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blobbed_enwik8 = \" \".join(cleaned_enwik8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145801\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notes view of abu dhabi for other uses see achilles disambiguation for other uses of the name abraha\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_sentences = word2vec.LineSentence(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = word2vec.Word2Vec(blobbed_enwik8, size=100, window=5, min_count=2, workers=4, sg=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3256045857578719"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"king\", \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7944913855594391"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"prince\", \"princess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26912014338901086"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"edible\", \"pikachu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8298283016765463"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"pikachu\", \"raichu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774525851874228"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"pikachu\", \"bulbasaur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LineSentence in module gensim.models.word2vec:\n",
      "\n",
      "class LineSentence(builtins.object)\n",
      " |  Simple format: one sentence = one line; words already preprocessed and separated by whitespace.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      " |      `source` can be either a string or a file object. Clip the file to the first\n",
      " |      `limit` lines (or not clipped if limit is None, the default).\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          sentences = LineSentence('myfile.txt')\n",
      " |      \n",
      " |      Or for compressed files::\n",
      " |      \n",
      " |          sentences = LineSentence('compressed_text.txt.bz2')\n",
      " |          sentences = LineSentence('compressed_text.txt.gz')\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate through the lines in the source.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec.LineSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
