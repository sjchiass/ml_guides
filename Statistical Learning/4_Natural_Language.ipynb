{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "Regular expressions (regex) are extremely useful in natural language processing. You can use them with the [`re` library](https://docs.python.org/3/library/re.html) in python. Regex may be intimidating, but it's worth the effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a common thing to want to remove punctuation from text. Regex makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown dog jumped over the lazy cheese  repeatedly  Without the cheese  there is boredom: the dog \n"
     ]
    }
   ],
   "source": [
    "text = \"The brown dog jumped over the lazy cheese; repeatedly. Without the cheese, there is boredom: the dog?\"\n",
    "\n",
    "print(re.sub(r'[.,;:!?-]', ' ', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives extra spaces which can be removed with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown dog jumped over the lazy cheese repeatedly Without the cheese there is boredom the dog \n"
     ]
    }
   ],
   "source": [
    "text = \"The brown dog jumped over the lazy cheese; repeatedly. Without the cheese, there is boredom: the dog?\"\n",
    "\n",
    "spaces = re.sub(r'[.,;:!?-]', ' ', text)\n",
    "\n",
    "print(re.sub(r'[ ]+', ' ', spaces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression looks weird at first, but a [good cheatsheet](https://pycon2016.regex.training/cheat-sheet) helps. Microsoft also makes [printable cheasheets](https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference), but there may be slight difference between implementations.\n",
    "\n",
    "If you're working with web data, regex is useful for cleaning. For example, the 100MB wikiepedia dataset has lots of XML and HTML tag everywhere, which you normally don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['        <id>8029</id>', '      </contributor>', '      <minor />', '      <comment>adding cur_id=5: {{R from CamelCase}}</comment>', '      <text xml:space=\"preserve\">#REDIRECT [[Algeria]]{{R from CamelCase}}</text>', '    </revision>', '  </page>', '  <page>', '    <title>AmericanSamoa</title>', '    <id>6</id>']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./enwik8.txt\", \"r\") as f:\n",
    "    enwik8 = f.read().splitlines()\n",
    "\n",
    "print(enwik8[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, a researcher named [Matt Mahoney](http://mattmahoney.net/) wrote a nice perl script for cleaning that stuff out. The fastText team has made that script available [here](https://github.com/facebookresearch/fastText/blob/master/wikifil.pl) and I've translated it into python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notes', 'view of abu dhabi', 'for other uses see achilles disambiguation', 'for other uses of the name abraham lincoln see abraham lincoln disambiguation', 'infobox_philosopher']\n"
     ]
    }
   ],
   "source": [
    "cleaned_enwik8 = []\n",
    "\n",
    "# I've kept the comments in the code, but I've otherwise tweaked it to run in Python\n",
    "\n",
    "# Program to filter Wikipedia XML dumps to \"clean\" text consisting only of lowercase\n",
    "# letters (a-z, converted from A-Z), and spaces (never consecutive).  \n",
    "# All other characters are converted to spaces.  Only text which normally appears \n",
    "# in the web browser is displayed.  Tables are removed.  Image captions are \n",
    "# preserved.  Links are converted to normal text.  Digits are spelled out.\n",
    "\n",
    "# Written by Matt Mahoney, June 10, 2006.  This program is released to the public domain.\n",
    "for line in enwik8:\n",
    "    if \"<text\" in line.lower() and \"#redirect\" not in line.lower():\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"<.*>\", r\"\", line) # remove xml tags\n",
    "        line = re.sub(r\"&amp;\", r\"&\", line) # decode URL encoded chars\n",
    "        line = re.sub(r\"&lt;\", r\"<\", line)\n",
    "        line = re.sub(r\"&gt;\", r\">\", line)\n",
    "        line = re.sub(r\"<ref[^<]*<\\/ref>\", r\"\", line) # remove references <ref...> ... </ref>\n",
    "        line = re.sub(r\"<[^>]*>\", r\"\", line) # remove xhtml tags\n",
    "        line = re.sub(r\"\\[http:[^] ]*\", r\"[]\", line) # remove normal url, preserve visible text\n",
    "        line = re.sub(r\"\\|thumb\", \"\", line) # remove images links, preserve caption\n",
    "        line = re.sub(r\"\\|left\", \"\", line)\n",
    "        line = re.sub(r\"\\|right\", \"\", line)\n",
    "        line = re.sub(r\"\\|\\d+px\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\[image:[^\\[\\]]*\\|\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\[category:([^|\\]]*)[^]]*\\]\\]\", \"[[$1]]\", line) # show categories without markup\n",
    "        line = re.sub(r\"\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\", \"\", line) # remove links to other languages\n",
    "        line = re.sub(r\"\\[\\[[^\\|\\]]*\\|\", \"[[\", line) # remove wiki url, preserve visible text\n",
    "        line = re.sub(r\"\\{\\{[^\\}]*\\}\\}\", \"\", line) # remove {{icons}} and {tables}\n",
    "        line = re.sub(r\"\\{[^\\}]*\\}\", \"\", line) # remove [ and ]\n",
    "        line = re.sub(r\"\\[\", \"\", line)\n",
    "        line = re.sub(r\"\\]\", \"\", line)\n",
    "        line = re.sub(r\"&[^;]*;\", \"\", line) # remove URL encoded chars\n",
    "        # convert to lowercase letters and spaces, spell digits\n",
    "        line = \" \"+line+\" \"\n",
    "        line = re.sub(r\"0\", \" zero \", line)\n",
    "        line = re.sub(r\"1\", \" one \", line)\n",
    "        line = re.sub(r\"2\", \" two \", line)\n",
    "        line = re.sub(r\"3\", \" three \", line)\n",
    "        line = re.sub(r\"4\", \" four \", line)\n",
    "        line = re.sub(r\"5\", \" five \", line)\n",
    "        line = re.sub(r\"6\", \" six \", line)\n",
    "        line = re.sub(r\"7\", \" seven \", line)\n",
    "        line = re.sub(r\"8\", \" eight \", line)\n",
    "        line = re.sub(r\"9\", \" nine\", line)\n",
    "        line = re.sub(r\"[^\\w]+\", \" \", line)\n",
    "        line = re.sub(r\"[ ]+\", \" \", line)\n",
    "        line = line.strip()\n",
    "        if len(line) > 0 :\n",
    "            cleaned_enwik8.append(line)\n",
    "\n",
    "print(cleaned_enwik8[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short script scrubs the data clean and leaves the text behind.\n",
    "\n",
    "## Natural Language ToolKit\n",
    "\n",
    "The `nltk` python package has lots of tools to help you work with text. The following functions may all appear to be magic, but they're mostly based off of statistical models.\n",
    "\n",
    "You can find tokenizers and part-of-speech taggers for language other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# You will likely have to download nltk packages to use them\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split text into tokens (words) using the punkt tokenizer. Tokenization is extremely useful for natural language modelling.\n",
    "\n",
    "Notice that the punctuation is properly separated from the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'dog', 'jumped', 'over', 'the', 'lazy', 'cheese', ';', 'repeatedly', '.', 'Without', 'the', 'cheese', ',', 'there', 'is', 'boredom', ':', 'the', 'dog', '?']\n"
     ]
    }
   ],
   "source": [
    "paragram = \"The brown dog jumped over the lazy cheese; repeatedly. Without the cheese, there is boredom: the dog?\"\n",
    "\n",
    "print(nltk.word_tokenize(paragram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural language toolkit can also do something called \"part of speech tagging\". It will identify the subjects, predicates, etc in your sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('cheese', 'NN'),\n",
       " (';', ':'),\n",
       " ('repeatedly', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Without', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cheese', 'NN'),\n",
       " (',', ','),\n",
       " ('there', 'EX'),\n",
       " ('is', 'VBZ'),\n",
       " ('boredom', 'NN'),\n",
       " (':', ':'),\n",
       " ('the', 'DT'),\n",
       " ('dog', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = nltk.word_tokenize(paragram)\n",
    "nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urbem Romam a principio reges habuere; libertatem et consulatum L. Brutus instituit. Dictaturae ad tempus sumebantur; neque decemviralis potestas ultra biennium, neque tribunorum militum consulare ius diu valuit. Non Cinnae, non Sullae longa dominatio; et Pompei Crassique potentia cito in Caesarem, Lepidi atque Antonii arma in Augustum cessere, qui cuncta discordiis civilibus fessa nomine principis sub imperium accepit.\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Principio.txt\", \"r\") as f:\n",
    "    principio = \" \".join(f.readlines()).replace(\"\\n\", \"\")\n",
    "\n",
    "print(principio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library can also tokenize sentences. This is useful for when you want to build a corpus of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Urbem Romam a principio reges habuere; libertatem et consulatum L. Brutus instituit.',\n",
       " 'Dictaturae ad tempus sumebantur; neque decemviralis potestas ultra biennium, neque tribunorum militum consulare ius diu valuit.',\n",
       " 'Non Cinnae, non Sullae longa dominatio; et Pompei Crassique potentia cito in Caesarem, Lepidi atque Antonii arma in Augustum cessere, qui cuncta discordiis civilibus fessa nomine principis sub imperium accepit.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(principio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim for word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're doing word-based vectorization, you're wasting energy treating \"castle\" (no adjacent period) the same as \"castle.\". You can `word_tokenize()` to fix this. Doing this also saves memory by reducing vocabulary size.\n",
    "\n",
    "https://github.com/facebookresearch/fastText/blob/master/wikifil.pl\n",
    "\n",
    "I checked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation will now be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">', '  <siteinfo>', '    <sitename>Wikipedia</sitename>', '    <base>http://en.wikipedia.org/wiki/Main_Page</base>', '    <generator>MediaWiki 1.6alpha</generator>', '    <case>first-letter</case>', '      <namespaces>', '      <namespace key=\"-2\">Media</namespace>', '      <namespace key=\"-1\">Special</namespace>', '      <namespace key=\"0\" />']\n"
     ]
    }
   ],
   "source": [
    "print(enwik8[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blobbed_enwik8 = \" \".join(cleaned_enwik8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145801\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notes view of abu dhabi for other uses see achilles disambiguation for other uses of the name abraha\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_sentences = word2vec.LineSentence(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = word2vec.Word2Vec(blobbed_enwik8, size=100, window=5, min_count=2, workers=4, sg=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3256045857578719"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"king\", \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7944913855594391"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"prince\", \"princess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26912014338901086"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"edible\", \"pikachu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8298283016765463"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"pikachu\", \"raichu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774525851874228"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.n_similarity(\"pikachu\", \"bulbasaur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LineSentence in module gensim.models.word2vec:\n",
      "\n",
      "class LineSentence(builtins.object)\n",
      " |  Simple format: one sentence = one line; words already preprocessed and separated by whitespace.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      " |      `source` can be either a string or a file object. Clip the file to the first\n",
      " |      `limit` lines (or not clipped if limit is None, the default).\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          sentences = LineSentence('myfile.txt')\n",
      " |      \n",
      " |      Or for compressed files::\n",
      " |      \n",
      " |          sentences = LineSentence('compressed_text.txt.bz2')\n",
      " |          sentences = LineSentence('compressed_text.txt.gz')\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate through the lines in the source.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec.LineSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
