{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language in Python\n",
    "\n",
    "Natural language is fun to work with in Python, thanks to easy-to-use tools. Text can be processed quickly with regular expressions, or libraries like `nltk` and `spaCy` can run pre-trained models to tokenize, parse, and vectorize text.\n",
    "\n",
    "This guide is intended as a quick overview of the options you have in Python.\n",
    "\n",
    "* Regular expressions\n",
    "    * Getting rid of punctuation\n",
    "    * Scrubbing XML\n",
    "* Natural Language ToolKit (nltk)\n",
    "    * Tokenization\n",
    "    * Part-of-speech tagging\n",
    "    * Sentence tokenization\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "* spaCy library\n",
    "    * Tokens and dependencies\n",
    "    * Named entity recognition\n",
    "    * Word vectors\n",
    "\n",
    "## Regular Expressions\n",
    "\n",
    "Regular expressions (regex) are extremely useful in natural language processing. You can use them with the [`re` library](https://docs.python.org/3/library/re.html) in python. Regex may be intimidating, but it's worth the effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "\n",
    "It's a common thing to want to remove punctuation from text. Regex makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown dog jumped over the lazy cheese  repeatedly  Without the cheese  there is boredom  the dog \n"
     ]
    }
   ],
   "source": [
    "text = \"The brown dog jumped over the lazy cheese; repeatedly. Without the cheese, there is boredom: the dog?\"\n",
    "\n",
    "print(re.sub(r'[.,;:!?-]', ' ', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives extra spaces which can be removed with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown dog jumped over the lazy cheese repeatedly Without the cheese there is boredom the dog \n"
     ]
    }
   ],
   "source": [
    "text = \"The brown dog jumped over the lazy cheese; repeatedly. Without the cheese, there is boredom: the dog?\"\n",
    "\n",
    "spaces = re.sub(r'[.,;:!?-]', ' ', text)\n",
    "\n",
    "print(re.sub(r'[ ]+', ' ', spaces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression looks weird at first, but a [good cheatsheet](https://pycon2016.regex.training/cheat-sheet) helps. Microsoft also makes [printable cheasheets](https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference), but there may be slight difference between implementations.\n",
    "\n",
    "### Getting rid of XML tags\n",
    "\n",
    "If you're working with web data, regex is useful for cleaning. For example, the 100MB wikiepedia dataset has lots of XML and HTML tag everywhere, which you normally don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['        <id>8029</id>', '      </contributor>', '      <minor />', '      <comment>adding cur_id=5: {{R from CamelCase}}</comment>', '      <text xml:space=\"preserve\">#REDIRECT [[Algeria]]{{R from CamelCase}}</text>', '    </revision>', '  </page>', '  <page>', '    <title>AmericanSamoa</title>', '    <id>6</id>']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./enwik8.txt\", \"r\") as f:\n",
    "    enwik8 = f.read().splitlines()\n",
    "\n",
    "print(enwik8[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, a researcher named [Matt Mahoney](http://mattmahoney.net/) wrote a nice perl script for cleaning that stuff out. The fastText team has made that script available [here](https://github.com/facebookresearch/fastText/blob/master/wikifil.pl) and I've translated it into python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notes', 'view of abu dhabi', 'for other uses see achilles disambiguation', 'for other uses of the name abraham lincoln see abraham lincoln disambiguation', 'infobox_philosopher']\n"
     ]
    }
   ],
   "source": [
    "cleaned_enwik8 = []\n",
    "\n",
    "# I've kept the comments in the code, but I've otherwise tweaked it to run in Python\n",
    "\n",
    "# Program to filter Wikipedia XML dumps to \"clean\" text consisting only of lowercase\n",
    "# letters (a-z, converted from A-Z), and spaces (never consecutive).  \n",
    "# All other characters are converted to spaces.  Only text which normally appears \n",
    "# in the web browser is displayed.  Tables are removed.  Image captions are \n",
    "# preserved.  Links are converted to normal text.  Digits are spelled out.\n",
    "\n",
    "# Written by Matt Mahoney, June 10, 2006.  This program is released to the public domain.\n",
    "for line in enwik8:\n",
    "    if \"<text\" in line.lower() and \"#redirect\" not in line.lower():\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"<.*>\", r\"\", line) # remove xml tags\n",
    "        line = re.sub(r\"&amp;\", r\"&\", line) # decode URL encoded chars\n",
    "        line = re.sub(r\"&lt;\", r\"<\", line)\n",
    "        line = re.sub(r\"&gt;\", r\">\", line)\n",
    "        line = re.sub(r\"<ref[^<]*<\\/ref>\", r\"\", line) # remove references <ref...> ... </ref>\n",
    "        line = re.sub(r\"<[^>]*>\", r\"\", line) # remove xhtml tags\n",
    "        line = re.sub(r\"\\[http:[^] ]*\", r\"[]\", line) # remove normal url, preserve visible text\n",
    "        line = re.sub(r\"\\|thumb\", \"\", line) # remove images links, preserve caption\n",
    "        line = re.sub(r\"\\|left\", \"\", line)\n",
    "        line = re.sub(r\"\\|right\", \"\", line)\n",
    "        line = re.sub(r\"\\|\\d+px\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\[image:[^\\[\\]]*\\|\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\[category:([^|\\]]*)[^]]*\\]\\]\", \"[[$1]]\", line) # show categories without markup\n",
    "        line = re.sub(r\"\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\", \"\", line) # remove links to other languages\n",
    "        line = re.sub(r\"\\[\\[[^\\|\\]]*\\|\", \"[[\", line) # remove wiki url, preserve visible text\n",
    "        line = re.sub(r\"\\{\\{[^\\}]*\\}\\}\", \"\", line) # remove {{icons}} and {tables}\n",
    "        line = re.sub(r\"\\{[^\\}]*\\}\", \"\", line) # remove [ and ]\n",
    "        line = re.sub(r\"\\[\", \"\", line)\n",
    "        line = re.sub(r\"\\]\", \"\", line)\n",
    "        line = re.sub(r\"&[^;]*;\", \"\", line) # remove URL encoded chars\n",
    "        # convert to lowercase letters and spaces, spell digits\n",
    "        line = \" \"+line+\" \"\n",
    "        line = re.sub(r\"0\", \" zero \", line)\n",
    "        line = re.sub(r\"1\", \" one \", line)\n",
    "        line = re.sub(r\"2\", \" two \", line)\n",
    "        line = re.sub(r\"3\", \" three \", line)\n",
    "        line = re.sub(r\"4\", \" four \", line)\n",
    "        line = re.sub(r\"5\", \" five \", line)\n",
    "        line = re.sub(r\"6\", \" six \", line)\n",
    "        line = re.sub(r\"7\", \" seven \", line)\n",
    "        line = re.sub(r\"8\", \" eight \", line)\n",
    "        line = re.sub(r\"9\", \" nine\", line)\n",
    "        line = re.sub(r\"[^\\w]+\", \" \", line)\n",
    "        line = re.sub(r\"[ ]+\", \" \", line)\n",
    "        line = line.strip()\n",
    "        if len(line) > 0 :\n",
    "            cleaned_enwik8.append(line)\n",
    "\n",
    "print(cleaned_enwik8[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short script scrubs the data clean and leaves the text behind.\n",
    "\n",
    "## Natural Language ToolKit\n",
    "\n",
    "The `nltk` python package has lots of tools to help you work with text. The following functions may all appear to be magic, but they're mostly based off of statistical model.\n",
    "\n",
    "You can find tokenizers and part-of-speech taggers for language other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# You will likely have to download nltk packages to use them\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "You can split text into tokens (words) using the punkt tokenizer model (`punkt`). Tokenization is extremely useful for natural language modelling.\n",
    "\n",
    "Notice that the punctuation is properly separated from the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'dog', 'jumped', 'over', 'the', 'lazy', 'cheese', ';', 'repeatedly', '.', 'Without', 'the', 'cheese', ',', 'there', 'is', 'boredom', ':', 'the', 'dog', '?']\n"
     ]
    }
   ],
   "source": [
    "paragram = \"The brown dog jumped over the lazy cheese; repeatedly. Without the cheese, there is boredom: the dog?\"\n",
    "\n",
    "print(nltk.word_tokenize(paragram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging\n",
    "\n",
    "The natural language toolkit can also do something called \"part of speech tagging\" (`averaged_perceptron_tagger` + `treebank`). It will identify the subjects, predicates, etc in your sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('cheese', 'NN'),\n",
       " (';', ':'),\n",
       " ('repeatedly', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Without', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cheese', 'NN'),\n",
       " (',', ','),\n",
       " ('there', 'EX'),\n",
       " ('is', 'VBZ'),\n",
       " ('boredom', 'NN'),\n",
       " (':', ':'),\n",
       " ('the', 'DT'),\n",
       " ('dog', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = nltk.word_tokenize(paragram)\n",
    "nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization\n",
    "\n",
    "Sentence tokenization is helpful when you want to feed sentences to your model but your raw data is in paragraphs. This uses the `punkt` tokenizer from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text\n",
      "Urbem Romam a principio reges habuere; libertatem et consulatum L. Brutus instituit. Dictaturae ad tempus sumebantur; neque decemviralis potestas ultra biennium, neque tribunorum militum consulare ius diu valuit. Non Cinnae, non Sullae longa dominatio; et Pompei Crassique potentia cito in Caesarem, Lepidi atque Antonii arma in Augustum cessere, qui cuncta discordiis civilibus fessa nomine principis sub imperium accepit.\n",
      "\n",
      "Tokenized sentences\n",
      "['Urbem Romam a principio reges habuere; libertatem et consulatum L. Brutus instituit.', 'Dictaturae ad tempus sumebantur; neque decemviralis potestas ultra biennium, neque tribunorum militum consulare ius diu valuit.', 'Non Cinnae, non Sullae longa dominatio; et Pompei Crassique potentia cito in Caesarem, Lepidi atque Antonii arma in Augustum cessere, qui cuncta discordiis civilibus fessa nomine principis sub imperium accepit.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Principio.txt\", \"r\") as f:\n",
    "    principio = \" \".join(f.readlines()).replace(\"\\n\", \"\")\n",
    "\n",
    "print(\"Raw text\")\n",
    "print(principio)\n",
    "print()\n",
    "print(\"Tokenized sentences\")\n",
    "print(nltk.sent_tokenize(principio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) algorithms crop words to their roots. They're a way of reducing your vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The        Stemmed: the        Changed: Y\n",
      "Original: quick      Stemmed: quick      Changed: \n",
      "Original: foxes      Stemmed: fox        Changed: Y\n",
      "Original: quickly    Stemmed: quick      Changed: Y\n",
      "Original: jumped     Stemmed: jump       Changed: Y\n",
      "Original: over       Stemmed: over       Changed: \n",
      "Original: the        Stemmed: the        Changed: \n",
      "Original: laziest    Stemmed: laziest    Changed: \n",
      "Original: dog        Stemmed: dog        Changed: \n",
      "Original: .          Stemmed: .          Changed: \n",
      "Original: The        Stemmed: the        Changed: Y\n",
      "Original: dogs       Stemmed: dog        Changed: Y\n",
      "Original: '          Stemmed: '          Changed: \n",
      "Original: owners     Stemmed: owner      Changed: Y\n",
      "Original: are        Stemmed: are        Changed: \n",
      "Original: saddened   Stemmed: sadden     Changed: Y\n",
      "Original: .          Stemmed: .          Changed: \n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "paragram = \"The quick foxes quickly jumped over the laziest dog. The dogs' owners are saddened.\"\n",
    "\n",
    "for word in nltk.word_tokenize(paragram):\n",
    "    stemmed = porter_stemmer.stem(word)\n",
    "    print(f\"Original: {word:<10} Stemmed: {stemmed:<10} Changed: {'Y'*(word!=stemmed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The `wordnet` [lemmatizer](https://en.wikipedia.org/wiki/Lemmatisation) will group inflections together, which serves to reduce your vocabulary size. An inflection is the modification of a word for various reasons. Examples are words in plural with an `s` or verbs that whether one person or many people perform it.\n",
    "\n",
    "The `lemmatize()` takes a `pos` argument, but I haven't been able to find this well-explained online. As a basic step I suggest using POS tagging to distinguish nouns and verbs. Below you can see that this helps lemmatize `learned` alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I          Lemmatized: I          Changed:       POS: PRP\n",
      "Original: learned    Lemmatized: learn      Changed: Y     POS: VBD\n",
      "Original: Latin      Lemmatized: Latin      Changed:       POS: NNP\n",
      "Original: .          Lemmatized: .          Changed:       POS: .\n",
      "Original: The        Lemmatized: The        Changed:       POS: DT\n",
      "Original: learned    Lemmatized: learned    Changed:       POS: JJ\n",
      "Original: learn      Lemmatized: learn      Changed:       POS: NN\n",
      "Original: Latin      Lemmatized: Latin      Changed:       POS: NNP\n",
      "Original: .          Lemmatized: .          Changed:       POS: .\n",
      "Original: Therefore  Lemmatized: Therefore  Changed:       POS: NNP\n",
      "Original: I          Lemmatized: I          Changed:       POS: PRP\n",
      "Original: am         Lemmatized: be         Changed: Y     POS: VBP\n",
      "Original: learned    Lemmatized: learn      Changed: Y     POS: VBN\n",
      "Original: .          Lemmatized: .          Changed:       POS: .\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "paragram = \"I learned Latin. The learned learn Latin. Therefore I am learned.\"\n",
    "\n",
    "for word, pos in nltk.pos_tag(nltk.word_tokenize(paragram)):\n",
    "    if pos[0] == \"V\": pos_arg = \"v\"\n",
    "    else: pos_arg = \"n\"\n",
    "    lemmatized = wordnet_lemmatizer.lemmatize(word, pos=pos_arg)\n",
    "    print(f\"Original: {word:<10} Lemmatized: {lemmatized:<10} Changed: {'Y'*(word!=lemmatized):<5} POS: {pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy library\n",
    "\n",
    "The `spacy` library can also work with natural text. I find it has a more modern feel than `nltk`. I recommend visiting their [website](https://spacy.io/usage/) since it has a lot of examples.\n",
    "\n",
    "You have to download and install models before you can use them.\n",
    "\n",
    "```\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "After you've installed a model, you can then load it in spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech, lemmas, etc\n",
    "\n",
    "This [code snippet](https://spacy.io/usage/linguistic-features#section-pos-tagging) from the website shows how to quickly do various transformations on your text. You can quickly get information from your text.\n",
    "\n",
    "Notice that spaCy is making some errors with its POS tagging. It always interprets `learned` as a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text       lemma      pos        tag        dep        shape      is_alpha   is_stop   \n",
      "--------------------------------------------------------------------------------\n",
      "I          -PRON-     PRON       PRP        nsubj      X          1          0         \n",
      "learned    learn      VERB       VBD        ROOT       xxxx       1          0         \n",
      "Latin      latin      PROPN      NNP        dobj       Xxxxx      1          0         \n",
      ".          .          PUNCT      .          punct      .          0          0         \n",
      "The        the        DET        DT         det        Xxx        1          0         \n",
      "learned    learn      VERB       VBN        nsubj      xxxx       1          0         \n",
      "learn      learn      VERB       VBP        ROOT       xxxx       1          0         \n",
      "Latin      latin      PROPN      NNP        dobj       Xxxxx      1          0         \n",
      ".          .          PUNCT      .          punct      .          0          0         \n",
      "Therefore  therefore  ADV        RB         advmod     Xxxxx      1          0         \n",
      "I          -PRON-     PRON       PRP        nsubjpass  X          1          0         \n",
      "am         be         VERB       VBP        auxpass    xx         1          1         \n",
      "learned    learn      VERB       VBN        ROOT       xxxx       1          0         \n",
      ".          .          PUNCT      .          punct      .          0          0         \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"I learned Latin. The learned learn Latin. Therefore I am learned.\")\n",
    "\n",
    "print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    \"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"shape\", \"is_alpha\", \"is_stop\"\n",
    "     ))\n",
    "print(80*\"-\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop\n",
    "         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy conveniently draws visuals for you. The `dep` style means dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1010\" height=\"317.0\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">dog</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">jumped</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">cheese.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,62.0 285.0,62.0 285.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M190,182.0 C190,122.0 280.0,122.0 280.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,184.0 L182,172.0 198,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M310,182.0 C310,122.0 400.0,122.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,184.0 L302,172.0 318,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,122.0 520.0,122.0 520.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520.0,184.0 L528.0,172.0 512.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M670,182.0 C670,62.0 885.0,62.0 885.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,184.0 L662,172.0 678,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M790,182.0 C790,122.0 880.0,122.0 880.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,184.0 L782,172.0 798,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M550,182.0 C550,2.0 890.0,2.0 890.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M890.0,184.0 L898.0,172.0 882.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u\"The brown dog jumped over the lazy cheese.\")\n",
    "\n",
    "spacy.displacy.render(doc, style=\"dep\", jupyter=True, options={\"distance\" : 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a bigger better model\n",
    "\n",
    "I'm going to switch to the medium model `en_core_web_md`, which is 120MB. This will make the following examples work better.\n",
    "\n",
    "There seems to be a problem with timeout when installing these larger models. You can use the [`pip install` instructions](https://spacy.io/usage/models#download-pip) in the guide with the `--timeout=10000` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "\n",
    "spaCy can also do named entity recognition. A model is used here, so the label is not 100% accurate (seems trained on current events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                 start_char end_char   label     \n",
      "--------------------------------------------------\n",
      "Cinna                18         23         PERSON    \n",
      "Sulla                28         33         PERSON    \n",
      "Pompeius             58         66         ORG       \n",
      "Crassus              74         81         ORG       \n",
      "Caesar               102        108        PRODUCT   \n",
      "Lepidus              122        129        WORK_OF_ART\n",
      "Antonius             134        142        PERSON    \n",
      "Augustus             150        158        DATE      \n",
      "Princeps             251        259        ORG       \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"\"\"The despotisms of Cinna and Sulla were brief; \"\"\"\n",
    "          \"\"\"the rule of Pompeius and of Crassus soon yielded before Caesar; \"\"\"\n",
    "          \"\"\"the arms of Lepidus and Antonius before Augustus; \"\"\"\n",
    "          \"\"\"who, when the world was wearied by civil strife, subjected it to empire under the title of Princeps.\"\"\")\n",
    "    \n",
    "print(\"{:<20} {:<10} {:<10} {:<10}\".format(\n",
    "    \"text\", \"start_char\", \"end_char\", \"label\"\n",
    "     ))\n",
    "print(50*\"-\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(\"{:<20} {:<10} {:<10} {:<10}\".format(\n",
    "        ent.text, ent.start_char, ent.end_char, ent.label_\n",
    "         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, spaCy can draw these nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">The despotisms of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Cinna\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sulla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " were brief; the rule of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Pompeius\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Crassus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " soon yielded before \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Caesar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       "; the arms of \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Lepidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Antonius\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " before \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Augustus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "; who, when the world was wearied by civil strife, subjected it to empire under the title of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Princeps\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"\"\"The despotisms of Cinna and Sulla were brief; \"\"\"\n",
    "          \"\"\"the rule of Pompeius and of Crassus soon yielded before Caesar; \"\"\"\n",
    "          \"\"\"the arms of Lepidus and Antonius before Augustus; \"\"\"\n",
    "          \"\"\"who, when the world was wearied by civil strife, subjected it to empire under the title of Princeps.\"\"\")\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors\n",
    "\n",
    "The spaCy library also comes with pretrained word embeddings. They recommend using a larger model than the default `en` (the default is \"sm\" for small), so the `md` model we got above is suitable.\n",
    "\n",
    "You can then check the similarity of tokens. Ham and bacon are similar to one another, and cars and trucks are similar to one another.\n",
    "\n",
    "The examples I show below can also be found in the [spaCy vector examples](https://spacy.io/usage/vectors-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n",
      "-----\n",
      "ham        ham        Similarity:  1.00\n",
      "ham        bacon      Similarity:  0.74\n",
      "ham        cars       Similarity:  0.10\n",
      "ham        trucks     Similarity:  0.12\n",
      "ham\n",
      "-----\n",
      "bacon\n",
      "-----\n",
      "bacon      ham        Similarity:  0.74\n",
      "bacon      bacon      Similarity:  1.00\n",
      "bacon      cars       Similarity:  0.11\n",
      "bacon      trucks     Similarity:  0.14\n",
      "bacon\n",
      "-----\n",
      "cars\n",
      "-----\n",
      "cars       ham        Similarity:  0.10\n",
      "cars       bacon      Similarity:  0.11\n",
      "cars       cars       Similarity:  1.00\n",
      "cars       trucks     Similarity:  0.72\n",
      "cars\n",
      "-----\n",
      "trucks\n",
      "-----\n",
      "trucks     ham        Similarity:  0.12\n",
      "trucks     bacon      Similarity:  0.14\n",
      "trucks     cars       Similarity:  0.72\n",
      "trucks     trucks     Similarity:  1.00\n",
      "trucks\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'ham bacon cars trucks')\n",
    "\n",
    "for token1 in tokens:\n",
    "    print(f\"{token1}\\n-----\")\n",
    "    for token2 in tokens:\n",
    "        print(f\"{token1.text:<10} {token2.text:<10} Similarity: {token1.similarity(token2):5.2f}\")\n",
    "    print(f\"{token1}\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up vectors, and if your word isn't in the vocabulary you'll get nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token      has_vector     vector_norm is_oov    \n",
      "---------------------------------------------\n",
      "ham        1                     7.30 0         \n",
      "bus        1                     7.10 0         \n",
      "hambus     0                     0.00 1         \n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'ham bus hambus')\n",
    "\n",
    "print(\"{:<10} {:<10} {:>15} {:<10}\".format(\n",
    "    \"token\", \"has_vector\", \"vector_norm\", \"is_oov\"\n",
    "     ))\n",
    "print(45*\"-\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(\"{:<10} {:<10} {:15.2f} {:<10}\".format(\n",
    "        token.text, token.has_vector, token.vector_norm, token.is_oov\n",
    "         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the vector value with the `.vector` property. You get a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(nlp(u'ham').vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use these vectors in a model, you can retrieve them all from a sentence and then average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens = nlp(u'The brown dog jumped over the lazy cheese.')\n",
    "\n",
    "print(np.mean([token.vector for token in tokens], axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this was useful. Please report any issues!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
