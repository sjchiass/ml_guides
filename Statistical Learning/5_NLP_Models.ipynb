{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load various models from scikit-learn's library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# also get some metrics to try\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized, fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_raw = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "ng_test_raw = fetch_20newsgroups(subset=\"test\", remove=(\"headers\", \"footers\", \"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(vectorizer):\n",
    "    vectorizer.fit(ng_train_raw.data)\n",
    "\n",
    "    ng_train_x = vectorizer.transform(ng_train_raw.data)\n",
    "    ng_test_x = vectorizer.transform(ng_test_raw.data)\n",
    "\n",
    "    return {\"X\" : ng_train_x, \"y\" : ng_train_raw.target}, {\"X\" : ng_test_x, \"y\" : ng_test_raw.target}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = {\"LogisticRegression\" : LogisticRegression(solver = \"lbfgs\", n_jobs = -1), \n",
    "                  \"RandomForestClassifier\" : RandomForestClassifier(n_jobs = -1), \n",
    "                  \"KNeighborsClassifier\" : KNeighborsClassifier(), \n",
    "                  \"GradientBoostingClassifier\" : GradientBoostingClassifier(),\n",
    "                  \"MLPClassifier\" : MLPClassifier(),\n",
    "                  \"MLPClassifier x2\" : MLPClassifier(hidden_layer_sizes=(100, 100, ))\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model):\n",
    "    model.fit(ng_train[\"X\"], ng_train[\"y\"])\n",
    "    train_score = accuracy_score(y_true = ng_train[\"y\"], y_pred = model.predict(X=ng_train[\"X\"]))\n",
    "    print(\"Train score {0}:\".format(train_score))\n",
    "    test_score = accuracy_score(y_true = ng_test[\"y\"], y_pred = model.predict(X=ng_test[\"X\"]))\n",
    "    print(\"Test score {0}:\".format(test_score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(x):\n",
    "    return re.sub(r\"[ ]+\", \" \", re.sub(r\"[^\\w]+\", \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ng_train, ng_test = vectorize(TfidfVectorizer(max_features = 50000, preprocessor=lambda x: preprocessor(x)))\n",
    "\n",
    "for model in list_of_models:\n",
    "    model_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ng_train, ng_test = vectorize(TfidfVectorizer(max_features = 50000, preprocessor=lambda x: preprocessor(x), ngram_range = [2, 2]))\n",
    "\n",
    "for model in list_of_models:\n",
    "    model_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# words/ngrams with hashing\n",
    "ng_train, ng_test = vectorize(HashingVectorizer(n_features = 50000, preprocessor=lambda x: preprocessor(x)))\n",
    "\n",
    "for model in list_of_models:\n",
    "    model_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words/ngrams with hashing\n",
    "ng_train, ng_test = vectorize(HashingVectorizer(n_features = 50000, ngram_range = [2, 2]))\n",
    "\n",
    "for model in list_of_models:\n",
    "    model_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.973660951034117:\n",
      "Test score 0.6825544344131704:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ng_train, ng_test = vectorize(HashingVectorizer(stop_words='english', n_features=2**18))\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), activation=\"identity\")\n",
    "model.fit(ng_train[\"X\"], ng_train[\"y\"])\n",
    "train_score = accuracy_score(y_true = ng_train[\"y\"], y_pred = model.predict(X=ng_train[\"X\"]))\n",
    "print(\"Train score {0}:\".format(train_score))\n",
    "test_score = accuracy_score(y_true = ng_test[\"y\"], y_pred = model.predict(X=ng_test[\"X\"]))\n",
    "print(\"Test score {0}:\".format(test_score))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144, 100)\n",
      "[[ 3.69646000e-02 -1.09253902e-01  9.03193336e-02 ... -2.23141568e-02\n",
      "  -9.29570301e-02 -1.48118969e-01]\n",
      " [ 4.96444005e-69  9.68094341e-69 -5.11246418e-67 ... -6.45638112e-67\n",
      "   5.67443303e-68 -3.77074294e-67]\n",
      " [ 3.32964781e-68 -1.16161984e-68  5.18097787e-67 ...  6.52557179e-67\n",
      "   5.76763022e-67  1.35773091e-68]\n",
      " ...\n",
      " [ 1.13088042e-01  1.16625357e-01 -1.65920495e-01 ... -1.18982418e-01\n",
      "   2.33227120e-02  5.99042944e-02]\n",
      " [-2.07373212e-67  3.49972255e-67 -5.68474464e-67 ... -3.63965325e-67\n",
      "  -3.60479684e-67 -9.59490359e-68]\n",
      " [-8.92083087e-69 -3.42089809e-68  1.82125711e-68 ... -6.15112787e-67\n",
      "  -5.52895534e-67 -1.45803300e-67]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.coefs_[0]\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144, 100)\n",
      "(7532, 262144)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(ng_test[\"X\"].todense().shape)\n",
    "\n",
    "train_split = np.array_split(ng_train[\"X\"].todense(), 1000)\n",
    "\n",
    "test_split = np.array_split(ng_test[\"X\"].todense(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_embed = np.dot(ng_train[\"X\"], embeddings)\n",
    "#x_test_embed = np.dot(ng_test[\"X\"], embeddings)\n",
    "\n",
    "x_train_embed = np.vstack([np.dot(i, embeddings) for i in train_split])\n",
    "x_test_embed = np.vstack([np.dot(i, embeddings) for i in test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-04d22d2c588b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mng_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mng_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} train score {1}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mng_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_models' is not defined"
     ]
    }
   ],
   "source": [
    "for name, model in list_of_models.items():\n",
    "    model.fit(x_train_embed, ng_train[\"y\"])\n",
    "    train_score = accuracy_score(y_true = ng_train[\"y\"], y_pred = model.predict(X=x_train_embed))\n",
    "    print(\"{0} train score {1}:\".format(name, train_score))\n",
    "    test_score = accuracy_score(y_true = ng_test[\"y\"], y_pred = model.predict(X=x_test_embed))\n",
    "    print(\"{0} test score {1}:\".format(name, test_score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Numpy_w_Classes as nwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train, ng_test = vectorize(HashingVectorizer(n_features = 50000, ngram_range = [2, 2], preprocessor=lambda x: preprocessor(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ng_train[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "train_x = ng_train[\"X\"]\n",
    "test_x = ng_test[\"X\"]\n",
    "\n",
    "train_labels = ng_train[\"y\"]\n",
    "test_labels = ng_test[\"y\"]\n",
    "\n",
    "train_y = np.eye(20)[ng_train[\"y\"]]\n",
    "test_y = np.eye(20)[ng_test[\"y\"]]\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "batch_pos = list(range(0, train_x.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNet = nwc.NeuralNetwork()\n",
    "\n",
    "NNet.layers.append(nwc.InputLayer(input_size=50000))\n",
    "NNet.layers.append(nwc.DenseLayer(predecessor=NNet.layers[-1], hidden=100, use_bias=False, positive_params=False))\n",
    "#NNet.layers.append(nwc.BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(nwc.DenseLayer(predecessor=NNet.layers[-1], hidden=50, use_bias=False, positive_params=False))\n",
    "NNet.layers.append(nwc.BatchNormLayer(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(nwc.ReLUActivation(predecessor=NNet.layers[-1]))\n",
    "NNet.layers.append(nwc.DropoutLayer(predecessor=NNet.layers[-1], probability=0.75))\n",
    "NNet.layers.append(nwc.SoftmaxCrossEntropy(predecessor=NNet.layers[-1], hidden=20, use_bias=True))\n",
    "\n",
    "NNet.optimizer = nwc.AdamOptimizer(NNet.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    for b in batch_pos:\n",
    "        batch_x = train_x[b:b+batch_size]\n",
    "        batch_y = train_y[b:b+batch_size]\n",
    "        batch_labels = train_labels[b:b+batch_size]\n",
    "        NNet.feed_forward(batch_x)\n",
    "        NNet.back_propagation(batch_y)\n",
    "        NNet.optimizer.step(lr/batch_size)\n",
    "        NNet.zero_gradients()\n",
    "    train_predicted = NNet.evaluate(train_x)\n",
    "    test_predicted = NNet.evaluate(test_x)\n",
    "    print(\"Epoch {:3d} stats: lr {:7.5f}, batch size {:3d}, validation loss {:6.2f}\".format(ep, lr, batch_size, \n",
    "        nwc.CrossEntropy(test_predicted, test_y)))\n",
    "    print(\"  training accuracy {:6.2f}%, validation accuracy {:6.2f}%\".format(\n",
    "        100 * nwc.accuracy(train_predicted, train_labels), 100 * nwc.accuracy(test_predicted, test_labels)))\n",
    "    #lr *= 0.8\n",
    "    p = np.random.permutation(train_x.shape[0])\n",
    "    train_x = train_x[p,:]\n",
    "    train_labels = train_labels[p]\n",
    "    train_y = train_y[p,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
