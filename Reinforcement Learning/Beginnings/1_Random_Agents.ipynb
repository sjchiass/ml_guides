{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agents\n",
    "\n",
    "This notebook covers basic usage of OpenAI gym, including how to display your RL agents as GIFs in Jupyter.\n",
    "\n",
    "Let's start by importing our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert HTML into Jupyter for it to display our GIFs\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Transform gym frames to GIFs\n",
    "# PIL is the pillow library\n",
    "import PIL.Image\n",
    "\n",
    "# numpy to deliver arrays to PIL\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI gym\n",
    "import gym\n",
    "\n",
    "# Import local script the defines simple linear agents\n",
    "import agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's cartpole?\n",
    "\n",
    "I'm still learning this stuff, so I'll be using one of the simplest RL games: cartpole.\n",
    "\n",
    "Cartpole is basically a pole in a little cart. If the pole falls over, you lose. Your goal is to move that cart left or right and keep that little pole safe.\n",
    "\n",
    "You can view a very useful description of cartpole [here](https://github.com/openai/gym/wiki/CartPole-v1). That information is crucial for knowing what kind of inputs and outputs cartpole works with. The OpenAI website also has a page with a little visual demo [here](https://gym.openai.com/envs/CartPole-v1/).\n",
    "\n",
    "In case you're curious, cartpole-v1 just seems to be cartpole-v0 with a higher time limit. Version 0 terminates at 200 time steps, while version 1 terminates at 500. I prefer version 1 since it's a bit more challenging.\n",
    "\n",
    "## Basic gym\n",
    "\n",
    "Here's the sample code given by the [gym documentation](https://gym.openai.com/docs/#environments). I've changed it a little bit so that the frames are imported into pillow (PIL). pillow can then save the sequence of frames as a GIF, which is easy to work with.\n",
    "\n",
    "If you run this code, you'll get a small window pop-up as the game as played.\n",
    "\n",
    "Here cartpole is being played randomly. The agent randomly decides whether to move the cart left of right. The pole doesn't stand a chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/random_actor.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Random actor\n",
    "observation = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "for t in range(1000):\n",
    "    # Render into buffer. \n",
    "    # You will still see the window.\n",
    "    frames.append(PIL.Image.fromarray(env.render(mode = 'rgb_array'), \"RGB\"))\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "# Save the GIF\n",
    "frames[0].save('./images/random_actor.gif', format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)\n",
    "\n",
    "# Display the GIF in Jupyter\n",
    "HTML('<img src=\"./images/random_actor.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the simple model randomly\n",
    "\n",
    "Let's try something else. I've written a little `LinearAgent` class in a separate python script file. This linear agent is just a logistic regression of the form $\\sigma(w_0 + w_1 a + w_2 b + w_3 c + w_4 d) = y$, where $w_0$ is an intercept term and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).\n",
    "\n",
    "The code below randomly initializes the weights and plays the game. It's just guessing at a solution and trying it. Sometimes it even solves the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[  2] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  3] Minimum  17.0 Maximum  53.0 Average  29.1\n",
      "[  4] Minimum   7.0 Maximum  10.0 Average   8.2\n",
      "[  5] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  6] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  7] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  8] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[  9] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 10] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 11] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 12] Minimum  13.0 Maximum  30.0 Average  20.4\n",
      "[ 13] Minimum   7.0 Maximum   9.0 Average   8.3\n",
      "[ 14] Minimum  32.0 Maximum  83.0 Average  45.0\n",
      "[ 15] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 16] Minimum   7.0 Maximum  11.0 Average   9.0\n",
      "[ 17] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 18] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 19] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 20] Minimum   7.0 Maximum  10.0 Average   8.2\n",
      "[ 21] Minimum 499.0 Maximum 499.0 Average 499.0\n",
      "[ 22] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 23] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 24] Minimum  13.0 Maximum  75.0 Average  27.8\n",
      "[ 25] Minimum   7.0 Maximum  10.0 Average   8.5\n",
      "499.0\n",
      "<agents.LinearAgent object at 0x7fd653391c18>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "best_w = None\n",
    "best_score = 0\n",
    "# Simple model\n",
    "for m in range(25):\n",
    "    actor = agents.LinearAgent()\n",
    "    scores = []\n",
    "    for i in range(100):\n",
    "        observation = env.reset()\n",
    "        score = 0\n",
    "        frames = []\n",
    "        for t in range(1000):\n",
    "            action = actor.predict(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            score += reward\n",
    "        scores.append(score)\n",
    "    print(f\"[{m+1:3}] Minimum {min(scores):5.1f} Maximum {max(scores):5.1f} Average {sum(scores)/len(scores):5.1f}\")\n",
    "    if sum(scores)/len(scores) > best_score:\n",
    "        best_score = sum(scores)/len(scores)\n",
    "        best_w = actor\n",
    "env.close()\n",
    "print(best_score)\n",
    "print(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./images/random_simple.gif'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f\"<img src='{best_w.render('random_simple.gif')}'>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the second-order agent randomly\n",
    "\n",
    "I also try a second-order linear agent of the form $\\sigma(w_0 + w_1 a + w_2 b + w_3 c + w_4 d + w_5 e + w_6 f + w_7 g + w_8 h) = y$. This try I try it 100 times. Sometimes the agents solves the games, usually it doesn't.\n",
    "\n",
    "That's about it. Try re-running the agents to see if you can hit a lucky solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  2] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  3] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[  4] Minimum   9.0 Maximum  17.0 Average  11.8\n",
      "[  5] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  6] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[  7] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[  8] Minimum  10.0 Maximum  17.0 Average  12.6\n",
      "[  9] Minimum   8.0 Maximum  12.0 Average  10.0\n",
      "[ 10] Minimum   9.0 Maximum  14.0 Average  11.9\n",
      "[ 11] Minimum   7.0 Maximum  21.0 Average   9.0\n",
      "[ 12] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 13] Minimum   7.0 Maximum  49.0 Average  10.0\n",
      "[ 14] Minimum   9.0 Maximum  19.0 Average  12.8\n",
      "[ 15] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 16] Minimum  25.0 Maximum 103.0 Average  46.0\n",
      "[ 17] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 18] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 19] Minimum  30.0 Maximum 204.0 Average  83.2\n",
      "[ 20] Minimum   8.0 Maximum  13.0 Average  10.1\n",
      "[ 21] Minimum   7.0 Maximum  10.0 Average   8.2\n",
      "[ 22] Minimum  11.0 Maximum  19.0 Average  14.8\n",
      "[ 23] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 24] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 25] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 26] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 27] Minimum  17.0 Maximum  40.0 Average  24.0\n",
      "[ 28] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 29] Minimum   7.0 Maximum  10.0 Average   8.6\n",
      "[ 30] Minimum   7.0 Maximum  12.0 Average   9.1\n",
      "[ 31] Minimum  22.0 Maximum 113.0 Average  46.3\n",
      "[ 32] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 33] Minimum   8.0 Maximum  14.0 Average  11.0\n",
      "[ 34] Minimum  13.0 Maximum  29.0 Average  19.7\n",
      "[ 35] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 36] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 37] Minimum  11.0 Maximum  19.0 Average  14.5\n",
      "[ 38] Minimum   7.0 Maximum 107.0 Average  11.2\n",
      "[ 39] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 40] Minimum   7.0 Maximum  10.0 Average   8.7\n",
      "[ 41] Minimum  12.0 Maximum  19.0 Average  15.4\n",
      "[ 42] Minimum   7.0 Maximum  11.0 Average   8.8\n",
      "[ 43] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 44] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 45] Minimum  14.0 Maximum  27.0 Average  20.6\n",
      "[ 46] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 47] Minimum   7.0 Maximum  10.0 Average   8.5\n",
      "[ 48] Minimum  31.0 Maximum  53.0 Average  40.4\n",
      "[ 49] Minimum  10.0 Maximum  21.0 Average  15.1\n",
      "[ 50] Minimum   7.0 Maximum  10.0 Average   8.2\n",
      "[ 51] Minimum  11.0 Maximum  19.0 Average  14.5\n",
      "[ 52] Minimum  12.0 Maximum  20.0 Average  16.5\n",
      "[ 53] Minimum  12.0 Maximum  20.0 Average  15.7\n",
      "[ 54] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 55] Minimum  10.0 Maximum  19.0 Average  13.8\n",
      "[ 56] Minimum   7.0 Maximum  10.0 Average   8.5\n",
      "[ 57] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 58] Minimum   7.0 Maximum  11.0 Average   9.2\n",
      "[ 59] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 60] Minimum  16.0 Maximum  34.0 Average  25.1\n",
      "[ 61] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 62] Minimum  10.0 Maximum  23.0 Average  15.3\n",
      "[ 63] Minimum   7.0 Maximum  10.0 Average   8.2\n",
      "[ 64] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 65] Minimum  12.0 Maximum  20.0 Average  15.4\n",
      "[ 66] Minimum   7.0 Maximum  10.0 Average   8.5\n",
      "[ 67] Minimum  12.0 Maximum  21.0 Average  15.3\n",
      "[ 68] Minimum  18.0 Maximum  32.0 Average  23.6\n",
      "[ 69] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 70] Minimum  31.0 Maximum  95.0 Average  45.3\n",
      "[ 71] Minimum   7.0 Maximum  10.0 Average   8.7\n",
      "[ 72] Minimum  24.0 Maximum  43.0 Average  34.3\n",
      "[ 73] Minimum   7.0 Maximum  29.0 Average  16.5\n",
      "[ 74] Minimum  22.0 Maximum  47.0 Average  30.6\n",
      "[ 75] Minimum  10.0 Maximum  18.0 Average  13.9\n",
      "[ 76] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 77] Minimum   7.0 Maximum  10.0 Average   8.6\n",
      "[ 78] Minimum  25.0 Maximum 173.0 Average  47.8\n",
      "[ 79] Minimum  13.0 Maximum  28.0 Average  20.2\n",
      "[ 80] Minimum   7.0 Maximum  11.0 Average   9.2\n",
      "[ 81] Minimum   9.0 Maximum  17.0 Average  13.0\n",
      "[ 82] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 83] Minimum  15.0 Maximum  30.0 Average  21.0\n",
      "[ 84] Minimum  10.0 Maximum  20.0 Average  14.8\n",
      "[ 85] Minimum  16.0 Maximum  26.0 Average  21.2\n",
      "[ 86] Minimum   7.0 Maximum  21.0 Average  12.6\n",
      "[ 87] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 88] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 89] Minimum   7.0 Maximum  10.0 Average   8.5\n",
      "[ 90] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 91] Minimum  52.0 Maximum 499.0 Average 173.1\n",
      "[ 92] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "[ 93] Minimum   7.0 Maximum  10.0 Average   8.2\n",
      "[ 94] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 95] Minimum   8.0 Maximum  14.0 Average  11.0\n",
      "[ 96] Minimum  14.0 Maximum  26.0 Average  19.8\n",
      "[ 97] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 98] Minimum   7.0 Maximum  10.0 Average   8.4\n",
      "[ 99] Minimum  25.0 Maximum 145.0 Average  54.0\n",
      "[100] Minimum   7.0 Maximum  10.0 Average   8.3\n",
      "173.09\n",
      "<agents.LinearAgent object at 0x7fd64d43a710>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "best_w = None\n",
    "best_score = 0\n",
    "# Complex model\n",
    "for m in range(100):\n",
    "    actor = agents.LinearAgent(order=2)\n",
    "    scores = []\n",
    "    for i in range(100):\n",
    "        observation = env.reset()\n",
    "        score = 0\n",
    "        frames = []\n",
    "        for t in range(1000):\n",
    "            action = actor.predict(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            score += reward\n",
    "        scores.append(score)\n",
    "    print(f\"[{m+1:3}] Minimum {min(scores):5.1f} Maximum {max(scores):5.1f} Average {sum(scores)/len(scores):5.1f}\")\n",
    "    if sum(scores)/len(scores) > best_score:\n",
    "        best_score = sum(scores)/len(scores)\n",
    "        best_w = actor\n",
    "env.close()\n",
    "print(best_score)\n",
    "print(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./images/random_complex.gif'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f\"<img src='{best_w.render('random_complex.gif')}'>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
