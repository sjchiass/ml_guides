{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lecture 9, Numpy code only\n",
    "\n",
    "This code appears in the main notebook, but here it is on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For plotting things\n",
    "from sklearn.datasets import load_digits # Data to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the main part of the code. It will train itself to recognize the digits. The line with `MyNeuralNetwork = NeuralNetwork(64, 64, 10)` is the one you want to modify to change the input, hidden, and output sizes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1, batch  50, lr 0.10000, loss   2.23\n",
      "epoch   1, batch 100, lr 0.10000, loss   1.93\n",
      "epoch   1, batch 150, lr 0.10000, loss   1.60\n",
      "accuracy  80.97%\n",
      "epoch   2, batch  50, lr 0.07500, loss   1.30\n",
      "epoch   2, batch 100, lr 0.07500, loss   1.09\n",
      "epoch   2, batch 150, lr 0.07500, loss   1.06\n",
      "accuracy  87.03%\n",
      "epoch   3, batch  50, lr 0.05625, loss   1.00\n",
      "epoch   3, batch 100, lr 0.05625, loss   0.85\n",
      "epoch   3, batch 150, lr 0.05625, loss   0.86\n",
      "accuracy  89.87%\n",
      "epoch   4, batch  50, lr 0.04219, loss   0.90\n",
      "epoch   4, batch 100, lr 0.04219, loss   0.73\n",
      "epoch   4, batch 150, lr 0.04219, loss   0.76\n",
      "accuracy  91.71%\n",
      "epoch   5, batch  50, lr 0.03164, loss   0.85\n",
      "epoch   5, batch 100, lr 0.03164, loss   0.67\n",
      "epoch   5, batch 150, lr 0.03164, loss   0.71\n",
      "accuracy  92.49%\n",
      "epoch   6, batch  50, lr 0.02373, loss   0.83\n",
      "epoch   6, batch 100, lr 0.02373, loss   0.64\n",
      "epoch   6, batch 150, lr 0.02373, loss   0.68\n",
      "accuracy  93.10%\n",
      "epoch   7, batch  50, lr 0.01780, loss   0.82\n",
      "epoch   7, batch 100, lr 0.01780, loss   0.61\n",
      "epoch   7, batch 150, lr 0.01780, loss   0.66\n",
      "accuracy  93.88%\n",
      "epoch   8, batch  50, lr 0.01335, loss   0.81\n",
      "epoch   8, batch 100, lr 0.01335, loss   0.60\n",
      "epoch   8, batch 150, lr 0.01335, loss   0.65\n",
      "accuracy  94.27%\n",
      "epoch   9, batch  50, lr 0.01001, loss   0.81\n",
      "epoch   9, batch 100, lr 0.01001, loss   0.58\n",
      "epoch   9, batch 150, lr 0.01001, loss   0.64\n",
      "accuracy  94.44%\n",
      "epoch  10, batch  50, lr 0.00751, loss   0.81\n",
      "epoch  10, batch 100, lr 0.00751, loss   0.57\n",
      "epoch  10, batch 150, lr 0.00751, loss   0.64\n",
      "accuracy  94.44%\n"
     ]
    }
   ],
   "source": [
    "# This class stores the NN's parameters\n",
    "class NeuralNetwork:\n",
    "    # This is the constructor: it's called as the object is created\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.b1 = np.random.normal(size=(1,hidden_size))\n",
    "        self.w1 = np.random.normal(size=(input_size,hidden_size))\n",
    "        self.b2 = np.random.normal(size=(1,hidden_size))\n",
    "        self.w2 = np.random.normal(size=(hidden_size,hidden_size))\n",
    "        self.b_out = np.random.normal(size=(1,output_size))\n",
    "        self.w_out = np.random.normal(size=(hidden_size,output_size))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# This is the negative log-likelihood cost\n",
    "def NLLcost(a_out, y):\n",
    "    return -np.mean(np.sum(y*np.log(a_out) + (1.0-y)*np.log(1.0-a_out), 1))\n",
    "\n",
    "# Calculating accuracy\n",
    "def accuracy(a_out, labels):\n",
    "    # The following will return True if the maximum index of the output activation\n",
    "    # is the same as the label\n",
    "    tests = a_out.argmax(axis=1) == labels\n",
    "    return(tests.sum() / a_out.shape[0])\n",
    "\n",
    "# Feed forward\n",
    "def feedforward(x, NN):\n",
    "    # as you can see, feed forward is short and sweet\n",
    "    z1 = x @ NN.w1 + NN.b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ NN.w2 + NN.b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z_out = a2 @ NN.w_out + NN.b_out\n",
    "    a_out = sigmoid(z_out)\n",
    "    return a1, a2, a_out\n",
    "\n",
    "# Back propagation\n",
    "# You can see all of the steps written out a few sections above\n",
    "def backprop(x, y, NN, a1, a2, a_out):\n",
    "    # prepare a row vector of ones of width equal to the batch size\n",
    "    ones = np.ones((1, x.shape[0])) # this ensures the bias gradient is the right shape\n",
    "    # output parameters\n",
    "    delta = (a_out - y) # the error\n",
    "    w_out_grad = a2.transpose() @ delta # gradient for w_out\n",
    "    b_out_grad = ones @ delta # gradient for b_out\n",
    "    delta_out = delta @ NN.w_out.transpose() # the delta passed on to the next layer\n",
    "    # layer 2 parameters\n",
    "    prime2 = a2 * (1.0 - a2) # the derivative of the sigmoid\n",
    "    w2_grad = a1.transpose() @ (prime2 * delta_out) # w2 gradient\n",
    "    b2_grad = ones @ (prime2 * delta_out) # b2 gradient\n",
    "    delta2 = delta_out * prime2 @ NN.w2.transpose() # layer 2's delta\n",
    "    # layer 1 parameters\n",
    "    prime1 = a1 * (1.0 - a1) # derivative of the sigmoid\n",
    "    w1_grad = x.transpose() @ (prime1 * delta2) # w1 gradient\n",
    "    b1_grad = ones @ (prime1 * delta2) # b1 gradient\n",
    "    return w_out_grad, b_out_grad, w2_grad, b2_grad, w1_grad, b1_grad\n",
    "\n",
    "# Updates the parameters\n",
    "def step(NN, w2_grad, b2_grad, w1_grad, b1_grad, w_out_grad, b_out_grad, lr, batch_size):\n",
    "    # Note\n",
    "    NN.w1 = NN.w1 - (lr / batch_size) * w1_grad\n",
    "    NN.b1 = NN.b1 - (lr / batch_size) * b1_grad\n",
    "    NN.w2 = NN.w2 - (lr / batch_size) * w2_grad\n",
    "    NN.b2 = NN.b2 - (lr / batch_size) * b2_grad\n",
    "    NN.w_out = NN.w_out - (lr / batch_size) * w_out_grad\n",
    "    NN.b_out = NN.b_out - (lr / batch_size) * b_out_grad\n",
    "\n",
    "# Set our learning rate\n",
    "lr = 0.1\n",
    "# Initialize a neural network    \n",
    "MyNeuralNetwork = NeuralNetwork(64, 64, 10)\n",
    "# Load the data, this time with labels, index 0 is the input, 1 is the output\n",
    "digits = load_digits(n_class=10, return_X_y=True)\n",
    "# A sloppy data rescale (16 is the max pixel intensity)\n",
    "x = digits[0]/16\n",
    "# Create one-hot vectors for the outputs\n",
    "# I am lucky here that the digits are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "# Otherwise you would need a label dictionary to concert indices back to their labels\n",
    "y = np.eye(10)[digits[1]]\n",
    "# We'll do batches of 10... we need to find out the indexes to use\n",
    "batch_size = 10\n",
    "batch_pos = list(range(0, digits[0].data.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)\n",
    "epochs = 10\n",
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    for b in batch_pos:\n",
    "        batch_x = x[b:b+batch_size]\n",
    "        batch_y = y[b:b+batch_size]\n",
    "        a1, a2, a_out = feedforward(batch_x, MyNeuralNetwork)\n",
    "        cost = NLLcost(a_out, batch_y)\n",
    "        w_out_grad, b_out_grad, w2_grad, b2_grad, w1_grad, b1_grad \\\n",
    "            = backprop(batch_x, batch_y, MyNeuralNetwork, a1, a2, a_out)\n",
    "        step(MyNeuralNetwork, w2_grad, b2_grad, w1_grad, b1_grad, w_out_grad, b_out_grad, lr, batch_size)\n",
    "        if batch_num % 50 == 0:\n",
    "            print(\"epoch {:3d}, batch {:3d}, lr {:7.5f}, loss {:6.2f}\".format(ep, batch_num, lr, cost))\n",
    "        batch_num += 1\n",
    "    lr *= 0.75\n",
    "    a1, a2, a_out = feedforward(x, MyNeuralNetwork)\n",
    "    print(\"accuracy {:6.2f}%\".format(100 * accuracy(a_out, digits[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some code for testing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f799c5a5ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwlRfCAhig8k9B8avZZS/szCSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79b4205eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict  6,  84.21% certain\n"
     ]
    }
   ],
   "source": [
    "def predict(x, NN):\n",
    "    pick = np.random.randint(1, x.shape[0])\n",
    "    image = np.reshape(x[pick,], (8,8))\n",
    "    plt.gray()\n",
    "    plt.matshow(image) \n",
    "    plt.show() \n",
    "    _, _, a_out = feedforward(x[pick,], NN)\n",
    "    # The certainty here is the value of the highest output activation\n",
    "    print(\"I predict {:2d}, {:6.2f}% certain\".format(np.asscalar(a_out.argmax(axis=1)), 100.0*np.max(a_out)))\n",
    "\n",
    "predict(x, MyNeuralNetwork)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
