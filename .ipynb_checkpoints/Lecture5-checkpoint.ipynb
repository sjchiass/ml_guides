{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Back-propagation\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks ([get a copy](https://web.stanford.edu/~hastie/ElemStatLearn/))\n",
    "* Computer Age Statistical Inference: Chapter 18, Neural Networks and Deep Learning (p. 351) ([get a copy](https://web.stanford.edu/~hastie/CASI/))\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) are a bit mathy and specific, but I've found them helpful when confused\n",
    "\n",
    "## Code\n",
    "\n",
    "### Feed-forward basics\n",
    "\n",
    "#### Numpy importing\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved. Each NN library you use will have a way of handling matrices. They tend to be similar and might even just work with `numpy` matrices seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values flowing\n",
    "\n",
    "The feed-forward of a neural network runs input through the network's weights and actication functions to produce an output. The first \"layer\" of the NN is called the input layer, and it's merely a copy of the input data. The first active layer is the first of the NN's hidden layers. Here is what it does:\n",
    "\n",
    "$$z_1 = B_1 + x W_1$$\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "\n",
    "Let's look at that more closely. $B_1$ (biases) and $W_1$ (weights) are the \"neurons\" of the first layer. The convention is that their dimensions are $in \\times out$: $in$ is the number of input variables and $out$ is the number of neurons emitting output signals. The number of columns in the biases and weights are the neurons that individually perform some work on the incoming data.\n",
    "\n",
    "The crucial part of the neural network is that $X$ is matrix-multiplied by $W_1$, which is really $1 \\times in$ by $in \\times out$ giving a $1 \\times out$ matrix. The properties of matrix multiplication being what they are, each neuron \"works\" on the whole data independently and outputs a value separately.\n",
    "\n",
    "#### Neurons working\n",
    "\n",
    "The data is attended by multiple neurons, each able to perform its own processing of the data.\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed-forward is then a great mixing of data among layers of neurons, finally creating a network output. This \"densely connected\" NN has each neuron working separately from its neighbors but seeing the whole of the previous layer's data. These plentiful connections mean that the NN can model much more complicated functions. Although each layer is essentially linear, great power comes from the layers *interacting*.\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive commitees. Each neuron of each commitee works on a group report sent to its superior commitee, who work at a \"higher\" level further removed from the raw data. The big cheese at the output layer summarizes everything into a value from 0 to 1.\n",
    "\n",
    "#### Hiddens layers a-feed-forwarding\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "$$z_1 = B_1 + X W_1$$\n",
    "$$a_1 = \\sigma(z_1)$$\n",
    "$$z_2 = B_2 + a_1 W_2$$\n",
    "$$a_2 = \\sigma(z_2)$$\n",
    "$$z_{output} = B_{output} + a_2 W_{output}$$\n",
    "$$a_{output} = \\sigma(z_{output})$$\n",
    "\n",
    "Let's generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.random((10,5)) # Ten records of 5 variables\n",
    "b1 = np.random.random((1,3)) # 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # 1 x output_size\n",
    "w_out = np.random.random((2,1)) # layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here then are the feed-forward results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89478841  0.89253446  0.80753584]\n",
      " [ 0.90960405  0.86890526  0.70819821]\n",
      " [ 0.9477195   0.89983057  0.88796524]\n",
      " [ 0.91068958  0.93866011  0.82139541]\n",
      " [ 0.85850324  0.86447675  0.78321093]\n",
      " [ 0.85971402  0.88829667  0.68667252]\n",
      " [ 0.88032779  0.8868804   0.76358102]\n",
      " [ 0.90843132  0.87460983  0.84149436]\n",
      " [ 0.89837268  0.87407415  0.85356932]\n",
      " [ 0.92133014  0.86259464  0.83790139]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = b1 + X.dot(w1)\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85199045  0.74286491]\n",
      " [ 0.83905824  0.73282678]\n",
      " [ 0.86341688  0.75287126]\n",
      " [ 0.85516863  0.74659035]\n",
      " [ 0.84718797  0.73797151]\n",
      " [ 0.83497427  0.72942731]\n",
      " [ 0.84583163  0.73780024]\n",
      " [ 0.85621194  0.74597947]\n",
      " [ 0.85737624  0.74672015]\n",
      " [ 0.85589858  0.74568201]]\n"
     ]
    }
   ],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8684594 ]\n",
      " [ 0.86674157]\n",
      " [ 0.86999788]\n",
      " [ 0.86891918]\n",
      " [ 0.8677859 ]\n",
      " [ 0.86618764]\n",
      " [ 0.86763444]\n",
      " [ 0.86901086]\n",
      " [ 0.8691588 ]\n",
      " [ 0.86896786]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
