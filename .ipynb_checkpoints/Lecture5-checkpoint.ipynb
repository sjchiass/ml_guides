{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lecture 9, Back-propagation\n",
    "\n",
    "These notes cover a bit of week 4 as well. It's better to have feed forward, back propagation, and gradient checking all in one place.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* Elements of Statistical Learning: Chapter 11, Neural Networks ([get a copy](https://web.stanford.edu/~hastie/ElemStatLearn/))\n",
    "* Computer Age Statistical Inference: Chapter 18, Neural Networks and Deep Learning (p. 351) ([get a copy](https://web.stanford.edu/~hastie/CASI/))\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) are a bit mathy and specific, but I've found them helpful when confused\n",
    "\n",
    "## Feed forward & back propagation\n",
    "\n",
    "### Prelude: the perceptron\n",
    "\n",
    "The following diagram shows a perceptron.\n",
    "\n",
    "![Just a lonely neuron](W5_perceptron.png \"Just a lonely neuron\")\n",
    "\n",
    "Depending on its setup, it may be known as a neuron, a linear classifier, a linear regression (identity), or a logistic regression (sigmoid).\n",
    "\n",
    "### Feed forward basics\n",
    "\n",
    "A neural network is a stack of layers of perceptrons working together, and the term feed forward refers to \"feeding\" data through this network from start to finish. As you can see with the arbitrary example below, that can be a lot of feeding.\n",
    "\n",
    "![A lot](W5_ManyNeurons.png \"A lot\")\n",
    "\n",
    "The diagram above is just to scare you. Neural networks typically have *a lot* more connections!\n",
    "\n",
    "[*Drawn with this python script*](https://gist.github.com/craffel/2d727968c3aaebd10359#file-draw_neural_net-py)\n",
    "\n",
    "#### Numpy importing\n",
    "\n",
    "To get started, we'll need to import `numpy` to deal with all the matrices involved.\n",
    "\n",
    "Each NN library you use will have a way of handling matrices. They tend to be similar and might even just work with `numpy` matrices seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values flowing\n",
    "\n",
    "In a neural network, values flow through layers of synapses and neurons. This is called feed-forward.\n",
    "\n",
    "To feed-forward data through a neural network is to pass data through the network's weights and activation functions to create an output. The feed-forward receives its data at the input layer, a copy of the input. The activity begins at the first hidden layer, when the input signal is passed through synapses (weights), and adjusted (bias) and transformed (activation function) by the neuron. Here is what it does:\n",
    "\n",
    "##### First hidden layer\n",
    "\n",
    "Signal $x$ passing though weights $w_1$: $x \\times w_1$\n",
    "\n",
    "Signal adjusted by neuron bias $b_1$: $x \\times w_1 + b_1 = z_1$\n",
    "\n",
    "Signal shaped by activation function $\\sigma$: $a_1 = \\sigma(z_1)$\n",
    "\n",
    "Let's look at that more closely. The $w_1$ matrix (weights) is the collection of \"synapses\" of the \"neurons\": they are the connections the neurons use to pull in data. These synapses can be increased to amplify an incoming variable, set to zero to ignore one, or made negative to invert the inbound signal. During training they are tuned by the neurons to help the NN minimize prediction error.\n",
    "\n",
    "The biases $b_1$ are unique to each neuron. They're used by the neuron to adjust what they receive.\n",
    "\n",
    "The activation function $\\sigma$, the sigmoid function, causes the neuron to output a binary signal. Unlike a digital computer though, the signal can range *between* 0 and 1 if the neuron is unsure.\n",
    "\n",
    "* Synapses strengthen or weaken incoming variables with their weights\n",
    "* Biases adjust the sum of these weighted signals\n",
    "* Neuron does a weighted sum of everything and transforms it with its activation function\n",
    "\n",
    "The weights are matrices with dimensions $in \\times out$, $in$ the size of the data coming in and $out$ the size of the data coming out. $out$ is the number of neurons in the layer, and $in$ is the amount of values each of these neurons is fed during feed-forward. The biases are $1 \\times out$, one bias for each neuron.\n",
    "\n",
    "![Dimensions](W5_NeuronSimple.png \"Dimensions\")\n",
    "\n",
    "Above you can see that the input data is 3 values and that each neuron has 3 synapses ($2 \\times 3 = 6$ in total). There are 2 neurons in the layer and they produce 2 outputs, one each neuron. There are also 2 biases integrated inside the 2 neurons.\n",
    "\n",
    "Each layer has only two quantities: how much comes in and how much goes out.\n",
    "\n",
    "The first aspect of the feed-forward is then the flowing of data through weights, biases, and activation functions.\n",
    "\n",
    "#### Neurons working\n",
    "\n",
    "A neural network has a dual nature: a linear nature at the unit level and a complex non-linear nature at the network level.\n",
    "\n",
    "The neurons' linear nature helps them perform computations. They each get their own copy of the data to work on. This amazing trick is possible because of matrix multiplication (or dot product). Rows don't mix with other rows, neither columns with other columns.\n",
    "\n",
    "##### Matrices are fun\n",
    "\n",
    "Here is a trivial but familiar example. You can see that each neuron (column) does its own thing. Change one of the weight's element to see the effect on the output. The five neurons' outputs are the five elements in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1]])\n",
    "x.dot(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will give you my own example of using the neurons' synapses to play with the input data. I can perform operations on the inputs separately for each neuron. Look at each weight column vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5]])\n",
    "weight = np.array([[-1,  0,  0,  0,  0],\n",
    "                   [ 0,  2,  0,  0,  0],\n",
    "                   [ 2,  0,  1, -1,  0],\n",
    "                   [ 0,  0,  0,  0, -1],\n",
    "                   [ 0,  0,  0,  1,  1]])\n",
    "x.dot(weight)\n",
    "\n",
    "# I could also just flip the identity matrix horizontally to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix algebra allows the NN to perform arithmetic.\n",
    "\n",
    "##### The non-linear mixing\n",
    "\n",
    "Data is not mixed within a layer, but it is mixed between them. Neural networks get their power from the interactions of their hidden layers.\n",
    "\n",
    "When you take a single-layer model like linear or logistic regression and add layers to it, you get the extra power.\n",
    "\n",
    "The feed-forward is a mixing of data over many layers of neurons. Each layer expands data into multiple copies and its neurons compress it back into a few outputs. In the diagram above you see 3 units of inputs expanded into 6 synapse signals and then collapsed into 2 output signals. The power of the neural network comes from the fact that the next layer *then copies* these 2 output signals to each of its own neurons, so everything affects everything.\n",
    "\n",
    "![Re-using footage](W5_ManyNeurons.png \"Re-using footage\")\n",
    "\n",
    "See how many arrows there are going from layer to layer? Information expands and contracts, and network elements are generously connected together.\n",
    "\n",
    "Or in other words, feed-forward is like a decision reached by successive committees. The neurons in a layer form a committee that looks at data together, performs analysis, and then summarizes its findings into a small report. Higher committees then analyze this report at a higher level, and so on. The final output layer makes a decision based on the accumulated wisdom of the executive summary it receives: it outputs a single value between 0 and 1.\n",
    "\n",
    "In the committee example, the office workers use\n",
    "\n",
    "* weights to increase, decrease or invert the importance of data\n",
    "* biases to make their voices louder or weaker\n",
    "* activation functions to simply their reports into a range [0,1]\n",
    "\n",
    "The expansion and contraction of information is repeated multiple times in the neural network.\n",
    "\n",
    "All this mixing allows the neural network to work with very complicated data.\n",
    "\n",
    "#### Hiddens layers feed-forwarding\n",
    "\n",
    "With all that in mind, this is the feed-forward:\n",
    "\n",
    "1. $z_1 = X \\times W_1 + B_1$\n",
    "2. $a_1 = \\sigma(z_1)$\n",
    "3. $z_2 = a_1 \\times W_2 + B_2$\n",
    "4. $a_2 = \\sigma(z_2)$\n",
    "5. $z_{output} = a_2 \\times W_{output} + B_{output}$\n",
    "6. $a_{output} = \\sigma(z_{output})$\n",
    "\n",
    "By way of comparison, here is Andrew Ng's notation.\n",
    "\n",
    "1. $a^{(1)} = x$\n",
    "2. $z^{(2)} = \\theta^{(1)} a^{(1)})$\n",
    "3. $a^{(2)} = g(z^{(2)})$\n",
    "2. $z^{(3)} = \\theta^{(2)} a^{(2)})$\n",
    "3. $a^{(3)} = g(z^{(3)})$\n",
    "2. $z^{(4)} = \\theta^{(3)} a^{(3)})$\n",
    "6. $a_{(4)} = h_{\\theta}(x)=g(z^{(4)})$\n",
    "\n",
    "Let's generate some data. Thanks to the properties of matrix multiplication, I can have 4 rows of input data and these will be processed fully separately, yielding 4 rows of output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random((4,5)) # Four records of 5 variables\n",
    "b1 = np.random.random((1,3)) # Bias: 1 x layer_1_size\n",
    "w1 = np.random.random((5,3)) # Weight: input_vars x layer_1_size\n",
    "b2 = np.random.random((1,2)) # Bias: 1 x layer_2_size\n",
    "w2 = np.random.random((3,2)) # Weight: layer_1_size x layer_2_size\n",
    "b_out = np.random.random((1,1)) # Bias: 1 x output_size\n",
    "w_out = np.random.random((2,1)) # Weight: layer_2_size x output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the activations of all the layers participating in the feed forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# First hidden layer, three neurons each give an output\n",
    "z1 = x.dot(w1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second hidden layer, two neurons each give an ouput\n",
    "z2 = b2 + a1.dot(w2)\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output layer: one output for each input record\n",
    "z_out = b_out + a2.dot(w_out)\n",
    "a_out = sigmoid(z_out)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation I\n",
    "\n",
    "Backprop is how the network evaluates its performance during feed forward.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "If the feed-forward is data pushed all the way forward to the outputs, then back-propagation is the trickling down of errors flowing back from the outputs all the way back to the very first hidden layer.\n",
    "\n",
    "Backprop is a way of using gradient descent on neural networks of multiple layers. It isn't necessary with a linear or logistic regression because these are simple single-layer networks. Back-propagation allows you to apply gradient descent more than once.\n",
    "\n",
    "It all starts at the output. Here there is a clear link between the choice of parameters (weights and biases) and the output error. The approach here is the same as simple gradient descent.\n",
    "\n",
    "At the layer preceeding the output, we'll call it $l_2$, there is an extra step. What is the link between $l_2$ weights and biases and the output error? It has multiple steps: $l_2$ has a direct effect on the output layer's data, and the output layer's data has a direct effect on what the model decides to output. It takes two steps to get back to the end.\n",
    "\n",
    "In other words, the output layer is the boss and it is directly responsible for the model's error. If the output layer changes its behaviour, it can directly improve its accuracy. It's the easiest to train.\n",
    "\n",
    "The hidden layers are not directly responsible for the model's error; however, they are responsible for providing the output layer accurate analyses of the model's input data. Knowing their boss, they have an idea of how to change their computations so that the big cheese makes more informed decisions. Their gradient formulas in fact depend on the output layer's weights (the boss's personality, you might say).\n",
    "\n",
    "### Detour: gradient checking\n",
    "\n",
    "Backprop takes snapshots of errors everywhere in the NN and uses these to adjust parameters. Normally this is done with calculus and repeated applications of the chain rule of differentiation.\n",
    "\n",
    "Backprop can also be done by more primitive methods, albeit much more slowly. Numerical differentiation is used to teach students calculus, so it makes sense to show it here first before breaking out the chain rule.\n",
    "\n",
    "The idea behind numerical differentiation is this: \n",
    "\n",
    "1. Take your NN as is\n",
    "2. Adjust a parameter slightly and see the effect on output error\n",
    "3. You now know the effect of that parameter on error\n",
    "\n",
    "Given $J(\\theta_{i,j})$ your cost function, \\theta any parameter anywhere in the neural network, and $\\epsilon$ a small value as a \"nudge\", \n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i,j}} \\approx \\frac{J(\\theta_{i,j} + \\epsilon) - J(\\theta_{i,j} - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "Backprop does this for every parameter in the NN. If it sees that error increases when a parameter is increased, it will decrease the parameter. The opposite is done with a parameter that increases error when it decreases.\n",
    "\n",
    "Ultimately, this makes a neural network more complicated than any collection of corporate committees. Except in rare prophetic instances, an office worker will not know how many dollars their actions win/lose their company. With neural networks though, it is calculated.\n",
    "\n",
    "We will incorporate gradient checking into a NN later.\n",
    "\n",
    "### Back propagation II\n",
    "\n",
    "I hope the above has made back propagation make some sense. It's now time for some light mathematics. Don't worry I'll just paste the answers and skip the algebraic Tetris.\n",
    "\n",
    "Here are the gradients with respect to error for parameters in the NN model.\n",
    "\n",
    "The gradients for the output layer weights and biases are\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial w_{out}} = a_2^T \\cdot (a_{out} - y), \\frac{\\partial J(\\theta)}{\\partial b_{out}} = (a_{out} - y)$$\n",
    "\n",
    "The above equations make some sense. The neuron's bias is a straight adjustment to the output, so it makes sense that its formula would be so simple. Weights are used to adjust incoming synapse signals, so the inclusion of $a_2^T$ makes some sense as well.\n",
    "\n",
    "To proceed lower into the previous layer, we have to do some backprop. Here it is:\n",
    "\n",
    "$$\\delta_{out} = (a_{out} - y) \\cdot w_{out}^T$$\n",
    "\n",
    "We also need the derivative of the sigmoid function. We'll just call it $\\sigma^\\prime$.\n",
    "\n",
    "We just have to include that in our equations and we'll be fine. The gradients for the second hidden layer are:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial w_2} = a_1^T \\cdot (\\sigma^\\prime(a_2) \\circ \\delta_{out}), \\frac{\\partial J(\\theta)}{\\partial b_2} = \\sigma^\\prime(a_2) \\circ \\delta_{out}$$\n",
    "\n",
    "The sigmoid derivative $\\sigma^\\prime$ is a newcomer, but otherwise these are similar to before. The weight gradients depend on layer 2's input, which comes from layer 1. The bias gradient is simpler, but it still has to pass through the $\\sigma^\\prime$ and the $\\delta_{out}$.\n",
    "\n",
    "For layer 1 we need a new delta.\n",
    "\n",
    "$$\\delta_2 = (\\delta_{out} \\circ \\sigma^\\prime(a_2)) \\cdot w_2^T$$\n",
    "\n",
    "Finally, the last backprop step.\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial w_1} = x^T \\cdot (\\sigma^\\prime(a_1) \\circ \\delta_2), \\frac{\\partial J(\\theta)}{\\partial b_1} = \\sigma^\\prime(a_1) \\circ \\delta_2$$\n",
    "\n",
    "That's all there is to it.\n",
    "\n",
    "### MNIST example\n",
    "\n",
    "[We'll use scikit-learn's toy dataset.](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is copy-pasted from the example on the scikit-learn website\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below. It runs but must be debugged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = 1\n",
    "batch += 1\n",
    "print(batch)\n",
    "batch += 1\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.b1 = np.random.normal(size=(1,hidden_size))\n",
    "        self.w1 = np.random.normal(size=(input_size,hidden_size))\n",
    "        self.b2 = np.random.normal(size=(1,hidden_size))\n",
    "        self.w2 = np.random.normal(size=(hidden_size,hidden_size))\n",
    "        self.b_out = np.random.normal(size=(1,output_size))\n",
    "        self.w_out = np.random.normal(size=(hidden_size,output_size))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "        \n",
    "def NLLcost(a_out, y):\n",
    "    return -np.mean(np.sum(y*np.log(a_out) + (1-y)*np.log(1-a_out), 1))\n",
    "        \n",
    "def accuracy(a_out, y):\n",
    "    tests = a_out.argmax(axis=0) == y\n",
    "    print(tests.sum())\n",
    "\n",
    "def feedforward(x, y, NN):\n",
    "    z1 = x.dot(NN.w1) + NN.b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(NN.w2) + NN.b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z_out = a2.dot(NN.w_out) + NN.b_out\n",
    "    a_out = sigmoid(z_out)\n",
    "    cost = NLLcost(a_out, y)\n",
    "    return a1, a2, a_out, cost\n",
    "\n",
    "def backprop(x, y, NN, a1, a2, a_out):\n",
    "    # output parameters\n",
    "    w_out_grad = a2.transpose().dot(a_out - y)\n",
    "    b_out_grad = a_out - y\n",
    "    delta_out = (a_out - y).dot(NN.w_out.transpose())\n",
    "    # layer 2 parameters\n",
    "    prime2 = a2 * (1 - a2)\n",
    "    w2_grad = a1.transpose().dot(prime2 * delta_out)\n",
    "    b2_grad = (prime2 * delta_out)\n",
    "    delta2 = (delta_out * prime2).dot(NN.w2.transpose())\n",
    "    # layer 1 parameters\n",
    "    prime1 = a1 * (1 - a1)\n",
    "    w1_grad = x.transpose().dot(prime1 * delta2)\n",
    "    b1_grad = (prime1 * delta2)\n",
    "    return w_out_grad, b_out_grad, w2_grad, b2_grad, w1_grad, b1_grad\n",
    "    \n",
    "def step(NN, w2_grad, b2_grad, w1_grad, b1_grad, w_out_grad, b_out_grad, lr, batch_size):\n",
    "    NN.w1 =- (lr / batch_size) * w1_grad\n",
    "    NN.b1 =- (lr / batch_size) * b1_grad.sum(axis=0)\n",
    "    NN.w2 =- (lr / batch_size) * w2_grad\n",
    "    NN.b2 =- (lr / batch_size) * b2_grad.sum(axis=0)\n",
    "    NN.w_out =- (lr / batch_size) * w_out_grad\n",
    "    NN.b_out =- (lr / batch_size) * b_out_grad.sum(axis=0)\n",
    "\n",
    "# Set our learning rate\n",
    "lr = 0.05\n",
    "# Initialize a neural network    \n",
    "MyNeuralNetwork = NeuralNetwork(64, 256, 10)\n",
    "# Load the data, this time with labels, index 0 is the input, 1 is the output\n",
    "digits = load_digits(n_class=10, return_X_y=True)\n",
    "# A sloppy data rescale (16 is the max pixel intensity)\n",
    "x = digits[0]/16 - 0.5\n",
    "# Create one-hot vectors for the outputs\n",
    "y = np.eye(10)[digits[1]]\n",
    "# We'll do batches of 10... we need to find out the indexes to use\n",
    "batch_size = 10\n",
    "batch_pos = list(range(0, digits[0].data.shape[0] - 1, batch_size))\n",
    "batch_amount = len(batch_pos)\n",
    "epochs = 5\n",
    "for ep in range(1, epochs+1):\n",
    "    batch_num = 1\n",
    "    for b in batch_pos:\n",
    "        batch_x = x[b:b+batch_size]\n",
    "        batch_y = y[b:b+batch_size]\n",
    "        a1, a2, a_out, cost = feedforward(batch_x, batch_y, MyNeuralNetwork)\n",
    "        w_out_grad, b_out_grad, w2_grad, b2_grad, w1_grad, b1_grad \\\n",
    "            = backprop(batch_x, batch_y, MyNeuralNetwork, a1, a2, a_out)\n",
    "        step(MyNeuralNetwork, w2_grad, b2_grad, w1_grad, b1_grad, w_out_grad, b_out_grad, lr, batch_size)\n",
    "        if batch_num % 25 == 0:\n",
    "            print(\"epoch {:3d}, batch {:3d}, lr {:7.5f}, loss {:6.2f}\".format(ep, batch_num, lr, cost))\n",
    "        batch_num += 1\n",
    "    lr /= 2\n",
    "    a1, a2, a_out, cost = feedforward(x, y, MyNeuralNetwork)\n",
    "    accuracy(a_out, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
