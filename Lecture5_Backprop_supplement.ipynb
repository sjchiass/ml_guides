{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lecture 9, Back-propagation Supplement\n",
    "\n",
    "These are meant as a guide to doing backpropagation calculus. I'll include copious comments to explain what I'm doing\n",
    "\n",
    "# Further reading\n",
    "\n",
    "* [Peter's Notes](http://peterroelants.github.io/posts/neural_network_implementation_part01/) goes over derivatives\n",
    "\n",
    "# Backpropagation Calculus\n",
    "\n",
    "## Simple stuff\n",
    "\n",
    "### Softmax and sigmoid-NLL\n",
    "\n",
    "The softmax and the sigmoid are most popular output activation layers for classification. The softmax selects one category out of many, by returning class probabilities, and the sigmoid tests each category separately, returning 0 to 1 values for each.\n",
    "\n",
    "Both of these simplify to $a_{out} - y$ but the algebra is more complicated. We'll start with a sigmoid using the quadratic cost function (a.k.a. mean squared error).\n",
    "\n",
    "### Full example\n",
    "\n",
    "Doing the full backprop requires repetive application of the chain rule of differentiation, to the point that\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial W_1} = \\frac{\\partial J(\\theta)}{\\partial a_{out}} $$\n",
    "\n",
    "Like the Andrew Ng course, we'll identify the cost function by $J(\\theta)$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} (a_{out} - y)^2$$\n",
    "\n",
    "Using the power rule of differentiation, this is easy:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial a_{out}} = \\frac{\\partial}{\\partial a_{out}} \\frac{1}{2}(a_{out} - y)^2 = a_{out} - y$$\n",
    "\n",
    "#### Sigmoid derivative\n",
    "\n",
    "Next is the derivative of the sigmoid function's output with respect to its input, $z_{out}$.\n",
    "\n",
    "$$\\frac{\\partial a_{out}}{\\partial z_{out}} = \\frac{\\partial \\sigma(z_{out})}{\\partial z_{out}}$$\n",
    "\n",
    "We first start by applying the reciprocal rule \n",
    "\n",
    "$$\\frac{d \\mathit{f(x)}}{dx} = - \\frac{\\mathit{f^\\prime(x)}}{\\mathit{f(x)^2}}$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\\frac{\\partial \\sigma(z_{out})}{\\partial z_{out}} = \\frac{e^{-z}}{(1+e^{-z})^2}$$\n",
    "\n",
    "We then use a trick where we add 1 to the numerator while also subtracting one. This has no effect on the value of the fraction but manages to help us in our quest.\n",
    "\n",
    "$$\\frac{1+e^{-z}-1}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}} - \\left(\\frac{1}{1+e^{-z}}\\right)^2$$\n",
    "\n",
    "We've managed to turn the derivative into the sigmoid function minus its squared value. With this we can factor out $\\frac{1}{1+e{-z}}$ and obtain the sigmoid derivative.\n",
    "\n",
    "$$\\frac{1}{1+e^{-z}}\\left(1 - \\frac{1}{1+e^{-z}}\\right) = \\frac{\\partial \\sigma(z_{out})}{\\partial z_{out}} = \\sigma(z_{out})(1-\\sigma(z_{out}))$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
